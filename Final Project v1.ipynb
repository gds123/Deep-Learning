{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import simplejson\n",
    "import json\n",
    "import time\n",
    "import scipy.spatial\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer as tokenizer\n",
    "\n",
    "# utils\n",
    "def initEmbeddingMap(fileName='glove.6B.50d.txt'):\n",
    "    embedding_map = {}\n",
    "    BASE_DIR = ''\n",
    "    GLOVE_DIR = os.path.join(BASE_DIR, 'data/glove.6B')\n",
    "    glove = open(os.path.join(GLOVE_DIR, fileName))\n",
    "    \n",
    "    print('constructing embedding dictionary')\n",
    "    embedding_map = {}\n",
    "    for line in glove:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        value = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_map[word] = value\n",
    "    glove.close()\n",
    "    return embedding_map\n",
    "\n",
    "def clean(text):\n",
    "    return text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                 lower=True, split=\" \")\n",
    "\n",
    "\n",
    "# returns dict with users and movies they rated as repeated rows\n",
    "# cleans review text and add to rawOutput\n",
    "def initRawData(input_file, save=False):\n",
    "    print('initializing raw data')\n",
    "    rawInputData = []\n",
    "    rawOutputData = []\n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            if len(line) < 4:\n",
    "                break\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rawInputDataObj = {'user':user, 'asin':item}\n",
    "            rawOutputDataObj = clean(lineObj['reviewText'])\n",
    "            rawInputData.append(rawInputDataObj)\n",
    "            rawOutputData.append(rawOutputDataObj)\n",
    "#     if save:\n",
    "#         pickle.dump((rawInputData, rawOutputData), open(fileName,'wb'))\n",
    "    return rawInputData, rawOutputData\n",
    "\n",
    "# creates dict of usrs w/ all movies rated + movies w/ all user ratings *** \n",
    "def group_data(inputData):\n",
    "    users = {}\n",
    "    items = {}\n",
    "    for datum in inputData:\n",
    "        u = datum['user']\n",
    "        i = datum['asin']\n",
    "        users.setdefault(u, []).append(i)\n",
    "        items.setdefault(i, []).append(u)\n",
    "    return (users, items)\n",
    "\n",
    "git\n",
    "def getSetFromData(key, data):\n",
    "    result = set()\n",
    "    for datum in data:\n",
    "        result.add(datum.get(key))\n",
    "    return result\n",
    "\n",
    "def seq_2_matrix(sequence, embedding_map):\n",
    "    m = []\n",
    "    for word in sequence:\n",
    "        emb = embedding_map.get(word)\n",
    "        if emb is not None:\n",
    "            m.append(emb)\n",
    "    return np.array(m)\n",
    "\n",
    "def matrix_2_avg(emb_matrix):\n",
    "    return np.mean(emb_matrix, 0)\n",
    "\n",
    "\n",
    "# utils - one hot encodes all data \n",
    "def initVecData(rawInputData, rawOutputData, embedding_map):\n",
    "    print('initializing vectorized data')\n",
    "    dictVect = DictVectorizer()\n",
    "    vecInputData = dictVect.fit_transform(rawInputData).toarray()\n",
    "    vecOutputData = [matrix_2_avg(seq_2_matrix(review, embedding_map)) for review in rawOutputData]\n",
    "    return vecInputData, vecOutputData\n",
    "\n",
    "def initMatInputData(rawInputData, rawOutputData, embedding_map, save=False):\n",
    "    print('initializing matrix data')\n",
    "    if len(rawInputData) != len(rawOutputData):\n",
    "        raise ValueError(\"Need same size of input and output\")\n",
    "    users = {}\n",
    "    extra_info = {}\n",
    "    items = {}\n",
    "    dictVect = DictVectorizer()\n",
    "    for i in range(len(rawInputData)):\n",
    "        vecOutput = seq_2_matrix(rawOutputData[i], embedding_map)\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        users.setdefault(user, []).append(vecOutput)\n",
    "        items.setdefault(item, []).append(vecOutput)\n",
    "        \n",
    "    matUserInputData = []\n",
    "    matItemInputData = []\n",
    "    users = {k: np.vstack(v) for k, v in users.items()}\n",
    "    items = {k: np.vstack(v) for k, v in items.items()}\n",
    "    extra_info['user_seq_sizes'] = [m.shape[0] for m in users.values()]\n",
    "    extra_info['item_seq_sizes'] = [m.shape[0] for m in items.values()]\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        matUserInputData.append(users.get(user))\n",
    "        matItemInputData.append(items.get(item))\n",
    "#     if save:\n",
    "#         pickle.dump((matUserInputData, matItemInputData), open(fileName,'wb'))\n",
    "    return matUserInputData, matItemInputData, extra_info\n",
    "\n",
    "def toKey(user, item):\n",
    "    return (user, item)\n",
    "\n",
    "def initRatingsOutputData(rawInputData, input_file, save=False):\n",
    "    ratingsData = []\n",
    "    userItemDict = {}\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        userItem = toKey(rawInput['user'], rawInput['asin'])\n",
    "        userItemDict[userItem] = i\n",
    "        ratingsData.append(None) # check later to make sure no Nones left\n",
    "        \n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rating = lineObj['overall']\n",
    "            i = userItemDict.get(toKey(user, item))\n",
    "            if i is not None:\n",
    "                ratingsData[i] = rating\n",
    "        failure = None in ratingsData\n",
    "        if failure:\n",
    "            raise ValueError(str(len([r for r in ratingsData if r is None])) + \" reviews did not have corresponding rating.\")\n",
    "#     if save:\n",
    "#         pickle.dump(ratingsData, open(fileName,'wb'))\n",
    "    return ratingsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_initRawData( )_** - returns rawInputData and rawOutputData.\n",
    "\n",
    "**_rawInputData:_** List of dicts where each dict is a userID and movieID grouping. Groupings are unqiue but a user \n",
    "or movie can/do repeat if multiple users have reviewed a particular movie\n",
    "    \n",
    "**_rawOutputData:_** List of lists where each sub list is text from a unique review. The reviews correspond to the\n",
    "user/movie key pairs in rawInputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing raw data\n",
      "Number of user/item data: 18563\n",
      "Number of reviews: 18563\n",
      "\n",
      "[{'user': 'A3BC8O2KCL29V2', 'asin': 'B000H00VBQ'}, {'user': 'A1RJPIGRSNX4PW', 'asin': 'B000H00VBQ'}, {'user': 'A1POFVVXUZR3IQ', 'asin': 'B000H00VBQ'}, {'user': 'ATASGS8HZHGIB', 'asin': 'B000H0X79O'}, {'user': 'AUX8EUBNTHIIU', 'asin': 'B000H0X79O'}]\n",
      "\n",
      "['i', 'highly', 'recommend', 'this', 'series', 'it', 'is', 'a', 'must', 'for', 'anyone', 'who', 'is', 'yearning', 'to', 'watch', 'grown', 'up', 'television', 'complex', 'characters', 'and', 'plots', 'to', 'keep', 'one', 'totally', 'involved', 'thank', 'you', 'amazin', 'prime']\n",
      "\n",
      "['mysteries', 'are', 'interesting', 'the', 'tension', 'between', 'robson', 'and', 'the', 'tall', 'blond', 'is', 'good', 'but', 'not', 'always', 'believable', 'she', 'often', 'seemed', 'uncomfortable']\n"
     ]
    }
   ],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "rawInputData, rawOutputData = initRawData(input_file=fileName, save=False)\n",
    "print(\"Number of user/item data:\", len(rawInputData))\n",
    "print(\"Number of reviews:\", len(rawOutputData))\n",
    "print()\n",
    "print(rawInputData[0:5])\n",
    "print()\n",
    "print(rawOutputData[0])\n",
    "print()\n",
    "print(rawOutputData[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_group-data( )_** - returns dicts of userID/movieID and movieID/userID groups\n",
    "\n",
    "**_users:_** Dict where each userID key has a corresponding list of movieID they've' reviewed\n",
    "    \n",
    "**_movies:_** Dict where each movieID key has all the userIDs of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All movies userID 'A3BC8O2KCL29V2' has seen:\n",
      " ['B000H00VBQ', 'B00F0CLHQO', 'B00I3MMN4I']\n",
      "\n",
      "All userIDs of watches of movie 'B00F0CLHQO':\n",
      " ['A3BC8O2KCL29V2', 'A821GRKOLGILD', 'AGTOYPUYZCCWN', 'AXO4PQU0XG3TG', 'A2LCOSYZL96HHA', 'A328S9RN3U5M68', 'AGUN5F2W85777', 'A1WXOTZHDNAV0Q', 'A3QLAOOTFEHCJI', 'A1TPW86OHXTXFC', 'AFVHO52P7IMPG', 'A18758S1PUYIDT']\n",
      "\n",
      "Number of movies: 1685\n",
      "Number of users: 5033\n"
     ]
    }
   ],
   "source": [
    "users, movies = group_data(rawInputData)\n",
    "print(\"All movies userID 'A3BC8O2KCL29V2' has seen:\\n\", users['A3BC8O2KCL29V2'])\n",
    "print()\n",
    "print(\"All userIDs of watches of movie 'B00F0CLHQO':\\n\", movies['B00F0CLHQO'])\n",
    "print()\n",
    "print(\"Number of movies:\", len(movies))\n",
    "print(\"Number of users:\", len(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized index list: [ 6197  9171 11547 ..., 14792   364 18251]\n",
      "Total number of rand_indxs: 18563\n"
     ]
    }
   ],
   "source": [
    "rand_idxs = np.random.permutation(len(rawOutputData))\n",
    "print(\"Randomized index list:\", rand_idxs)\n",
    "print(\"Total number of rand_indxs:\", len(rand_idxs))\n",
    "rawInputData = [rawInputData[i] for i in rand_idxs]\n",
    "rawOutputData = [rawOutputData[i] for i in rand_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Embedding Matrix **\n",
    "\n",
    "Uses GloVe 50d (100d, 300d) pre-trained embedding to create a giant word dict of possible words.\n",
    "We then use this matrix to create encodings of movie reviews \n",
    "\n",
    "__requires__: GloVe Embeddings to be downloaded - http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing embedding dictionary\n"
     ]
    }
   ],
   "source": [
    "embedding_map = initEmbeddingMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_getSetFromData( )_**: Gets list of unique userID or unique movieID from data\n",
    "\n",
    "**_initVecData( )_**: Returns vectorized versions _InputData_ and dense encodings of review text using embedding matrix\n",
    "\n",
    "**_vecInputData_**: \n",
    "Is rawInputData dict of 18563 samples of userID to movieID, converted to one-hot encoding of where first 5033 col represent user encodings and remaining represent 1685 movieIDs\n",
    "\n",
    "**_vecOutputData_**: \n",
    "Is rawOutputData list of reviews seperated into words where each \n",
    "word in the review is swapped for it's embedding representation i.e.\n",
    "1x50 row per word. These rows are turn into a matrix of words x 50 dims\n",
    "then you take mean of each col to get a 1x50 array (why?). These arrays\n",
    "are returned for each  18563 sample yielding a 18563x50 list matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = getSetFromData('user', rawInputData)\n",
    "all_movies = getSetFromData('asin', rawInputData)\n",
    "vecInputData, vecOutputData = initVecData(rawInputData, rawOutputData, embedding_map)\n",
    "\n",
    "print(\"Shape of matrix vecInputData: {}\".format(vecInputData.shape))\n",
    "print(\"Shape of list vecOutputData: ({}, {})\".format(len(vecOutputData), len(vecOutputData[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18563, 6718)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecInputData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_initMatInputData_**: Creates dense encodings UserID/MovieID data and text review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing matrix data\n",
      "[array([[  5.50249994e-01,  -2.49420002e-01,  -9.38599987e-04, ...,\n",
      "          3.37630004e-01,   5.01389988e-02,   3.34650010e-01],\n",
      "       [  6.80469990e-01,  -3.92629988e-02,   3.01860005e-01, ...,\n",
      "         -7.32970014e-02,  -6.46990016e-02,  -2.60439992e-01],\n",
      "       [  3.68079990e-01,   2.08340004e-01,  -2.23189995e-01, ...,\n",
      "         -6.72360003e-01,  -3.97089988e-01,   2.51830012e-01],\n",
      "       ..., \n",
      "       [  3.30419987e-01,   2.49950007e-01,  -6.08739972e-01, ...,\n",
      "         -5.07030010e-01,  -2.72729993e-02,  -5.32850027e-01],\n",
      "       [  5.30740023e-01,   4.01169986e-01,  -4.07849997e-01, ...,\n",
      "          2.87620008e-01,   1.44400001e-01,   2.36110002e-01],\n",
      "       [ -3.11500013e-01,   5.29200017e-01,  -1.34570003e+00, ...,\n",
      "         -7.68649995e-01,  -2.59369999e-01,   1.56680000e+00]], dtype=float32)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matUserInputData, matMovieInputData, extra_info = initMatInputData(rawInputData, rawOutputData, embedding_map)\n",
    "#print(len(matUserInputData))\n",
    "#print(len(matMovieInputData))\n",
    "print(matUserInputData[0:1])\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2194, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.026567  ,  1.33570004, -1.028     , ..., -0.49658   ,\n",
       "        -0.41618001, -0.25490001],\n",
       "       [-1.03859997,  0.52319998, -0.73141003, ...,  0.18791001,\n",
       "        -0.024801  ,  0.42411   ],\n",
       "       [ 0.53074002,  0.40116999, -0.40785   , ...,  0.28762001,\n",
       "         0.1444    ,  0.23611   ],\n",
       "       ..., \n",
       "       [ 0.35934001, -0.26570001, -0.046477  , ...,  0.10673   ,\n",
       "        -0.11503   ,  0.074678  ],\n",
       "       [ 0.53074002,  0.40116999, -0.40785   , ...,  0.28762001,\n",
       "         0.1444    ,  0.23611   ],\n",
       "       [ 0.81544   ,  0.30171001,  0.54720002, ...,  0.31964999,\n",
       "        -0.34740999,  0.41672   ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(matMovieInputData[5].shape)\n",
    "matMovieInputData[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_initRatingsOutputData( )_**: \n",
    "Index each unique key pair (usr/movie) in rawInputData then assigns the correct\n",
    "rating to the ratings list based on that index of the key pair in dict userItemDict.\n",
    "Returns list of scores ordered correctly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few ratings...\n",
      " [4.0, 5.0, 5.0, 1.0, 5.0, 5.0, 5.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 1.0, 5.0, 5.0, 5.0, 4.0]\n",
      "\n",
      "Total ratings: 18563\n"
     ]
    }
   ],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "ratingsData = initRatingsOutputData(rawInputData, input_file=fileName,save=False)\n",
    "\n",
    "print(\"Printing a few ratings...\\n\", ratingsData[0:20])\n",
    "print()\n",
    "print(\"Total ratings:\", len(ratingsData))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCoNN Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Dot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers.merge import Add, Dot, Concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepCoNN():\n",
    "    def __init__(self, embedding_size, hidden_size, u_seq_len, m_seq_len, filters=2, kernel_size=8, strides=6):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.inputU, self.towerU = self.create_deepconn_tower(u_seq_len)\n",
    "        self.inputM, self.towerM = self.create_deepconn_tower(m_seq_len)\n",
    "        self.joined = Concatenate()([self.towerU, self.towerM])\n",
    "        self.outNeuron = Dense(1)(self.joined)\n",
    "\n",
    "    def create_deepconn_tower(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        tower = Conv1D(filters=self.filters, kernel_size=self.kernel_size, activation=\"relu\")(input_layer)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Flatten()(tower)\n",
    "        tower = Dense(self.hidden_size, activation=\"relu\")(tower)\n",
    "        return input_layer, tower\n",
    "\n",
    "    def create_deepconn_dp(self):\n",
    "        dotproduct = Dot(axes=1)([self.towerU, self.towerM])\n",
    "        output = Add()([self.outNeuron, dotproduct])\n",
    "        model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
    "        model.compile(optimizer='Adam', loss='mse')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_DeepcoNN(matUserInputData, matItemInputData, ratingsData, u_seq_len=200, i_seq_len=200, hidden_size=4, epochs=3500, training=None):\n",
    "    embed_dims = matUserInputData[0].shape[1]\n",
    "    deepconn = DeepCoNN(embed_dims, hidden_size, u_seq_len, i_seq_len)\n",
    "\n",
    "    model = deepconn.create_deepconn_dp()\n",
    "\n",
    "    user_input = pad_sequences(np.asarray(matUserInputData), maxlen=u_seq_len)\n",
    "    item_input = pad_sequences(np.asarray(matItemInputData), maxlen=i_seq_len)\n",
    "\n",
    "    trainingN = int(len(user_input) * training) if type(training) is float else training\n",
    "\n",
    "    inputs = [user_input, item_input]\n",
    "    outputs = np.asarray(ratingsData)\n",
    "    print(model.summary())\n",
    "\n",
    "    train_inputs = [user_input[:trainingN], item_input[:trainingN]]\n",
    "    train_outputs = outputs[:trainingN]\n",
    "    test_inputs = [user_input[trainingN:], item_input[trainingN:]]\n",
    "    test_outputs = outputs[trainingN:]\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=4)\n",
    "    early_stopping_val = EarlyStopping(monitor='val_loss', patience=6)\n",
    "    batch_size = 32\n",
    "    model.fit(train_inputs, train_outputs, validation_split=0.2, callbacks=[early_stopping, early_stopping_val], batch_size=batch_size, epochs=epochs)\n",
    "    evaluate(model, train_inputs, train_outputs, test_inputs, test_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n",
      "474\n"
     ]
    }
   ],
   "source": [
    "# Calculates median user review length and item length. We then pad each review to these numbers\n",
    "ptile = 50\n",
    "u_seq_len = int(np.percentile(np.array(extra_info['user_seq_sizes']), ptile))\n",
    "i_seq_len = int(np.percentile(np.array(extra_info['item_seq_sizes']), ptile))\n",
    "print(u_seq_len)\n",
    "print(i_seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour some sugar on meeee...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 141, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 474, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 134, 2)       802         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 467, 2)       802         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 67, 2)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 233, 2)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 134)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 466)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            540         flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            1868        flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8)            0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            9           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1)            0           dense_6[0][0]                    \n",
      "                                                                 dot_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,021\n",
      "Trainable params: 4,021\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 14850 samples, validate on 3713 samples\n",
      "Epoch 1/20\n",
      "14850/14850 [==============================] - 28s 2ms/step - loss: 2.8514 - val_loss: 1.4843\n",
      "Epoch 2/20\n",
      "14850/14850 [==============================] - 26s 2ms/step - loss: 1.2567 - val_loss: 1.3161\n",
      "Epoch 3/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 1.1255 - val_loss: 1.2574\n",
      "Epoch 4/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 1.0733 - val_loss: 1.1903\n",
      "Epoch 5/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 1.0467 - val_loss: 1.2020\n",
      "Epoch 6/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 1.0159 - val_loss: 1.1649\n",
      "Epoch 7/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 1.0044 - val_loss: 1.2503\n",
      "Epoch 8/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 0.9856 - val_loss: 1.1878\n",
      "Epoch 9/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 0.9727 - val_loss: 1.2479\n",
      "Epoch 10/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 0.9552 - val_loss: 1.1499\n",
      "Epoch 11/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 0.9601 - val_loss: 1.2380\n",
      "Epoch 12/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 0.9454 - val_loss: 1.2083\n",
      "Epoch 13/20\n",
      "14850/14850 [==============================] - 24s 2ms/step - loss: 0.9323 - val_loss: 1.1751\n",
      "Epoch 14/20\n",
      "14850/14850 [==============================] - 25s 2ms/step - loss: 0.9269 - val_loss: 1.1523\n",
      "Epoch 15/20\n",
      "14850/14850 [==============================] - 24s 2ms/step - loss: 0.9230 - val_loss: 1.1428\n",
      "Epoch 16/20\n",
      "14850/14850 [==============================] - 24s 2ms/step - loss: 0.9162 - val_loss: 1.1405\n",
      "Epoch 17/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 0.9118 - val_loss: 1.1974\n",
      "Epoch 18/20\n",
      "14850/14850 [==============================] - 27s 2ms/step - loss: 0.9045 - val_loss: 1.1809\n",
      "Epoch 19/20\n",
      "14850/14850 [==============================] - 29s 2ms/step - loss: 0.9005 - val_loss: 1.2728\n",
      "Epoch 20/20\n",
      "14850/14850 [==============================] - 28s 2ms/step - loss: 0.8900 - val_loss: 1.1332\n",
      "Printing a few test outputs...\n",
      " [ 1.  5.  5.  5.  5.  4.  5.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.\n",
      "  5.  5.]\n",
      "\n",
      "baseline train mse: 1.26453680572\n",
      "model train mse: 0.909241389228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "build_DeepcoNN(matUserInputData, matMovieInputData, ratingsData, \n",
    "               u_seq_len=u_seq_len, i_seq_len=i_seq_len, hidden_size=4, \n",
    "               epochs=20, training=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_tf",
   "language": "python",
   "name": "dl_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
