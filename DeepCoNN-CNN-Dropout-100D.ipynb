{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/michael/Deep-Learning/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions we implemented\n",
    "from custom_functions import init_embeddings_map, get_embed_and_pad_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 100\n",
    "embedding_map = init_embeddings_map(\"glove.6B.\" + str(emb_size) + \"d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/unembedded_grouped_cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split for our model is unique, we need to hold out a\n",
    "# set of users and movies so that our network never learns those \n",
    "test_size = 0.005\n",
    "\n",
    "# get test_size percentage of users\n",
    "unique_users = raw_data.loc[:, \"reviewerID\"].unique()\n",
    "users_size = len(unique_users)\n",
    "test_idx = np.random.choice(users_size,\n",
    "                              size=int(users_size * test_size),\n",
    "                              replace=False)\n",
    "\n",
    "# get test users\n",
    "test_users = unique_users[test_idx]\n",
    "\n",
    "# everyone else is a training user\n",
    "train_users = np.delete(unique_users, test_idx)\n",
    "\n",
    "test = raw_data[raw_data[\"reviewerID\"].isin(test_users)]\n",
    "train = raw_data[raw_data[\"reviewerID\"].isin(train_users)]\n",
    "\n",
    "unique_test_movies = test[\"asin\"].unique()\n",
    "\n",
    "# drop the movies that also appear in our test set. In order to be\n",
    "# a true train/test split, we are forced to discard some data entirely\n",
    "train = train.where(np.logical_not(train[\"asin\"].isin(unique_test_movies))).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_seq_sizes = raw_data.loc[:, \"userReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "item_seq_sizes = raw_data.loc[:, \"movieReviews\"].apply(lambda x: x.split()).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_ptile = 40\n",
    "i_ptile = 15\n",
    "u_seq_len = int(np.percentile(user_seq_sizes, u_ptile))\n",
    "i_seq_len = int(np.percentile(item_seq_sizes, i_ptile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1405c8e2634c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membedding_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embed_and_pad_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0memb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Deep-Learning/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4852\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4853\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4854\u001b[0;31m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[1;32m   4855\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4856\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Deep-Learning/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4948\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4949\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4950\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4951\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4952\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Deep-Learning/custom_functions.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieReviews\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_seq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         reviews = list(map(lambda word: embedding_map.get(word)\n\u001b[0;32m---> 20\u001b[0;31m             if word in embedding_map else pad_value, sentence))\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieReviews\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mpad_value\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi_seq_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Deep-Learning/custom_functions.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieReviews\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_seq_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         reviews = list(map(lambda word: embedding_map.get(word)\n\u001b[0;32m---> 20\u001b[0;31m             if word in embedding_map else pad_value, sentence))\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"movieReviews\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mpad_value\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi_seq_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_fn = get_embed_and_pad_func(i_seq_len, u_seq_len, np.array([0.0] * emb_size), embedding_map)\n",
    "    \n",
    "train_embedded = train.apply(embedding_fn, axis=1)\n",
    "test_embedded = test.apply(embedding_fn, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCoNN Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling imports\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.merge import Add, Dot, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoNN():\n",
    "    def __init__(self,\n",
    "                 embedding_size,\n",
    "                 hidden_size,\n",
    "                 u_seq_len,\n",
    "                 m_seq_len,\n",
    "                 filters=2,\n",
    "                 kernel_size=10,\n",
    "                 strides=6):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.inputU, self.towerU = self.create_deepconn_tower(u_seq_len)\n",
    "        self.inputM, self.towerM = self.create_deepconn_tower(m_seq_len)\n",
    "        self.joined = Concatenate()([self.towerU, self.towerM])\n",
    "        self.outNeuron = Dense(1)(self.joined)\n",
    "\n",
    "    def create_deepconn_tower(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        tower = Conv1D(filters=self.filters,\n",
    "                       kernel_size=self.kernel_size,\n",
    "                       activation=\"tanh\")(input_layer)\n",
    "        tower = Dropout(0.1)(tower)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Flatten()(tower)\n",
    "        tower = Dense(self.hidden_size, activation=\"relu\")(tower)\n",
    "        return input_layer, tower\n",
    "\n",
    "    def create_deepconn_dp(self):\n",
    "        dotproduct = Dot(axes=1)([self.towerU, self.towerM])\n",
    "        output = Add()([self.outNeuron, dotproduct])\n",
    "        self.model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
    "        self.model.compile(optimizer='Adam', loss='mse')\n",
    "        \n",
    "    def train(self, train_data, batch_size, epochs=3500):\n",
    "        tensorboard = TensorBoard(log_dir=\"tf_logs/{}\".format(pd.Timestamp(int(time()), unit=\"s\")))\n",
    "        self.create_deepconn_dp()\n",
    "        print(self.model.summary())\n",
    "        \n",
    "        user_reviews = np.array(list(train_data.loc[:, \"userReviews\"]))\n",
    "        movie_reviews = np.array(list(train_data.loc[:, \"movieReviews\"]))\n",
    "\n",
    "        self.train_inputs = [user_reviews, movie_reviews]\n",
    "        self.train_outputs = train_data.loc[:, \"overall\"]\n",
    "        \n",
    "        self.history = self.model.fit(self.train_inputs,\n",
    "                                      self.train_outputs,\n",
    "                                      callbacks=[tensorboard],\n",
    "                                      validation_split=0.05,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=epochs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "deepconn = DeepCoNN(emb_size, hidden_size, u_seq_len, i_seq_len)\n",
    "\n",
    "batch_size = 32\n",
    "deepconn.train(train_embedded, batch_size, epochs=1)\n",
    "\n",
    "# deepconn.model.save(\"cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews = np.array(list(test_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(test_embedded.loc[:, \"movieReviews\"]))\n",
    "\n",
    "test_inputs = [user_reviews, movie_reviews]\n",
    "\n",
    "true_rating = np.array(list(test_embedded.loc[:, \"overall\"])).reshape((-1, 1))\n",
    "\n",
    "predictions = deepconn.model.predict(test_inputs)\n",
    "\n",
    "error = np.absolute(predictions - true_rating)\n",
    "\n",
    "print(\"average test error (not the squared loss):\", np.average(error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
