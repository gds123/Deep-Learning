(venv) michael@dl-final-project:~/Deep-Learning$ python DeepCoNN-GRU-Dropout-100D.py
Using TensorFlow backend.
/home/michael/Deep-Learning/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compileti
me version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime versio
n 3.6
  return f(*args, **kwds)
  __________________________________________________________________________________________________
  Layer (type)                    Output Shape         Param #     Connected to
  ==================================================================================================
  input_1 (InputLayer)            (None, 243, 100)     0
  __________________________________________________________________________________________________
  input_2 (InputLayer)            (None, 736, 100)     0
  __________________________________________________________________________________________________
  gru_1 (GRU)                     (None, 64)           31680       input_1[0][0]
  __________________________________________________________________________________________________
  gru_2 (GRU)                     (None, 64)           31680       input_2[0][0]
  __________________________________________________________________________________________________
  dropout_1 (Dropout)             (None, 64)           0           gru_1[0][0]
  __________________________________________________________________________________________________
  dropout_2 (Dropout)             (None, 64)           0           gru_2[0][0]
  __________________________________________________________________________________________________
  dense_1 (Dense)                 (None, 64)           4160        dropout_1[0][0]
  __________________________________________________________________________________________________
  dense_2 (Dense)                 (None, 64)           4160        dropout_2[0][0]
  __________________________________________________________________________________________________
  concatenate_1 (Concatenate)     (None, 128)          0           dense_1[0][0]
                                                                   dense_2[0][0]
                                                                   __________________________________________________________________________________________________
                                                                   dense_3 (Dense)                 (None, 1)            129         concatenate_1[0][0]
                                                                   __________________________________________________________________________________________________
                                                                   dot_1 (Dot)                     (None, 1)            0           dense_1[0][0]
                                                                                                                                    dense_2[0][0]
                                                                                                                                    __________________________________________________________________________________________________
                                                                                                                                    add_1 (Add)                     (None, 1)            0           dense_3[0][0]
                                                                                                                                                                                                     dot_1[0][0]
                                                                                                                                                                                                     ==================================================================================================
                                                                                                                                                                                                     Total params: 71,809
                                                                                                                                                                                                     Trainable params: 71,809
                                                                                                                                                                                                     Non-trainable params: 0
                                                                                                                                                                                                     __________________________________________________________________________________________________
                                                                                                                                                                                                     None
                                                                                                                                                                                                     Train on 24462 samples, validate on 1288 samples
                                                                                                                                                                                                     2017-12-15 06:39:30.421773: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports in
                                                                                                                                                                                                     structions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
                                                                                                                                                                                                     Epoch 1/50
                                                                                                                                                                                                     24462/24462 [==============================] - 311s 13ms/step - loss: 16.0946 - val_loss: 12.2297
                                                                                                                                                                                                     Epoch 2/50
                                                                                                                                                                                                     24462/24462 [==============================] - 305s 12ms/step - loss: 11.1715 - val_loss: 8.2967
                                                                                                                                                                                                     Epoch 3/50
                                                                                                                                                                                                     24462/24462 [==============================] - 304s 12ms/step - loss: 7.5209 - val_loss: 5.4903
                                                                                                                                                                                                     Epoch 4/50
                                                                                                                                                                                                     24462/24462 [==============================] - 306s 12ms/step - loss: 4.9132 - val_loss: 3.5993
                                                                                                                                                                                                     Epoch 5/50
                                                                                                                                                                                                     24462/24462 [==============================] - 306s 13ms/step - loss: 3.1584 - val_loss: 2.4452
Epoch 6/50
24462/24462 [==============================] - 303s 12ms/step - loss: 2.0849 - val_loss: 1.8537
Epoch 7/50
24462/24462 [==============================] - 306s 12ms/step - loss: 1.5223 - val_loss: 1.6414
Epoch 8/50
24462/24462 [==============================] - 309s 13ms/step - loss: 1.2926 - val_loss: 1.6227
Epoch 9/50
24462/24462 [==============================] - 309s 13ms/step - loss: 1.2289 - val_loss: 1.6511
Epoch 10/50
24462/24462 [==============================] - 317s 13ms/step - loss: 1.2188 - val_loss: 1.6692
Epoch 11/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2182 - val_loss: 1.6686
Epoch 12/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2183 - val_loss: 1.6690
Epoch 13/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2183 - val_loss: 1.6693
Epoch 14/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2182 - val_loss: 1.6775
Epoch 15/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2182 - val_loss: 1.6718
Epoch 16/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2182 - val_loss: 1.6695
Epoch 17/50
24462/24462 [==============================] - 325s 13ms/step - loss: 1.2183 - val_loss: 1.6745
Epoch 18/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2182 - val_loss: 1.6704
Epoch 19/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2182 - val_loss: 1.6717
Epoch 20/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2183 - val_loss: 1.6709
Epoch 21/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2183 - val_loss: 1.6731
Epoch 22/50
24462/24462 [==============================] - 323s 13ms/step - loss: 1.2182 - val_loss: 1.6773
Epoch 23/50
24462/24462 [==============================] - 326s 13ms/step - loss: 1.2183 - val_loss: 1.6735
Epoch 24/50
24462/24462 [==============================] - 325s 13ms/step - loss: 1.2182 - val_loss: 1.6694
Epoch 25/50
24462/24462 [==============================] - 325s 13ms/step - loss: 1.2183 - val_loss: 1.6706
Epoch 26/50
24462/24462 [==============================] - 325s 13ms/step - loss: 1.2182 - val_loss: 1.6678
Epoch 27/50
24462/24462 [==============================] - 325s 13ms/step - loss: 1.2183 - val_loss: 1.6732
Epoch 28/50
24462/24462 [==============================] - 325s 13ms/step - loss: 1.2182 - val_loss: 1.6660
Epoch 29/50
24462/24462 [==============================] - 324s 13ms/step - loss: 1.2183 - val_loss: 1.6702
Epoch 30/50
24462/24462 [==============================] - 314s 13ms/step - loss: 1.2182 - val_loss: 1.6754
Epoch 31/50
24462/24462 [==============================] - 311s 13ms/step - loss: 1.2182 - val_loss: 1.6691
Epoch 32/50
24462/24462 [==============================] - 312s 13ms/step - loss: 1.2182 - val_loss: 1.6733
Epoch 33/50
24462/24462 [==============================] - 313s 13ms/step - loss: 1.2183 - val_loss: 1.6729
Epoch 34/50
24462/24462 [==============================] - 315s 13ms/step - loss: 1.2183 - val_loss: 1.6727
Epoch 35/50
24462/24462 [==============================] - 314s 13ms/step - loss: 1.2182 - val_loss: 1.6650
Epoch 36/50
24462/24462 [==============================] - 314s 13ms/step - loss: 1.2183 - val_loss: 1.6711
Epoch 37/50
24462/24462 [==============================] - 314s 13ms/step - loss: 1.2182 - val_loss: 1.6740
Epoch 38/50
24462/24462 [==============================] - 313s 13ms/step - loss: 1.2182 - val_loss: 1.6751
Epoch 39/50
24462/24462 [==============================] - 315s 13ms/step - loss: 1.2182 - val_loss: 1.6774
Epoch 40/50
24462/24462 [==============================] - 312s 13ms/step - loss: 1.2183 - val_loss: 1.6759
Epoch 41/50
24462/24462 [==============================] - 313s 13ms/step - loss: 1.2183 - val_loss: 1.6742
Epoch 42/50
24462/24462 [==============================] - 316s 13ms/step - loss: 1.2183 - val_loss: 1.6741
Epoch 43/50
24462/24462 [==============================] - 316s 13ms/step - loss: 1.2182 - val_loss: 1.6685
Epoch 44/50
24462/24462 [==============================] - 318s 13ms/step - loss: 1.2183 - val_loss: 1.6702
Epoch 45/50
24462/24462 [==============================] - 316s 13ms/step - loss: 1.2183 - val_loss: 1.6746
Epoch 46/50
24462/24462 [==============================] - 315s 13ms/step - loss: 1.2182 - val_loss: 1.6706
Epoch 47/50
24462/24462 [==============================] - 315s 13ms/step - loss: 1.2182 - val_loss: 1.6748
Epoch 48/50
24462/24462 [==============================] - 315s 13ms/step - loss: 1.2182 - val_loss: 1.6700
Epoch 49/50
24462/24462 [==============================] - 315s 13ms/step - loss: 1.2182 - val_loss: 1.6717
Epoch 50/50
24462/24462 [==============================] - 313s 13ms/step - loss: 1.2182 - val_loss: 1.6674
average test error (not the squared loss): 0.790328793409

