{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/michael/Documents/deep_learning/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import scipy.spatial\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def initEmbeddingMap(fileName):\n",
    "    print(\"initializing embeddings\")\n",
    "    with open(join(\"data\", \"glove.6B\", fileName)) as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in [line.split() for line in glove]}\n",
    "\n",
    "def clean(text):\n",
    "    return text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                 lower=True, split=\" \")\n",
    "\n",
    "\n",
    "# returns dict with users and movies they rated as repeated rows\n",
    "# cleans review text and add to rawOutput\n",
    "def initRawData(input_file):\n",
    "    print(\"initializing raw data\")\n",
    "    rawInputData = []\n",
    "    rawOutputData = []\n",
    "    with open(input_file,\"r\") as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            if len(line) < 4:\n",
    "                break\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj[\"reviewerID\"]\n",
    "            item = lineObj[\"asin\"]\n",
    "            rawInputDataObj = {\"user\": user, \"asin\": item}\n",
    "            rawOutputDataObj = clean(lineObj[\"reviewText\"])\n",
    "            rawInputData.append(rawInputDataObj)\n",
    "            rawOutputData.append(rawOutputDataObj)\n",
    "    return rawInputData, rawOutputData\n",
    "\n",
    "# creates dict of usrs w/ all movies rated + movies w/ all user ratings *** \n",
    "def group_data(inputData):\n",
    "    users = {}\n",
    "    items = {}\n",
    "    for datum in inputData:\n",
    "        u = datum[\"user\"]\n",
    "        i = datum[\"asin\"]\n",
    "        users.setdefault(u, []).append(i)\n",
    "        items.setdefault(i, []).append(u)\n",
    "    return users, items\n",
    "\n",
    "def getSetFromData(key, data):\n",
    "    return set([datum.get(key) for datum in data])\n",
    "\n",
    "def seq_2_matrix(sequence, embedding_map):\n",
    "    return np.array([embedding_map.get(word) for word in sequence if word in embedding_map])\n",
    "\n",
    "def matrix_2_avg(emb_matrix):\n",
    "    return np.mean(emb_matrix, 0)\n",
    "\n",
    "\n",
    "# utils - one hot encodes all data \n",
    "def initVecData(rawInputData, rawOutputData, embedding_map):\n",
    "    print('initializing vectorized data')\n",
    "    dictVect = DictVectorizer()\n",
    "    vecInputData = dictVect.fit_transform(rawInputData).toarray()\n",
    "    vecOutputData = [matrix_2_avg(seq_2_matrix(review, embedding_map)) for review in rawOutputData]\n",
    "    return vecInputData, vecOutputData\n",
    "\n",
    "def initMatInputData(rawInputData, rawOutputData, embedding_map, save=False):\n",
    "    print('initializing matrix data')\n",
    "    if len(rawInputData) != len(rawOutputData):\n",
    "        raise ValueError(\"Need same size of input and output\")\n",
    "    users = {}\n",
    "    extra_info = {}\n",
    "    items = {}\n",
    "    dictVect = DictVectorizer()\n",
    "    for i in range(len(rawInputData)):\n",
    "        vecOutput = seq_2_matrix(rawOutputData[i], embedding_map)\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        users.setdefault(user, []).append(vecOutput)\n",
    "        items.setdefault(item, []).append(vecOutput)\n",
    "        \n",
    "    matUserInputData = []\n",
    "    matItemInputData = []\n",
    "    users = {k: np.vstack(v) for k, v in users.items()}\n",
    "    items = {k: np.vstack(v) for k, v in items.items()}\n",
    "    extra_info['user_seq_sizes'] = [m.shape[0] for m in users.values()]\n",
    "    extra_info['item_seq_sizes'] = [m.shape[0] for m in items.values()]\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        matUserInputData.append(users.get(user))\n",
    "        matItemInputData.append(items.get(item))\n",
    "    return matUserInputData, matItemInputData, extra_info\n",
    "\n",
    "def toKey(user, item):\n",
    "    return (user, item)\n",
    "\n",
    "def initRatingsOutputData(rawInputData, input_file, save=False):\n",
    "    ratingsData = []\n",
    "    userItemDict = {}\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        userItem = toKey(rawInput['user'], rawInput['asin'])\n",
    "        userItemDict[userItem] = i\n",
    "        ratingsData.append(None) # check later to make sure no Nones left\n",
    "        \n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rating = lineObj['overall']\n",
    "            i = userItemDict.get(toKey(user, item))\n",
    "            if i is not None:\n",
    "                ratingsData[i] = rating\n",
    "        failure = None in ratingsData\n",
    "        if failure:\n",
    "            raise ValueError(str(len([r for r in ratingsData if r is None])) + \" reviews did not have corresponding rating.\")\n",
    "    return ratingsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing raw data\n"
     ]
    }
   ],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "rawInputData, rawOutputData = initRawData(input_file=fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, movies = group_data(rawInputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idxs = np.random.permutation(len(rawOutputData))\n",
    "rawInputData = [rawInputData[i] for i in rand_idxs]\n",
    "rawOutputData = [rawOutputData[i] for i in rand_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing embeddings\n"
     ]
    }
   ],
   "source": [
    "embedding_map = initEmbeddingMap(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing vectorized data\n"
     ]
    }
   ],
   "source": [
    "all_users = getSetFromData('user', rawInputData)\n",
    "all_movies = getSetFromData('asin', rawInputData)\n",
    "vecInputData, vecOutputData = initVecData(rawInputData, rawOutputData, embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing matrix data\n"
     ]
    }
   ],
   "source": [
    "matUserInputData, matMovieInputData, extra_info = initMatInputData(rawInputData, rawOutputData, embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "ratingsData = initRatingsOutputData(rawInputData, input_file=fileName,save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCoNN Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Dot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU\n",
    "from keras.layers.merge import Add, Dot, Concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoNN():\n",
    "    def __init__(self, embedding_size, hidden_size, rnn_hidden_size, u_seq_len, m_seq_len, filters=2, kernel_size=8,\n",
    "                 strides=6):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.inputU, self.towerU = self.create_deepconn_tower(u_seq_len)\n",
    "        self.inputM, self.towerM = self.create_deepconn_tower(m_seq_len)\n",
    "        self.joined = Concatenate()([self.towerU, self.towerM])\n",
    "        self.outNeuron = Dense(1)(self.joined)\n",
    "\n",
    "    def create_deepconn_tower(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        tower = GRU(self.rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(self.hidden_size, activation=\"relu\")(tower)\n",
    "        return input_layer, tower\n",
    "\n",
    "    def create_deepconn_dp(self):\n",
    "        dotproduct = Dot(axes=1)([self.towerU, self.towerM])\n",
    "        output = Add()([self.outNeuron, dotproduct])\n",
    "        self.model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
    "        self.model.compile(optimizer='Adam', loss='mse')\n",
    "        \n",
    "    def train(self, matUserInputData, matItemInputData, ratingsData, u_seq_len=200, i_seq_len=200, epochs=3500, training=None):\n",
    "        self.create_deepconn_dp()\n",
    "        \n",
    "        self.user_input = pad_sequences(np.asarray(matUserInputData), maxlen=u_seq_len)\n",
    "        self.item_input = pad_sequences(np.asarray(matItemInputData), maxlen=i_seq_len)\n",
    "\n",
    "        self.trainingN = int(len(user_input) * training) if type(training) is float else training\n",
    "\n",
    "        self.outputs = np.asarray(ratingsData)\n",
    "        print(self.model.summary())\n",
    "\n",
    "        self.train_inputs = [self.user_input[:self.trainingN], self.item_input[:self.trainingN]]\n",
    "        self.train_outputs = self.outputs[:self.trainingN]\n",
    "        self.test_inputs = [self.user_input[self.trainingN:], self.item_input[self.trainingN:]]\n",
    "        self.test_outputs = self.outputs[self.trainingN:]\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=4)\n",
    "        early_stopping_val = EarlyStopping(monitor='val_loss', patience=6)\n",
    "        \n",
    "        batch_size = 32\n",
    "        \n",
    "        self.history = self.model.fit(self.train_inputs, self.train_outputs, callbacks=[early_stopping, early_stopping_val], validation_split=0.2, batch_size=batch_size, epochs=epochs)\n",
    "        self.predicts = self.model.predict(self.test_inputs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 141, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 474, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 64)           22080       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 64)           22080       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            260         gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            260         gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            9           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dense_3[0][0]                    \n",
      "                                                                 dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 44,689\n",
      "Trainable params: 44,689\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 14850 samples, validate on 3713 samples\n",
      "Epoch 1/20\n",
      "14850/14850 [==============================] - 122s 8ms/step - loss: 1.7801 - val_loss: 1.1730\n",
      "Epoch 2/20\n",
      "14850/14850 [==============================] - 120s 8ms/step - loss: 1.1178 - val_loss: 1.0800\n",
      "Epoch 3/20\n",
      "14850/14850 [==============================] - 127s 9ms/step - loss: 1.0530 - val_loss: 1.1090\n",
      "Epoch 4/20\n",
      "14850/14850 [==============================] - 121s 8ms/step - loss: 1.0058 - val_loss: 1.0188\n",
      "Epoch 5/20\n",
      "14850/14850 [==============================] - 131s 9ms/step - loss: 0.9762 - val_loss: 1.0115\n",
      "Epoch 6/20\n",
      "14850/14850 [==============================] - 121s 8ms/step - loss: 0.9516 - val_loss: 1.0296\n",
      "Epoch 7/20\n",
      "14850/14850 [==============================] - 125s 8ms/step - loss: 0.9289 - val_loss: 1.0941\n",
      "Epoch 8/20\n",
      "14850/14850 [==============================] - 124s 8ms/step - loss: 0.9013 - val_loss: 0.9985\n",
      "Epoch 9/20\n",
      "14850/14850 [==============================] - 116s 8ms/step - loss: 0.8788 - val_loss: 1.0430\n",
      "Epoch 10/20\n",
      "14850/14850 [==============================] - 121s 8ms/step - loss: 0.8598 - val_loss: 1.0323\n",
      "Epoch 11/20\n",
      "14850/14850 [==============================] - 119s 8ms/step - loss: 0.8404 - val_loss: 1.0944\n",
      "Epoch 12/20\n",
      "14850/14850 [==============================] - 117s 8ms/step - loss: 0.8167 - val_loss: 0.9982\n",
      "Epoch 13/20\n",
      "14850/14850 [==============================] - 120s 8ms/step - loss: 0.8025 - val_loss: 1.0166\n",
      "Epoch 14/20\n",
      "14850/14850 [==============================] - 136s 9ms/step - loss: 0.7778 - val_loss: 0.9926\n",
      "Epoch 15/20\n",
      "14850/14850 [==============================] - 118s 8ms/step - loss: 0.7641 - val_loss: 1.0057\n",
      "Epoch 16/20\n",
      "14850/14850 [==============================] - 122s 8ms/step - loss: 0.7410 - val_loss: 1.0004\n",
      "Epoch 17/20\n",
      "14850/14850 [==============================] - 117s 8ms/step - loss: 0.7262 - val_loss: 1.0028\n",
      "Epoch 18/20\n",
      "14850/14850 [==============================] - 122s 8ms/step - loss: 0.7146 - val_loss: 1.0465\n",
      "Epoch 19/20\n",
      "14850/14850 [==============================] - 118s 8ms/step - loss: 0.6999 - val_loss: 1.0102\n",
      "Epoch 20/20\n",
      "14850/14850 [==============================] - 117s 8ms/step - loss: 0.6834 - val_loss: 1.0314\n"
     ]
    }
   ],
   "source": [
    "# Calculates median user review length and item length. We then pad each review to these numbers\n",
    "ptile = 50\n",
    "u_seq_len = int(np.percentile(np.array(extra_info['user_seq_sizes']), ptile))\n",
    "i_seq_len = int(np.percentile(np.array(extra_info['item_seq_sizes']), ptile))\n",
    "embed_dims = matUserInputData[0].shape[1]\n",
    "hidden_size = 4\n",
    "rnn_hidden_size = 64\n",
    "deepconn = DeepCoNN(embed_dims, hidden_size, rnn_hidden_size, u_seq_len, i_seq_len)\n",
    "\n",
    "deepconn.train(matUserInputData, matMovieInputData, ratingsData, \n",
    "           u_seq_len=u_seq_len, i_seq_len=i_seq_len,\n",
    "           epochs=20, training=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.780106227069792,\n",
       "  1.1177633499055599,\n",
       "  1.0530408533735307,\n",
       "  1.0057512607257375,\n",
       "  0.9761646201714923,\n",
       "  0.95157362563441494,\n",
       "  0.92890582876574712,\n",
       "  0.90134774475386648,\n",
       "  0.87880379117297802,\n",
       "  0.85976035755993141,\n",
       "  0.84037195849900292,\n",
       "  0.81669084581060447,\n",
       "  0.80249396690415209,\n",
       "  0.77784146817846334,\n",
       "  0.76408807398293555,\n",
       "  0.74098415178883359,\n",
       "  0.72615296420023501,\n",
       "  0.71464120830274391,\n",
       "  0.69986240770881025,\n",
       "  0.68341696423713605],\n",
       " 'val_loss': [1.1729773180506604,\n",
       "  1.079968953844779,\n",
       "  1.1089615221156639,\n",
       "  1.0187626074439724,\n",
       "  1.0115435915030169,\n",
       "  1.0295507333563967,\n",
       "  1.0941389608227015,\n",
       "  0.99854102752823859,\n",
       "  1.0429756717669605,\n",
       "  1.0323311419735033,\n",
       "  1.0944018455745006,\n",
       "  0.99816843005458589,\n",
       "  1.016575960949597,\n",
       "  0.99257765334915227,\n",
       "  1.0056642256406256,\n",
       "  1.0003933426330842,\n",
       "  1.0027686385791512,\n",
       "  1.0464854605562148,\n",
       "  1.0101629993001662,\n",
       "  1.0313575438705618]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(\"data/run_data/gru_50dEmb_50ptile.npy\", deepconn.history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
