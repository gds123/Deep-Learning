{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/michael/Documents/deep_learning/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import scipy.spatial\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def initEmbeddingMap(fileName):\n",
    "    print(\"initializing embeddings\")\n",
    "    with open(join(\"data\", \"glove.6B\", fileName)) as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in [line.split() for line in glove]}\n",
    "\n",
    "def clean(text):\n",
    "    return text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                 lower=True, split=\" \")\n",
    "\n",
    "\n",
    "# returns dict with users and movies they rated as repeated rows\n",
    "# cleans review text and add to rawOutput\n",
    "def initRawData(input_file):\n",
    "    print(\"initializing raw data\")\n",
    "    rawInputData = []\n",
    "    rawOutputData = []\n",
    "    with open(input_file,\"r\") as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            if len(line) < 4:\n",
    "                break\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj[\"reviewerID\"]\n",
    "            item = lineObj[\"asin\"]\n",
    "            rawInputDataObj = {\"user\": user, \"asin\": item}\n",
    "            rawOutputDataObj = clean(lineObj[\"reviewText\"])\n",
    "            rawInputData.append(rawInputDataObj)\n",
    "            rawOutputData.append(rawOutputDataObj)\n",
    "    return rawInputData, rawOutputData\n",
    "\n",
    "# creates dict of usrs w/ all movies rated + movies w/ all user ratings *** \n",
    "def group_data(inputData):\n",
    "    users = {}\n",
    "    items = {}\n",
    "    for datum in inputData:\n",
    "        u = datum[\"user\"]\n",
    "        i = datum[\"asin\"]\n",
    "        users.setdefault(u, []).append(i)\n",
    "        items.setdefault(i, []).append(u)\n",
    "    return users, items\n",
    "\n",
    "def getSetFromData(key, data):\n",
    "    return set([datum.get(key) for datum in data])\n",
    "\n",
    "def seq_2_matrix(sequence, embedding_map):\n",
    "    return np.array([embedding_map.get(word) for word in sequence if word in embedding_map])\n",
    "\n",
    "def matrix_2_avg(emb_matrix):\n",
    "    return np.mean(emb_matrix, 0)\n",
    "\n",
    "\n",
    "# utils - one hot encodes all data \n",
    "def initVecData(rawInputData, rawOutputData, embedding_map):\n",
    "    print('initializing vectorized data')\n",
    "    dictVect = DictVectorizer()\n",
    "    vecInputData = dictVect.fit_transform(rawInputData).toarray()\n",
    "    vecOutputData = [matrix_2_avg(seq_2_matrix(review, embedding_map)) for review in rawOutputData]\n",
    "    return vecInputData, vecOutputData\n",
    "\n",
    "def initMatInputData(rawInputData, rawOutputData, embedding_map, save=False):\n",
    "    print('initializing matrix data')\n",
    "    if len(rawInputData) != len(rawOutputData):\n",
    "        raise ValueError(\"Need same size of input and output\")\n",
    "    users = {}\n",
    "    extra_info = {}\n",
    "    items = {}\n",
    "    dictVect = DictVectorizer()\n",
    "    for i in range(len(rawInputData)):\n",
    "        vecOutput = seq_2_matrix(rawOutputData[i], embedding_map)\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        users.setdefault(user, []).append(vecOutput)\n",
    "        items.setdefault(item, []).append(vecOutput)\n",
    "        \n",
    "    matUserInputData = []\n",
    "    matItemInputData = []\n",
    "    users = {k: np.vstack(v) for k, v in users.items()}\n",
    "    items = {k: np.vstack(v) for k, v in items.items()}\n",
    "    extra_info['user_seq_sizes'] = [m.shape[0] for m in users.values()]\n",
    "    extra_info['item_seq_sizes'] = [m.shape[0] for m in items.values()]\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        matUserInputData.append(users.get(user))\n",
    "        matItemInputData.append(items.get(item))\n",
    "    return matUserInputData, matItemInputData, extra_info\n",
    "\n",
    "def toKey(user, item):\n",
    "    return (user, item)\n",
    "\n",
    "def initRatingsOutputData(rawInputData, input_file, save=False):\n",
    "    ratingsData = []\n",
    "    userItemDict = {}\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        userItem = toKey(rawInput['user'], rawInput['asin'])\n",
    "        userItemDict[userItem] = i\n",
    "        ratingsData.append(None) # check later to make sure no Nones left\n",
    "        \n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rating = lineObj['overall']\n",
    "            i = userItemDict.get(toKey(user, item))\n",
    "            if i is not None:\n",
    "                ratingsData[i] = rating\n",
    "        failure = None in ratingsData\n",
    "        if failure:\n",
    "            raise ValueError(str(len([r for r in ratingsData if r is None])) + \" reviews did not have corresponding rating.\")\n",
    "    return ratingsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing raw data\n"
     ]
    }
   ],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "rawInputData, rawOutputData = initRawData(input_file=fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, movies = group_data(rawInputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idxs = np.random.permutation(len(rawOutputData))\n",
    "rawInputData = [rawInputData[i] for i in rand_idxs]\n",
    "rawOutputData = [rawOutputData[i] for i in rand_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing embeddings\n"
     ]
    }
   ],
   "source": [
    "embedding_map = initEmbeddingMap(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing vectorized data\n"
     ]
    }
   ],
   "source": [
    "all_users = getSetFromData('user', rawInputData)\n",
    "all_movies = getSetFromData('asin', rawInputData)\n",
    "vecInputData, vecOutputData = initVecData(rawInputData, rawOutputData, embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing matrix data\n"
     ]
    }
   ],
   "source": [
    "matUserInputData, matMovieInputData, extra_info = initMatInputData(rawInputData, rawOutputData, embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "ratingsData = initRatingsOutputData(rawInputData, input_file=fileName,save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCoNN Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Dot\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.merge import Add, Dot, Concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoNN():\n",
    "    def __init__(self, embedding_size, hidden_size, rnn_hidden_size, u_seq_len, m_seq_len, filters=2, kernel_size=8,\n",
    "                 strides=6):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.inputU, self.towerU = self.create_deepconn_tower(u_seq_len)\n",
    "        self.inputM, self.towerM = self.create_deepconn_tower(m_seq_len)\n",
    "        self.joined = Concatenate()([self.towerU, self.towerM])\n",
    "        self.outNeuron = Dense(1)(self.joined)\n",
    "\n",
    "    def create_deepconn_tower(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        tower = LSTM(self.rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(self.hidden_size, activation=\"relu\")(tower)\n",
    "        return input_layer, tower\n",
    "\n",
    "    def create_deepconn_dp(self):\n",
    "        dotproduct = Dot(axes=1)([self.towerU, self.towerM])\n",
    "        output = Add()([self.outNeuron, dotproduct])\n",
    "        self.model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
    "        self.model.compile(optimizer='Adam', loss='mse')\n",
    "        \n",
    "    def train(self, matUserInputData, matItemInputData, ratingsData, u_seq_len=200, i_seq_len=200, epochs=3500, training=None):\n",
    "        self.create_deepconn_dp()\n",
    "        tensorboard = TensorBoard(log_dir=\"tf_logs/{}\".format(time()))\n",
    "        self.user_input = pad_sequences(np.asarray(matUserInputData), maxlen=u_seq_len)\n",
    "        self.item_input = pad_sequences(np.asarray(matItemInputData), maxlen=i_seq_len)\n",
    "\n",
    "        self.trainingN = int(len(user_input) * training) if type(training) is float else training\n",
    "\n",
    "        self.outputs = np.asarray(ratingsData)\n",
    "        print(self.model.summary())\n",
    "\n",
    "        self.train_inputs = [self.user_input[:self.trainingN], self.item_input[:self.trainingN]]\n",
    "        self.train_outputs = self.outputs[:self.trainingN]\n",
    "        self.test_inputs = [self.user_input[self.trainingN:], self.item_input[self.trainingN:]]\n",
    "        self.test_outputs = self.outputs[self.trainingN:]\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=4)\n",
    "        early_stopping_val = EarlyStopping(monitor='val_loss', patience=6)\n",
    "        \n",
    "        batch_size = 32\n",
    "        \n",
    "        self.history = self.model.fit(self.train_inputs, self.train_outputs, callbacks=[tensorboard, early_stopping, early_stopping_val], validation_split=0.2, batch_size=batch_size, epochs=epochs)\n",
    "        self.predicts = self.model.predict(self.test_inputs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 141, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 474, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 64)           29440       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 64)           29440       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           2080        lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64)           0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            65          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1)            0           dense_6[0][0]                    \n",
      "                                                                 dot_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 63,105\n",
      "Trainable params: 63,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 14850 samples, validate on 3713 samples\n",
      "Epoch 1/20\n",
      "14850/14850 [==============================] - 159s 11ms/step - loss: 1.4529 - val_loss: 1.2579\n",
      "Epoch 2/20\n",
      "14850/14850 [==============================] - 168s 11ms/step - loss: 1.2060 - val_loss: 1.2756\n",
      "Epoch 3/20\n",
      "14850/14850 [==============================] - 157s 11ms/step - loss: 1.1507 - val_loss: 1.4071\n",
      "Epoch 4/20\n",
      "14850/14850 [==============================] - 155s 10ms/step - loss: 1.1156 - val_loss: 1.2062\n",
      "Epoch 5/20\n",
      "14850/14850 [==============================] - 250s 17ms/step - loss: 1.0664 - val_loss: 1.0711\n",
      "Epoch 6/20\n",
      "14850/14850 [==============================] - 250s 17ms/step - loss: 1.0170 - val_loss: 1.1059\n",
      "Epoch 7/20\n",
      "14850/14850 [==============================] - 253s 17ms/step - loss: 0.9923 - val_loss: 1.0494\n",
      "Epoch 8/20\n",
      "14850/14850 [==============================] - 339s 23ms/step - loss: 0.9555 - val_loss: 1.0379\n",
      "Epoch 9/20\n",
      "14850/14850 [==============================] - 358s 24ms/step - loss: 0.9492 - val_loss: 1.0484\n",
      "Epoch 10/20\n",
      "14850/14850 [==============================] - 368s 25ms/step - loss: 0.9268 - val_loss: 1.0243\n",
      "Epoch 11/20\n",
      "14850/14850 [==============================] - 354s 24ms/step - loss: 0.8847 - val_loss: 1.0425\n",
      "Epoch 12/20\n",
      "14850/14850 [==============================] - 348s 23ms/step - loss: 0.8672 - val_loss: 1.0206\n",
      "Epoch 13/20\n",
      "14850/14850 [==============================] - 369s 25ms/step - loss: 0.8372 - val_loss: 1.0187\n",
      "Epoch 14/20\n",
      "14850/14850 [==============================] - 356s 24ms/step - loss: 0.8131 - val_loss: 1.0568\n",
      "Epoch 15/20\n",
      "14850/14850 [==============================] - 339s 23ms/step - loss: 0.7912 - val_loss: 1.0392\n",
      "Epoch 16/20\n",
      "14850/14850 [==============================] - 353s 24ms/step - loss: 0.7647 - val_loss: 1.0632\n",
      "Epoch 17/20\n",
      "14850/14850 [==============================] - 359s 24ms/step - loss: 0.7487 - val_loss: 1.0584\n",
      "Epoch 18/20\n",
      "14850/14850 [==============================] - 187s 13ms/step - loss: 0.7189 - val_loss: 1.0532\n",
      "Epoch 19/20\n",
      "14850/14850 [==============================] - 151s 10ms/step - loss: 0.7042 - val_loss: 1.0798\n"
     ]
    }
   ],
   "source": [
    "# Calculates median user review length and item length. We then pad each review to these numbers\n",
    "ptile = 50\n",
    "u_seq_len = int(np.percentile(np.array(extra_info['user_seq_sizes']), ptile))\n",
    "i_seq_len = int(np.percentile(np.array(extra_info['item_seq_sizes']), ptile))\n",
    "embed_dims = matUserInputData[0].shape[1]\n",
    "hidden_size = 32\n",
    "rnn_hidden_size = 64\n",
    "deepconn = DeepCoNN(embed_dims, hidden_size, rnn_hidden_size, u_seq_len, i_seq_len)\n",
    "\n",
    "deepconn.train(matUserInputData, matMovieInputData, ratingsData, \n",
    "           u_seq_len=u_seq_len, i_seq_len=i_seq_len,\n",
    "           epochs=20, training=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def graph_loss(loss_arr, title):\n",
    "    plt.scatter(x=range(len(loss)), y=loss)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "graph_loss(loss, \"Training Loss\")\n",
    "graph_loss(validation_loss, \"Validation Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
