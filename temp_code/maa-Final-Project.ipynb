{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import simplejson\n",
    "import json\n",
    "import time\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataImporter(object):\n",
    "    def __init__(self, path_to_data):\n",
    "        # get the raw data\n",
    "        raw_data = [json.loads(i) for i in gzip.open(path_to_data, \"rt\")]\n",
    "#         raw_data = raw_data[:1000]\n",
    "        self.data = pd.DataFrame(raw_data)\n",
    "        self.num_reviews = len(self.data)\n",
    "        \n",
    "        # get the counts of users and products\n",
    "        self.reviewers = self.data[\"reviewerID\"].unique()\n",
    "        self.num_reviewers = len(self.reviewers)\n",
    "        self.products = self.data[\"asin\"].unique()\n",
    "        self.num_products = len(self.products)\n",
    "        \n",
    "        # create int-to-name dictionaries for each\n",
    "        self.reviewer_to_num = {reviewer.lower(): idx for idx, reviewer in enumerate(self.reviewers)}\n",
    "        self.num_to_reviewer = {self.reviewer_to_num[reviewer]: reviewer for reviewer in self.reviewer_to_num}\n",
    "        self.product_to_num = {product.lower(): idx for idx, product in enumerate(self.products)}\n",
    "        self.num_to_product = {self.product_to_num[product]: product for product in self.product_to_num}\n",
    "        \n",
    "    def create_train_test_split(self, frac_test):\n",
    "        # get test data indices\n",
    "        test_indices = np.random.choice(self.num_reviews,\n",
    "                                         size=int(self.num_reviews * frac_test),\n",
    "                                         replace=False)\n",
    "        \n",
    "        # split raw data into train/test\n",
    "        raw_test = self.data.iloc[test_indices, :]\n",
    "        raw_train = self.data.drop(test_indices).dropna()\n",
    "        \n",
    "        # get dimensions of the matrices\n",
    "        dim = (self.num_reviewers, self.num_products)\n",
    "        self.test_matrix = self.populate_user_product_review_matrix(dim, raw_test)\n",
    "        self.train_matrix = self.populate_user_product_review_matrix(dim, raw_train)\n",
    "        \n",
    "        self.cos_sim_mat = np.zeros((self.num_products, self.num_products))\n",
    "        for root_prod_idx, prod_ratings in enumerate(self.train_matrix.T):\n",
    "            for comp_prod_idx, comp_ratings in enumerate(self.train_matrix.T):\n",
    "                if comp_prod_idx >= root_prod_idx:\n",
    "                    self.cos_sim_mat[root_prod_idx, comp_prod_idx] = 1 - scipy.spatial.distance.cosine(prod_ratings,\n",
    "                                                                                                       comp_ratings)\n",
    "        \n",
    "        self.test = self.permute_matrix(self.test_matrix)\n",
    "        self.train = self.permute_matrix(self.train_matrix)\n",
    "        \n",
    "    def permute_matrix(self, mat):\n",
    "        perm = []\n",
    "        for u_idx, row in enumerate(mat):\n",
    "            for prod_idx, score in enumerate(row):\n",
    "                if score > 0.0:\n",
    "                    user_cpy = row.copy()\n",
    "                    score = user_cpy[prod_idx]\n",
    "                    user_cpy[prod_idx] = 0\n",
    "                    prod_cpy = mat[:, prod_idx].copy()\n",
    "                    other_movies_reviewed = np.where(user_cpy > 0)[0]\n",
    "                    similarities = [cos_sim for cos_sim in self.cos_sim_mat[prod_idx, other_movies_reviewed]]\n",
    "                    prod_cpy[u_idx] = 0\n",
    "                    perm.append({\"user\": user_cpy,\n",
    "                                 \"reviewerID\": self.num_to_reviewer[u_idx],\n",
    "                                 \"product\": prod_cpy,\n",
    "                                 \"score\": score,\n",
    "                                 \"asin\": self.num_to_product[prod_idx],\n",
    "                                 \"cosine_similarity\": np.mean(similarities)\n",
    "                                 })\n",
    "        return perm\n",
    "    \n",
    "    def populate_user_product_review_matrix(self, dimensions, dataset):\n",
    "        assert isinstance(dataset, pd.DataFrame)\n",
    "        ret_matrix = np.zeros(dimensions)\n",
    "        for idx, row in dataset.iterrows():\n",
    "            ret_matrix[self.reviewer_to_num[row[\"reviewerID\"].lower()], self.product_to_num[row[\"asin\"].lower()]] = row[\"overall\"]\n",
    "        return ret_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "data_importer = DataImporter(\"data/reviews_Amazon_Instant_Video_5.json.gz\")\n",
    "start = time.time()\n",
    "data_importer.create_train_test_split(0.1)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class wide_and_deep(object):\n",
    "    \n",
    "    def __init__(self, train, test, num_users, num_prods):\n",
    "        self.num_users = num_users\n",
    "        self.num_products = num_prods\n",
    "        self.train = np.copy(train)\n",
    "        self.test = np.copy(test)\n",
    "        \n",
    "        np.random.shuffle(self.train)\n",
    "        np.random.shuffle(self.test)\n",
    "        \n",
    "        self.num_examples = self.train.shape[0] + self.test.shape[0]\n",
    "        self.perc_test = self.test.shape[0] / self.train.shape[0]\n",
    "        \n",
    "        self.train = self.add_all_features_columns(self.train)\n",
    "        self.test = self.add_all_features_columns(self.test)\n",
    "        \n",
    "    def add_all_features_columns(self, dataset):\n",
    "        dataset = np.array(list(map(self.add_num_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_top_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_bottom_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_average_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_percent_rating, dataset)) )\n",
    "        \n",
    "        dataset = pd.DataFrame(list(dataset))\n",
    "        return dataset\n",
    "        \n",
    "    def add_num_ratings(self, data):\n",
    "        data['num_user_ratings'] = np.count_nonzero(data['user'])\n",
    "        data['num_movie_ratings'] = np.count_nonzero(data['product'])\n",
    "        return data\n",
    "        \n",
    "    def add_top_ratings(self, data):\n",
    "        data['top_user_rating'] = np.amax(data['user'])\n",
    "        data['top_movie_rating'] = np.amax(data['product'])\n",
    "        return data\n",
    "    \n",
    "    def add_bottom_ratings(self, data):\n",
    "        data['bottom_user_rating'] = np.amin(data['user'])\n",
    "        data['bottom_movie_rating'] = np.amin(data['product'])\n",
    "        return data\n",
    "    \n",
    "    def add_average_ratings(self, data):\n",
    "        data['average_user_rating'] = np.average(data['user'])\n",
    "        data['average_movie_rating'] = np.average(data['product'])\n",
    "        return data\n",
    "    \n",
    "    def add_percent_rating(self, data):\n",
    "        l = len(np.where(data['user'] > 0.0)[0])\n",
    "        if l > 0:\n",
    "            data['percent_one_star'] = len(np.where(data['user'] == 1.0)[0])/l\n",
    "            data['percent_two_star'] = len(np.where(data['user'] == 2.0)[0])/l\n",
    "            data['percent_three_star'] = len(np.where(data['user'] == 3.0)[0])/l\n",
    "            data['percent_four_star'] = len(np.where(data['user'] == 4.0)[0])/l\n",
    "            data['percent_five_star'] = len(np.where(data['user'] == 5.0)[0])/l\n",
    "        return data\n",
    "        \n",
    "    def total_ratings(self, data):\n",
    "        count = 0.0\n",
    "        for i in data:\n",
    "            if i['score'] != 0.0:\n",
    "                count += 1\n",
    "        print(count)\n",
    "        \n",
    "    def build_model(self, deep_layers, deep_layer_activation, wide_output_dim, wide_activation):\n",
    "        \n",
    "        D = self.train.drop([\"reviewerID\", \"asin\", \"score\", \"product\", \"user\"], axis=1).shape[1] \n",
    "        \n",
    "        with tf.name_scope(\"input\"):\n",
    "            X = tf.placeholder(tf.float32, [None, D], name=\"x_input\")\n",
    "        \n",
    "        with tf.name_scope(\"input_normalization\"):\n",
    "            X = tf.layers.batch_normalization(X)\n",
    "        \n",
    "        T = lambda x: tf.matmul(tf.reshape(x, [x.shape[0], 1]), tf.reshape(x, [1, x.shape[0]]))\n",
    "        cross_prod_mat = tf.map_fn(T, X)\n",
    "        cross_prod = tf.map_fn(lambda x: tf.reshape(x, [-1]), cross_prod_mat)\n",
    "        \n",
    "        # wide part\n",
    "        wide = tf.layers.dense(cross_prod, wide_output_dim, activation=wide_activation, use_bias=False)\n",
    "        \n",
    "        deep = X\n",
    "        \n",
    "        # deep part\n",
    "        for i in deep_layers:\n",
    "            deep = tf.layers.dense(deep, i, activation=deep_layer_activation, use_bias=False)\n",
    "            \n",
    "        # combine the output of wide and deep\n",
    "        wide_and_deep = tf.concat([wide, deep], 1)\n",
    "        logits = tf.layers.dense(wide_and_deep, 5, name=\"logits\")\n",
    "        \n",
    "    def add_optimizer(self, learning_rate):\n",
    "        \n",
    "        default_graph = tf.get_default_graph()\n",
    "        logits = default_graph.get_tensor_by_name(\"logits/BiasAdd:0\")\n",
    "        \n",
    "        with tf.name_scope(\"optimizer_input\"):\n",
    "            global_step = tf.placeholder(tf.int64, [], name=\"global_step\")\n",
    "            y = tf.placeholder(dtype=tf.int64, shape=[None], name=\"y_input\")\n",
    "        \n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y, 5), logits=logits)\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"softmax\"):\n",
    "            # Softmax layer\n",
    "            softmax = tf.nn.softmax(logits)\n",
    "            # Predict train\n",
    "            y_hat = tf.argmax(softmax, axis=1, name=\"y_hat\")\n",
    "            \n",
    "        with tf.name_scope('train'):\n",
    "            learning_rate = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.96, staircase=True)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(xentropy, name=\"training_op\")\n",
    "            \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # Train Evaluation\n",
    "            train_correct = tf.equal(y, y_hat)\n",
    "            tf.reduce_mean(tf.cast(train_correct, tf.float64), name=\"acc\")\n",
    "            \n",
    "    def train_model(self, sess, n_epochs, batch_size):\n",
    "        \n",
    "        # New***\n",
    "        init = tf.global_variables_initializer() \n",
    "\n",
    "        # get default graph\n",
    "        default_graph = tf.get_default_graph()\n",
    "        \n",
    "        # Get individual graph tensors\n",
    "        X = default_graph.get_tensor_by_name(\"input/x_input:0\")\n",
    "        Y = default_graph.get_tensor_by_name(\"optimizer_input/y_input:0\")\n",
    "        global_step = default_graph.get_tensor_by_name(\"optimizer_input/global_step:0\")\n",
    "        \n",
    "        # Get specific operations in graph\n",
    "        training_op = default_graph.get_operation_by_name(\"train/training_op\")\n",
    "        accuracy = default_graph.get_tensor_by_name(\"accuracy/acc:0\")\n",
    "        loss = default_graph.get_tensor_by_name(\"cross_entropy/loss/loss:0\")\n",
    "        y_hat = default_graph.get_tensor_by_name(\"softmax/y_hat:0\")\n",
    "        \n",
    "        # Create logdir\n",
    "        now = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"tf_logs\"\n",
    "        logdir = f\"{root_logdir}/run\"\n",
    "        \n",
    "        # Create Tensorboard file_writers\n",
    "        summary_writer = tf.summary.FileWriter(logdir, default_graph)\n",
    "        \n",
    "        # Create Tensorboard Summaries \n",
    "        acc_train_summary = tf.summary.scalar(\"train_accuracy\", accuracy)\n",
    "        acc_test_summary = tf.summary.scalar(\"test_accuracy\", accuracy)\n",
    "        loss_summary = tf.summary.scalar('train_loss', loss)\n",
    "        \n",
    "        # Create Tensorboard histograms\n",
    "        conv_histograms = []\n",
    "        for i in tf.global_variables():\n",
    "            if i.name[:6] == \"conv2d\":\n",
    "                # need to switch colon with underscore because colons aren't allowed in summary names\n",
    "                conv_histograms.append(tf.summary.histogram(i.name.replace(\":\", \"_\"), i))\n",
    "        \n",
    "        # Wrapper tells Tensorboard to assume everything is within same session\n",
    "        sess.run(init)\n",
    "\n",
    "        step = 0\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            n_iters = int((self.num_examples * (1 - self.perc_test)) / batch_size)\n",
    "            print_interval = 100\n",
    "            best_test = 0.\n",
    "\n",
    "            print(f\"\\nStarting epoch {epoch}, running for {n_iters} iterations\")\n",
    "            print(f\"Printing evaluation metrics every {print_interval} iterations\\n\")\n",
    "\n",
    "            for iteration in range(1, n_iters):\n",
    "                step += 1\n",
    "                indices = np.random.choice(int(self.num_examples * (1 - self.perc_test)), batch_size)\n",
    "                X_batch = self.train.iloc[indices, :]\n",
    "                user_series = X_batch[\"user\"].apply(pd.Series)\n",
    "                prod_series = X_batch[\"product\"].apply(pd.Series)\n",
    "                x_numerical_cols = X_batch.drop([\"reviewerID\", \"asin\", \"score\", \"product\", \"user\"], axis=1)\n",
    "                y_batch = self.train.iloc[indices, :][\"score\"].as_matrix() - 1\n",
    "                sess.run(training_op, feed_dict={X: x_numerical_cols,\n",
    "                                                 Y: y_batch,\n",
    "                                                 global_step: step})\n",
    "\n",
    "                if iteration % print_interval == 0:\n",
    "                    acc_train, train_summary, y_pred = sess.run([accuracy, tf.summary.merge([loss_summary, acc_train_summary]), y_hat],\n",
    "                                                        {X: x_numerical_cols, Y: y_batch})\n",
    "                    \n",
    "                    test_x = self.test.drop([\"reviewerID\", \"asin\", \"score\", \"product\", \"user\"], axis=1).as_matrix()\n",
    "                    test_y = self.test[\"score\"].as_matrix()\n",
    "                    acc_test, test_summary = sess.run([accuracy, acc_test_summary], {X: test_x,\n",
    "                                                                                     Y: test_y - 1})\n",
    "\n",
    "                    # Add batch train loss & accuracy to Tensorboard\n",
    "                    summary_writer.add_summary(tf.summary.merge([train_summary, test_summary]).eval(session=sess), step)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                    # Print out batch evaluation metrics\n",
    "                    print(\"Iteration: {} train acc: {:.4f} test acc: {:.4f}\".format(iteration, \n",
    "                                                                                    acc_train, \n",
    "                                                                                    acc_test))\n",
    "        # Ensure file_writers closed\n",
    "        summary_writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "wad = wide_and_deep(data_importer.train, data_importer.test, data_importer.num_reviewers, data_importer.num_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wad.build_model([1024, 512, 256], tf.nn.relu, 128, tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wad.add_optimizer(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 1, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0200 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0300 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.0400 test acc: 0.0455\n",
      "\n",
      "Starting epoch 2, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0800 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0300 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.0400 test acc: 0.0455\n",
      "\n",
      "Starting epoch 3, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0800 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0500 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.1000 test acc: 0.0455\n",
      "\n",
      "Starting epoch 4, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0300 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0500 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.0600 test acc: 0.0455\n",
      "\n",
      "Starting epoch 5, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0200 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0200 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.0600 test acc: 0.0455\n",
      "\n",
      "Starting epoch 6, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0300 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0600 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.0200 test acc: 0.0455\n",
      "\n",
      "Starting epoch 7, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0300 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0400 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.0300 test acc: 0.0455\n",
      "\n",
      "Starting epoch 8, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0900 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0700 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.0400 test acc: 0.0455\n",
      "\n",
      "Starting epoch 9, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0500 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0400 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.0600 test acc: 0.0455\n",
      "\n",
      "Starting epoch 10, running for 326 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n",
      "Iteration: 100 train acc: 0.0500 test acc: 0.0455\n",
      "Iteration: 200 train acc: 0.0500 test acc: 0.0455\n",
      "Iteration: 300 train acc: 0.0300 test acc: 0.0455\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "wad.train_model(sess, 10, 100)\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_tf",
   "language": "python",
   "name": "dl_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
