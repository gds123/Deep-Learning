{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import scipy.spatial\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils\n",
    "def initEmbeddingMap(fileName):\n",
    "    print(\"initializing embeddings\")\n",
    "    with open(join(\"data\", \"glove.6B\", fileName)) as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in [line.split() for line in glove]}\n",
    "\n",
    "def clean(text):\n",
    "    return text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                 lower=True, split=\" \")\n",
    "\n",
    "\n",
    "# returns dict with users and movies they rated as repeated rows\n",
    "# cleans review text and add to rawOutput\n",
    "def initRawData(input_file):\n",
    "    print(\"initializing raw data\")\n",
    "    rawInputData = []\n",
    "    rawOutputData = []\n",
    "    with open(input_file,\"r\") as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            if len(line) < 4:\n",
    "                break\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj[\"reviewerID\"]\n",
    "            item = lineObj[\"asin\"]\n",
    "            rawInputDataObj = {\"user\": user, \"asin\": item}\n",
    "            rawOutputDataObj = clean(lineObj[\"reviewText\"])\n",
    "            rawInputData.append(rawInputDataObj)\n",
    "            rawOutputData.append(rawOutputDataObj)\n",
    "    return rawInputData, rawOutputData\n",
    "\n",
    "# creates dict of usrs w/ all movies rated + movies w/ all user ratings *** \n",
    "def group_data(inputData):\n",
    "    users = {}\n",
    "    items = {}\n",
    "    for datum in inputData:\n",
    "        u = datum[\"user\"]\n",
    "        i = datum[\"asin\"]\n",
    "        users.setdefault(u, []).append(i)\n",
    "        items.setdefault(i, []).append(u)\n",
    "    return users, items\n",
    "\n",
    "def getSetFromData(key, data):\n",
    "    return set([datum.get(key) for datum in data])\n",
    "\n",
    "def seq_2_matrix(sequence, embedding_map):\n",
    "    return np.array([embedding_map.get(word) for word in sequence if word in embedding_map])\n",
    "\n",
    "def matrix_2_avg(emb_matrix):\n",
    "    return np.mean(emb_matrix, 0)\n",
    "\n",
    "\n",
    "# utils - one hot encodes all data \n",
    "def initVecData(rawInputData, rawOutputData, embedding_map):\n",
    "    print('initializing vectorized data')\n",
    "    dictVect = DictVectorizer()\n",
    "    vecInputData = dictVect.fit_transform(rawInputData).toarray()\n",
    "    vecOutputData = [matrix_2_avg(seq_2_matrix(review, embedding_map)) for review in rawOutputData]\n",
    "    return vecInputData, vecOutputData\n",
    "\n",
    "def initMatInputData(rawInputData, rawOutputData, embedding_map, save=False):\n",
    "    print('initializing matrix data')\n",
    "    if len(rawInputData) != len(rawOutputData):\n",
    "        raise ValueError(\"Need same size of input and output\")\n",
    "    users = {}\n",
    "    extra_info = {}\n",
    "    items = {}\n",
    "    dictVect = DictVectorizer()\n",
    "    for i in range(len(rawInputData)):\n",
    "        vecOutput = seq_2_matrix(rawOutputData[i], embedding_map)\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        users.setdefault(user, []).append(vecOutput)\n",
    "        items.setdefault(item, []).append(vecOutput)\n",
    "        \n",
    "    matUserInputData = []\n",
    "    matItemInputData = []\n",
    "    users = {k: np.vstack(v) for k, v in users.items()}\n",
    "    items = {k: np.vstack(v) for k, v in items.items()}\n",
    "    extra_info['user_seq_sizes'] = [m.shape[0] for m in users.values()]\n",
    "    extra_info['item_seq_sizes'] = [m.shape[0] for m in items.values()]\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        matUserInputData.append(users.get(user))\n",
    "        matItemInputData.append(items.get(item))\n",
    "    return matUserInputData, matItemInputData, extra_info\n",
    "\n",
    "def toKey(user, item):\n",
    "    return (user, item)\n",
    "\n",
    "def initRatingsOutputData(rawInputData, input_file, save=False):\n",
    "    ratingsData = []\n",
    "    userItemDict = {}\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        userItem = toKey(rawInput['user'], rawInput['asin'])\n",
    "        userItemDict[userItem] = i\n",
    "        ratingsData.append(None) # check later to make sure no Nones left\n",
    "        \n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rating = lineObj['overall']\n",
    "            i = userItemDict.get(toKey(user, item))\n",
    "            if i is not None:\n",
    "                ratingsData[i] = rating\n",
    "        failure = None in ratingsData\n",
    "        if failure:\n",
    "            raise ValueError(str(len([r for r in ratingsData if r is None])) + \" reviews did not have corresponding rating.\")\n",
    "    return ratingsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing raw data\n"
     ]
    }
   ],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "rawInputData, rawOutputData = initRawData(input_file=fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users, movies = group_data(rawInputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_idxs = np.random.permutation(len(rawOutputData))\n",
    "rawInputData = [rawInputData[i] for i in rand_idxs]\n",
    "rawOutputData = [rawOutputData[i] for i in rand_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing embeddings\n"
     ]
    }
   ],
   "source": [
    "embedding_map = initEmbeddingMap(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing vectorized data\n"
     ]
    }
   ],
   "source": [
    "all_users = getSetFromData('user', rawInputData)\n",
    "all_movies = getSetFromData('asin', rawInputData)\n",
    "vecInputData, vecOutputData = initVecData(rawInputData, rawOutputData, embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing matrix data\n",
      "18563\n",
      "18563\n"
     ]
    }
   ],
   "source": [
    "matUserInputData, matMovieInputData, extra_info = initMatInputData(rawInputData, rawOutputData, embedding_map)\n",
    "print(len(matUserInputData))\n",
    "print(len(matMovieInputData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "ratingsData = initRatingsOutputData(rawInputData, input_file=fileName,save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCoNN Recommendation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Dot\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.merge import Add, Dot, Concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "## Need to call these functions for the library <tffm> to work\n",
    "from tffm import TFFMClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepCoNN():\n",
    "    def __init__(self, embedding_size, hidden_size, rnn_hidden_size, u_seq_len, m_seq_len, filters=2, kernel_size=8,\n",
    "                 strides=6):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.inputU, self.towerU = self.create_deepconn_tower(u_seq_len)\n",
    "        self.inputM, self.towerM = self.create_deepconn_tower(m_seq_len)\n",
    "        self.joined = Concatenate()([self.towerU, self.towerM])\n",
    "        self.outNeuron = Dense(1)(self.joined)\n",
    "        print(type(self.joined))\n",
    "        print(tf.size(self.joined))\n",
    "        #print(tf.Dimension(self.joined))\n",
    "        #print(tensor_shape(self.joined).as_list())\n",
    "        t = tf.Print(self.joined, [self.joined])  # Here we are using the value returned by tf.Print\n",
    "        print(t) \n",
    "\n",
    "    def create_deepconn_tower(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        tower = LSTM(self.rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(self.hidden_size, activation=\"relu\")(tower)\n",
    "        return input_layer, tower\n",
    "\n",
    "    def create_deepconn_dp(self):\n",
    "        dotproduct = Dot(axes=1)([self.towerU, self.towerM])\n",
    "        output = Add()([self.outNeuron, dotproduct])\n",
    "        self.model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
    "        self.model.compile(optimizer='Adam', loss='mse')\n",
    "        \n",
    "    # this is the function that calculates the factorization machine. \n",
    "    # I ahve a toy version of this working well on my laptop. \n",
    "    \n",
    "#     def fac_mach(self, matUserInputData, matItemInputData, ratingsData, u_seq_len=200, i_seq_len=200, epochs=3500, training=None):\n",
    "        \n",
    "#         concat = self.joined\n",
    "        \n",
    "#         model = TFFMClassifier(\n",
    "#             order=6,\n",
    "#             rank=10,\n",
    "#             optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "#             n_epochs=50,\n",
    "#             batch_size=-1,\n",
    "#             init_std=0.001,\n",
    "#             input_type='dense'\n",
    "#         )\n",
    "#         fac_mach_out = model.fit(concat, ratingsData, show_progress=True)\n",
    "#         return fac_mach_out\n",
    "        \n",
    "        \n",
    "    def train(self, matUserInputData, matItemInputData, ratingsData, u_seq_len=200, i_seq_len=200, epochs=3500, training=None):\n",
    "        self.create_deepconn_dp()\n",
    "        \n",
    "        ## Rather than calling deepconn_dp, we call the function below to give us the fm\n",
    "        #self.fac_mach(self, matUserInputData, matItemInputData, ratingsData, u_seq_len=200, i_seq_len=200, epochs=3500, training=None)\n",
    "                 \n",
    "        tensorboard = TensorBoard(log_dir=\"tf_logs/{}\".format(time()))\n",
    "        self.user_input = pad_sequences(np.asarray(matUserInputData), maxlen=u_seq_len)\n",
    "        self.item_input = pad_sequences(np.asarray(matItemInputData), maxlen=i_seq_len)\n",
    "\n",
    "        self.trainingN = int(len(user_input) * training) if type(training) is float else training\n",
    "\n",
    "        self.outputs = np.asarray(ratingsData)\n",
    "        print(self.model.summary())\n",
    "\n",
    "        self.train_inputs = [self.user_input[:self.trainingN], self.item_input[:self.trainingN]]\n",
    "        self.train_outputs = self.outputs[:self.trainingN]\n",
    "        self.test_inputs = [self.user_input[self.trainingN:], self.item_input[self.trainingN:]]\n",
    "        self.test_outputs = self.outputs[self.trainingN:]\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=4)\n",
    "        early_stopping_val = EarlyStopping(monitor='val_loss', patience=6)\n",
    "        \n",
    "        batch_size = 32\n",
    "        \n",
    "        self.history = self.model.fit(self.train_inputs, self.train_outputs, callbacks=[tensorboard, early_stopping, early_stopping_val], validation_split=0.2, batch_size=batch_size, epochs=epochs)\n",
    "        self.predicts = self.model.predict(self.test_inputs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"Size_8:0\", shape=(), dtype=int32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 141, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 474, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 64)           29440       input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 64)           29440       input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 32)           2080        lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 32)           2080        lstm_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 64)           0           dense_31[0][0]                   \n",
      "                                                                 dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1)            65          concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1)            0           dense_31[0][0]                   \n",
      "                                                                 dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 1)            0           dense_33[0][0]                   \n",
      "                                                                 dot_4[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 63,105\n",
      "Trainable params: 63,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Calculates median user review length and item length. We then pad each review to these numbers\n",
    "ptile = 50\n",
    "u_seq_len = int(np.percentile(np.array(extra_info['user_seq_sizes']), ptile))\n",
    "i_seq_len = int(np.percentile(np.array(extra_info['item_seq_sizes']), ptile))\n",
    "embed_dims = matUserInputData[0].shape[1]\n",
    "hidden_size = 32\n",
    "rnn_hidden_size = 64\n",
    "deepconn = DeepCoNN(embed_dims, hidden_size, rnn_hidden_size, u_seq_len, i_seq_len)\n",
    "\n",
    "deepconn.train(matUserInputData, matMovieInputData, ratingsData, \n",
    "           u_seq_len=u_seq_len, i_seq_len=i_seq_len,\n",
    "           epochs=20, training=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def graph_loss(loss_arr, title):\n",
    "    plt.scatter(x=range(len(loss)), y=loss)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "graph_loss(loss, \"Training Loss\")\n",
    "graph_loss(validation_loss, \"Validation Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
