{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import simplejson\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_raw_input_data(path_to_data='data/reviews_Amazon_Instant_Video_5.json', save=False):\n",
    "    print('Aggregating all the review text')\n",
    "    rawData = []\n",
    "    with open(path_to_data,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            movie = lineObj['asin']\n",
    "            rating = lineObj['overall']\n",
    "            review = lineObj['reviewText']\n",
    "            rawInputDataObj = {'user':user, 'movieID':movie, 'rating':rating, 'review':review}\n",
    "            rawData.append(rawInputDataObj)\n",
    "            \n",
    "    if save:\n",
    "        pickle.dump((rawData), open('raw_input_data.pkl','wb'))\n",
    "    return rawData\n",
    "\n",
    "# I think this is what Rich wants but not sure...\n",
    "def get_model_data(raw_data):\n",
    "    users = {}\n",
    "    movies = {}\n",
    "    users_and_reviews = {}\n",
    "    movies_and_reviews = {}\n",
    "    for item in raw_data:\n",
    "        user = item['user']\n",
    "        movie = item['movieID']\n",
    "        review = item['review']\n",
    "        users.setdefault(user, []).append(movie)\n",
    "        movies.setdefault(movie, []).append(user)\n",
    "        users_and_reviews.setdefault(user, []).append(review)\n",
    "        movies_and_reviews.setdefault(movie, []).append(review)\n",
    "    return (users, movies, users_and_reviews,movies_and_reviews)\n",
    "\n",
    "def get_vect_review_data(tokenizer, word_len=250, path_to_data='data/reviews_Amazon_Instant_Video_5.json', save=False):\n",
    "    print('Constructing vectorized model input data')\n",
    "    vecUserReviews = []\n",
    "    vecMovieReviews = []\n",
    "    ratingsData = []\n",
    "    with open(path_to_data,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            movie = lineObj['asin']\n",
    "            rating = lineObj['overall']\n",
    "            review = lineObj['reviewText']\n",
    "            # Tokenize the review then add to dict1 user|review, dict2 movie|review\n",
    "            vectReview = tokenize_reviews(review, tokenizer, word_len)\n",
    "            user_and_vectReview = {'user':user, 'review':vectReview}\n",
    "            movie_and_vectReview = {'movieID':movie, 'review':vectReview}\n",
    "            # Aggregate user|review dicts, movie|review dicts, ratings into seperate lists\n",
    "            vecUserReviews.append(user_and_vectReview)\n",
    "            vecMovieReviews.append(movie_and_vectReview)\n",
    "            ratingsData.append(rating)      \n",
    "    if save:\n",
    "        pickle.dump((vecUserReviews), open('vec_user_rev_data.pkl','wb'))\n",
    "        pickle.dump((vecMovieReviews), open('vec_movie_rev_data.pkl','wb'))\n",
    "        pickle.dump((ratingsData), open('ratings_data.pkl','wb'))\n",
    "    print('Returning vectorized user|review data, movie|review data, and ratings data')\n",
    "    return vecUserReviews, vecMovieReviews, ratingsData\n",
    "    \n",
    "\n",
    "def aggregate_all_reviews(path_to_data='data/reviews_Amazon_Instant_Video_5.json', save=False):\n",
    "    print('Aggregating all the review text')\n",
    "    rawReviewData = []\n",
    "    with open(path_to_data,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            review = lineObj['reviewText']\n",
    "            rawReviewData.append(review)\n",
    "            \n",
    "    if save:\n",
    "        pickle.dump((rawReviewData), open('agg_review_data.pkl','wb'))\n",
    "    return rawReviewData\n",
    "\n",
    "def build_vocab(agg_text, word_len=250, vocab_size=25000):\n",
    "    #length of vocab, Tokenizer will only use vocab_len most common words\n",
    "    print(\"Length of vocabulary: \", vocab_size)\n",
    "\n",
    "    #we tokenize the texts and convert all the words to tokens\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(agg_text)\n",
    "    print(\"Fitting and returning tokenizer object\")\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_reviews(text, token, word_len=250):\n",
    "    #clip the sentence length to first (word_len) words.\n",
    "    # print(\"Max word length: \", word_len)\n",
    "    # Default text_to_sq removes punc,lowercases, + splits\" \"\n",
    "    token_data = token.texts_to_sequences(text) #No filters, lowercase, not sure which is better\n",
    "    \n",
    "    #Ensure all reviews have the same length, we pad the smaller reviews with 0, \n",
    "    #print(\"Padding sequences and returning sequence\")\n",
    "    return sequence.pad_sequences(token_data, maxlen=word_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating all the review text\n",
      "Length of vocabulary:  25000\n",
      "Fitting and returning tokenizer object\n",
      "18563\n"
     ]
    }
   ],
   "source": [
    "path_to_data = 'data/reviews_Amazon_Instant_Video_5.json'\n",
    "review_data = aggregate_all_reviews(path_to_data)\n",
    "token = build_vocab(review_data)\n",
    "tokenized_reviews = tokenize_reviews(review_data, token, word_len=250)\n",
    "#print(\"tokenized_reviews:\\n\", tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample review:\n",
      " ['I highly recommend this series. It is a must for anyone who is yearning to watch \"grown up\" television. Complex characters and plots to keep one totally involved. Thank you Amazin Prime.', 'Mysteries are interesting.  The tension between Robson and the tall blond is good but not always believable.  She often seemed uncomfortable.']\n",
      "\n",
      "Padded tokenized review:\n",
      " [[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     7   346   238     9    30     8     6     3   273    12\n",
      "    333    41     6  9276     4    56  1346    58   316   974    60     2\n",
      "    446     4   167    28   534   586   833    20 24680   315]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0   981    19   106     1   881   195 12092     2     1  3913  4703\n",
      "      6    31    14    21   143   495    77   366   463  2750]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_review = review_data[0:2]\n",
    "print(\"Sample review:\\n\", sample_review)\n",
    "tokens = tokenize_reviews(sample_review, tokenizer, word_len=250)\n",
    "print(\"\\nPadded tokenized review:\\n\", tokens)\n",
    "len(tokens[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating all the review text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'movieID': 'B000H00VBQ',\n",
       "  'rating': 5.0,\n",
       "  'review': 'I highly recommend this series. It is a must for anyone who is yearning to watch \"grown up\" television. Complex characters and plots to keep one totally involved. Thank you Amazin Prime.',\n",
       "  'user': 'A3BC8O2KCL29V2'},\n",
       " {'movieID': 'B000H00VBQ',\n",
       "  'rating': 4.0,\n",
       "  'review': 'Mysteries are interesting.  The tension between Robson and the tall blond is good but not always believable.  She often seemed uncomfortable.',\n",
       "  'user': 'A1RJPIGRSNX4PW'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Returns aggregated raw data where each entry in list is dictionary of a unique (user, review, rating, movie)\n",
    "So user can show up multiple times in list but not for same review/movie\n",
    "\"\"\"\n",
    "raw_data = get_raw_input_data()\n",
    "raw_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns lists of dictionaries \n",
    "user_data: users and list of moviesIDs they reviewed\n",
    "movie_data: movies with userIDs of users who have rated them\n",
    "user_to_reviews: (*important*) users with their reviews\n",
    "movie_to_reviews: (*important*) movies with their reviews\n",
    "\"\"\"\n",
    "user_data, movie_data, user_to_reviews, movie_to_reviews = get_model_data(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I highly recommend this series. It is a must for anyone who is yearning to watch \"grown up\" television. Complex characters and plots to keep one totally involved. Thank you Amazin Prime.', \"I watched this film on Saturday and it is now Wednesday and I am stilling thinking about it. This is an Australian film, so the pace of the story is somewhat different...the crescendos are quieter but the sentinel events are clear. The film explores the boundaries of  a friendship and what liberties are accepted and why. Both Naomi Watts and Robin Wright give performances which ring true and are compelling.  The young male leads are also quite strong. It's good and surprising story telling.\", \"I was not sure what I was getting but this film kept me watching. I was curious about. Each character's history and and motivation. The acting was good and there were familiar faces sprinkled throughout. I continue to be curious web out what comes next. All in all, I think this is a well written and well acted drama and there are probably surprises. Watch it and see what you think!\"]\n",
      "\n",
      "['If I had a friend Like MacGyver, than I would have try Survival Island and we would win all the money.', 'It was  Good and funny I enjoy it it is worth watching a family movie Just enjoy  very light  Enjoy', \"James Bond minus the adult stuff.  Great show that shows the mind is still the best weapon.  Sure there is some liberal stuff thrown in but it is family friendly.  And who can't resist the MacGyverisms as you think could that work.  Maybe not but it will make the kids think and want to experiment.  Good stuff for the whole family.\"]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "If you run the entire thing you may/will get IO warning (requires alot of memory to print)\n",
    "Here's one example of a user's reviews before Encoding!\n",
    "\"\"\"\n",
    "print(user_to_reviews['A3BC8O2KCL29V2']) #A3BC8O2KCL29V2 #A1RJPIGRSNX4PW\n",
    "print()\n",
    "# Example of a movie's reviews\n",
    "print(movie_to_reviews['B000HKWE3O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing vectorized model input data\n",
      "Returning vectorized user|review data, movie|review data, and ratings data\n"
     ]
    }
   ],
   "source": [
    "path_to_data = 'data/reviews_Amazon_Instant_Video_5.json'\n",
    "word_len = 250\n",
    "# tokenizer - returned above\n",
    "vecUserReviews, vecMovieReviews, ratingsData = get_vect_review_data(token, word_len, path_to_data, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_raw_reviews(filePath):\n",
    "    reviews = []\n",
    "    with open(filePath,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            review = lineObj['reviewText']\n",
    "            reviews.append(review) \n",
    "    return reviews\n",
    "\n",
    "def get_ratings(filePath):\n",
    "    ratings = []\n",
    "    with open(filePath,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            rating = lineObj['overall']\n",
    "            ratings.append(rating) \n",
    "    return pd.DataFrame(ratings)\n",
    "\n",
    "def get_test_train_split(data, filePath, frac_tect=0.1):\n",
    "    ratings_data = get_ratings(filePath)\n",
    "    num_reviews = len(data)\n",
    "    assert(num_reviews == len(ratings_data))\n",
    "    \n",
    "    test_indices = np.random.choice(num_reviews,\n",
    "                                    size=int(num_reviews * frac_test), \n",
    "                                    replace=False)\n",
    "    # Split raw data into train/test\n",
    "    x_test = data.iloc[test_indices, :]\n",
    "    y_test = ratings.iloc[test_indices, :]\n",
    "    x_train = data.drop(test_indices)\n",
    "    y_train = ratings.drop(test_indices)\n",
    "    # Review dimensions\n",
    "    print(\"x_train samples:\", len(x_train))\n",
    "    print(\"x_test samples:\", len(x_test))\n",
    "    print(\"y_train samples:\", len(y_train))\n",
    "    print(\"y_test samples:\", len(y_test))\n",
    "    return X_train, y_train, X_test, y_test  \n",
    "\n",
    "def get_embedding_weights(tokenizer, embedding_dim=100, fileName='data/glove.6B/glove.6B.100d.txt'):\n",
    "    embedding_map = {}\n",
    "    BASE_DIR = ''\n",
    "    GLOVE_DIR = os.path.join(BASE_DIR, 'data/glove.6B')\n",
    "    glove = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "    \n",
    "    print('constructing embedding dictionary...pls wait ~2min')\n",
    "    #dimension of Glove 300 Embeddings\n",
    "    EMBEDDING_DIM = embedding_dim\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "    #load glove embeddings\n",
    "    embedding_map = {}\n",
    "    for line in glove:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_map[word] = embedding\n",
    "    glove.close()\n",
    "    print('GloVe Word embeddings:', len(embedding_map))\n",
    "\n",
    "    # nb_words contains the total length of vocab\n",
    "    nb_words = len(word_index) + 1\n",
    "\n",
    "    #get glove embeddings for each word in tokenizer.\n",
    "    #word_embedding_matrix holds the embeddings dictionary\n",
    "    word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    #total words in the tokenizer not in Embedding matrix\n",
    "    print('Null words in GloVe embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
    "    return word_embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (16707, 250)\n",
      "y_train shape: (16707, 1)\n",
      "X_test shape: (1856, 250)\n",
      "y_test shape: (1856, 1)\n"
     ]
    }
   ],
   "source": [
    "filePath = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "\n",
    "raw_reviews = get_raw_reviews(filePath)\n",
    "ratings = get_ratings(filePath)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=25000)\n",
    "tokenizer.fit_on_texts(raw_reviews)\n",
    "train_seq = tokenizer.texts_to_sequences(raw_reviews)\n",
    "\n",
    "X_data = tokenizer.texts_to_sequences(raw_reviews)\n",
    "X_data = sequence.pad_sequences(X_data, maxlen = 250)\n",
    "X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Get test/ train split \n",
    "frac_test = 0.1\n",
    "test_indices = np.random.choice(num_reviews,\n",
    "                                    size=int(num_reviews * frac_test), \n",
    "                                    replace=False)\n",
    "# Split raw data into train/test\n",
    "X_test = X_data.iloc[test_indices, :]\n",
    "y_test = ratings.iloc[test_indices, :]\n",
    "X_train = X_data.drop(test_indices)\n",
    "y_train = ratings.drop(test_indices)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing embedding dictionary...pls wait ~2min\n",
      "Found 43445 unique tokens\n",
      "GloVe Word embeddings: 400000\n",
      "Null words in GloVe embeddings: 9032\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This may take a few. The greater embedding dim the longer. Preset to GloVe 100d\"\"\"\n",
    "embedding_matrix = get_embedding_map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 250, 100)          4344600   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 4,425,505\n",
      "Trainable params: 80,905\n",
      "Non-trainable params: 4,344,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "          output_dim=100,\n",
    "          weights=[embedding_matrix],\n",
    "          input_length=250,\n",
    "          trainable=False))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'arrays' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-424-f0a5f0a798be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(X_train, y_train,\n\u001b[1;32m      3\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           epochs=4)\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/DL_tf/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda2/envs/DL_tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1556\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/DL_tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1410\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1411\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/DL_tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# Make arrays at least 2D.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'arrays' referenced before assignment"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 100)          4344600   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 248, 128)          38528     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               999552    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 5,414,141\n",
      "Trainable params: 1,069,541\n",
      "Non-trainable params: 4,344,600\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D,Conv1D, GRU, MaxPooling1D\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embedding_matrix.shape[0],\n",
    "          output_dim=100,\n",
    "          weights=[embedding_matrix],\n",
    "          input_length=250,\n",
    "          trainable=False))\n",
    "     \n",
    "model.add(Conv1D(64, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(32, 3))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'arrays' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-415-327d5e8ba032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(X_train, y_train,\n\u001b[1;32m      3\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           epochs=6)\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/DL_tf/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda2/envs/DL_tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1556\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/DL_tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1410\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1411\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/DL_tf/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# Make arrays at least 2D.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'arrays' referenced before assignment"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_tf",
   "language": "python",
   "name": "dl_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
