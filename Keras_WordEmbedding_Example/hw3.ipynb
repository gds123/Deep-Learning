{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Homework 3\n",
    "## Sentiment analysis using Neural Networks\n",
    "\n",
    "Total: 50 Points\n",
    "\n",
    "\n",
    "In this homework we will perform sentiment analysis using a few simple Neural Network based architectures.\n",
    "For this problem we use the IMDB Large Movie Review Dataset. The dataset contains 25,000 highly polar movie reviews for both train and test dataset, each with 12,500 positive (greater than equal to 7/10 rating) and 12,500 negative reviews(less than equal to 4/10 rating). \n",
    "\n",
    "Use \"https://keras.io/\" for keras documentation. Please use Python 3. GPU is not required but it will help improve the training speed for each problem.\n",
    "\n",
    "Please save the notebook with your cell outputs. You will not be graded if your outputs are not present below the homework cell. Also note your outputs will be unique since you will be using your the last numbers of your uni as your random seed (In the third cell). Make sure you submit this iPython file, with the saved outputs. The submission format must be 'hw3/hw3.ipynb'. You will not submit any other files. If you do save your model weights, you will not submit them. You will however, make sure your model weights do get saved in the 'weights' folder and can be retrieved from there as well.\n",
    "\n",
    "Please fill your details below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Name: Colby Wise\n",
    "\n",
    "Uni: cjw2165\n",
    "\n",
    "Email: cjw2165@columbia.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda, Activation, LSTM, Flatten, Convolution1D,Conv1D, GRU, MaxPooling1D\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "#from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#we retrieve train and test file names\n",
    "\n",
    "train_dir = \"./aclImdb/train/\"\n",
    "test_dir = \"./aclImdb/test/\"\n",
    "tr_review = [re_filename for re_filename in listdir(train_dir)]\n",
    "te_review = [re_filename for re_filename in listdir(test_dir)]\n",
    "\n",
    "#we initialize the train and test arrays\n",
    "\n",
    "tr_X = []\n",
    "tr_Y = []\n",
    "te_X = []\n",
    "te_Y = []\n",
    "\n",
    "#we arrange the reviews into the train and test arrays \n",
    "\n",
    "for review_file in tr_review:\n",
    "    f_review = open(train_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    tr_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        tr_Y.append(1)\n",
    "    else:\n",
    "        tr_Y.append(0)\n",
    "        \n",
    "for review_file in te_review:\n",
    "    f_review = open(test_dir+review_file, \"r\")\n",
    "    str_review = f_review.readline()\n",
    "    str_review = \" \".join(str_review.split(' '))\n",
    "    te_X.append(str_review)\n",
    "    y_truth = int (review_file.split('.')[0].split('_')[1])\n",
    "    if y_truth>=7:\n",
    "        te_Y.append(1)\n",
    "    else:\n",
    "        te_Y.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the validation set from the train set\n",
    "\n",
    "use the last 4 numbers of your uni for the seed value seed to ensure all answers remain unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2529\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#replace 2 (SEED) with the last 4 numbers of your Uni\n",
    "#Uni: cjw2165\n",
    "SEED = 2165\n",
    "seed_counter = 0\n",
    "while(1):\n",
    "\n",
    "    shuffle_combine = list(zip(tr_X, tr_Y))\n",
    "    random.seed(SEED+seed_counter)\n",
    "    seed_counter+=1\n",
    "    random.shuffle(shuffle_combine)\n",
    "\n",
    "    tr_X, tr_Y = zip(*shuffle_combine)\n",
    "\n",
    "    val_X = tr_X[:5000]\n",
    "    val_Y = tr_Y[:5000]\n",
    "\n",
    "    counter = 0\n",
    "    for label in val_Y:\n",
    "        counter+=label\n",
    "\n",
    "    print (counter)\n",
    "    print (seed_counter)\n",
    "    if(counter>2400 and counter <2600):\n",
    "        tr_X = tr_X[5000:]\n",
    "        tr_Y = tr_Y[5000:]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train review set : 20000\n",
      "Length of Train label set : 20000\n",
      "Length of Validation review set : 5000\n",
      "Length of Validation label set : 5000\n",
      "Length of Test review set : 25000\n",
      "Length of Test label set : 25000\n",
      "*****************************************\n",
      "Some sample Reviews Train sets and their labels\n",
      "This type of plot really does have a lot of potential, but it was butchered here. Honestly, I sensed the cheese element in the beginning, but I though\n",
      "0\n",
      "SPOILER ALERT ! ! ! Personally I don't understand why Pete did not help to save Williams life,I mean that would be great to know why William was motiv\n",
      "0\n",
      "Tough guys, sexy women, lots of swearing, and a most unconvincing monster that rises from the depths of a polluted lake. You'd think \"Monster\" would b\n",
      "0\n",
      "One of the first OVA's (\"original video animation\") I ever bought, this still has to be one of my favourite anime titles. A cyberpunk sci-fi action co\n",
      "1\n",
      "I was very disappointed in this movie. Plotwise it was weak bordering on silly: Souls who can affect reality in the way they do? A mission apparently \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Length of Train review set : \" + str(len(tr_X)))\n",
    "print(\"Length of Train label set : \" + str(len(tr_Y)))\n",
    "print(\"Length of Validation review set : \" + str(len(val_X)))\n",
    "print(\"Length of Validation label set : \" + str(len(val_Y)))\n",
    "print(\"Length of Test review set : \" + str(len(te_X)))\n",
    "print(\"Length of Test label set : \" + str(len(te_Y)))\n",
    "print(\"*****************************************\")\n",
    "print(\"Some sample Reviews Train sets and their labels\")\n",
    "print(tr_X[0][:150])\n",
    "print(tr_Y[0])\n",
    "print(tr_X[1][:150])\n",
    "print(tr_Y[1])\n",
    "print(tr_X[2][:150])\n",
    "print(tr_Y[2])\n",
    "print(tr_X[3][:150])\n",
    "print(tr_Y[3])\n",
    "print(tr_X[4][:150])\n",
    "print(tr_Y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we collect all the reviews from train validation and test set to generate \n",
    "texts = []\n",
    "texts += tr_X \n",
    "texts += te_X \n",
    "texts += val_X\n",
    "len(texts)\n",
    "\n",
    "\n",
    "\n",
    "#we clip the sentence length to first 250 words. \n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "#length of vocab, Tokenizer will only use vocab_len most common words\n",
    "vocab_len = 25000\n",
    "\n",
    "#we tokenize the texts and convert all the words to tokens\n",
    "tokenizer = Tokenizer(num_words=vocab_len)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(tr_X)\n",
    "token_te_X = tokenizer.texts_to_sequences(te_X)\n",
    "token_val_X = tokenizer.texts_to_sequences(val_X)\n",
    "\n",
    "#to ensure all reviews have the same length, we pad the smaller reviews with 0, \n",
    "#and cut the larger reviews to a max length \n",
    "#(we clip from the top, as the end of the reviews generally have a conclusion which provides better features)\n",
    "x_train = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = sequence.pad_sequences(token_te_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_val = sequence.pad_sequences(token_val_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "#changes the labels to one-hot encoding\n",
    "y_train = np_utils.to_categorical(tr_Y)\n",
    "y_test = np_utils.to_categorical(te_Y)\n",
    "y_val = np_utils.to_categorical(val_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 250)\n",
      "X_test shape: (25000, 250)\n",
      "X_val shape: (5000, 250)\n",
      "y_train shape: (20000, 2)\n",
      "y_test shape: (25000, 2)\n",
      "y_val shape: (5000, 2)\n",
      "*****************************************\n",
      "Tokenized Reviews Train sets and their labels\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n",
      "[   0    0    0    0    0   27    4    1   86  212  373  839   10  123 1226\n",
      "   11  130   45    5   26]\n",
      "[ 0.  1.]\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 1.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('X_val shape:', x_val.shape)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "\n",
    "print(\"*****************************************\")\n",
    "print(\"Tokenized Reviews Train sets and their labels\")\n",
    "print(x_train[0][:20])\n",
    "print(y_train[0])\n",
    "print()\n",
    "print(x_train[1][:20])\n",
    "print(y_train[1])\n",
    "print()\n",
    "print(x_train[2][:20])\n",
    "print(y_train[2])\n",
    "print()\n",
    "print(x_train[3][:20])\n",
    "print(y_train[3])\n",
    "print()\n",
    "print(x_train[4][:20])\n",
    "print(y_train[4])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************************************\n",
    "\n",
    "As you can see the reviews have now been transformed into indices to tokenized vocabulary and the labels have been converted to one-hot encoding. We can now go ahead and feed these sequences to Neural Network Models.\n",
    "\n",
    "********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A\n",
    "\n",
    "Building your first model (5 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,600,602\n",
      "Trainable params: 9,600,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=25000, output_dim=128, input_length=250))\n",
    "model.add(Flatten( ) )\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='Adam',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.3963 - acc: 0.8058 - val_loss: 0.2760 - val_acc: 0.8888\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.0411 - acc: 0.9861 - val_loss: 0.4266 - val_acc: 0.8622\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.0027 - acc: 0.9996 - val_loss: 0.5191 - val_acc: 0.8662\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 1.7571e-04 - acc: 1.0000 - val_loss: 0.5658 - val_acc: 0.8660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f499efe0cf8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B\n",
    "\n",
    "Stacking Fully Connected Layers (5 points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               6400200   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 9,640,802\n",
      "Trainable params: 9,640,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=25000, output_dim=128, input_length=250))\n",
    "model.add(Flatten( ) )\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "               optimizer='Adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 57s 3ms/step - loss: 0.4016 - acc: 0.8006 - val_loss: 0.2903 - val_acc: 0.8782\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.0434 - acc: 0.9848 - val_loss: 0.4570 - val_acc: 0.8452\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.8088 - val_acc: 0.8526\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.0139 - acc: 0.9952 - val_loss: 0.6902 - val_acc: 0.8380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f499e6efef0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=4,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C\n",
    "\n",
    "Using LSTMS based networks(5 Points) \n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "![title](img/model3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,348,354\n",
      "Trainable params: 3,348,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=25000, output_dim=128, input_length=250))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 157s 8ms/step - loss: 0.4548 - acc: 0.7867 - val_loss: 0.3515 - val_acc: 0.8548\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 156s 8ms/step - loss: 0.2157 - acc: 0.9176 - val_loss: 0.3265 - val_acc: 0.8778\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 156s 8ms/step - loss: 0.1209 - acc: 0.9571 - val_loss: 0.3445 - val_acc: 0.8748\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 155s 8ms/step - loss: 0.0831 - acc: 0.9708 - val_loss: 0.4156 - val_acc: 0.8486\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 154s 8ms/step - loss: 0.0555 - acc: 0.9813 - val_loss: 0.5607 - val_acc: 0.8628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f499cbd4a20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D\n",
    "\n",
    "Adding Pretrained Word Embeddings(10 Points)\n",
    "\n",
    "Construct this sequential model using Keras :\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 124252 unique tokens\n",
      "G Word embeddings: 1917494\n",
      "G Null word embeddings: 35772\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "#dimension of Glove Embeddings.\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "#load glove embeddings\n",
    "gembeddings_index = {}\n",
    "with codecs.open('glove.42B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "# nb_words contains the total length of vocab\n",
    "nb_words = len(word_index) +1\n",
    "\n",
    "#get glove embeddings for each word in tokenizer.\n",
    "#g_word_embedding_matrix holds the embeddings dictionary\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "#total words in the tokenizer not in Embedding matrix\n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 37,512,318\n",
      "Trainable params: 236,418\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "## to use the glove embeddings, your embedding layer would take the vocab size as input dimension, \n",
    "## Glove embedding dimension as the output dimsion\n",
    "## and you will provide the  embedding dictionary as the 'weights' parameter (!important) to the embedding layer.\n",
    "\n",
    "model.add(Embedding(input_dim=g_word_embedding_matrix.shape[0],\n",
    "          output_dim=300,\n",
    "          weights=[g_word_embedding_matrix],\n",
    "          input_length=250,\n",
    "          trainable=False))\n",
    "\n",
    "model.add(LSTM(128, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 207s 10ms/step - loss: 0.4757 - acc: 0.7767 - val_loss: 0.3411 - val_acc: 0.8556\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 0.3127 - acc: 0.8694 - val_loss: 0.2877 - val_acc: 0.8858\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 0.2652 - acc: 0.8918 - val_loss: 0.2789 - val_acc: 0.8878\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 0.2309 - acc: 0.9072 - val_loss: 0.2635 - val_acc: 0.8964\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 201s 10ms/step - loss: 0.1944 - acc: 0.9223 - val_loss: 0.2827 - val_acc: 0.8910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f495cd03710>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont attempt this\n",
    "\n",
    "Stacking LSTM layers\n",
    "\n",
    "Unfortunately it takes very long to train, be aware we can stack LTMSs over each other like this.\n",
    "This requires bottom LSTM to return a sequences instead instead of single vector, which becomes input for the top LSTM.\n",
    "\n",
    "\n",
    "![title](img/model5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part E\n",
    "\n",
    "Using Convolutional Networks (10 points)\n",
    "\n",
    "Construct the model, shown below. Use the same loss functions and optimizers as before\n",
    "\n",
    "Correction: The Embedding Layer Dimension (1st box) is 300, not 128.\n",
    "\n",
    "![title](img/model6.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 2,145,762\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=g_word_embedding_matrix.shape[0],\n",
    "          output_dim=300,\n",
    "          weights=[g_word_embedding_matrix],\n",
    "          input_length=250,\n",
    "          trainable=False))\n",
    "\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "     \n",
    "model.add(Conv1D(64, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(32, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 77s 4ms/step - loss: 0.4654 - acc: 0.7577 - val_loss: 0.3358 - val_acc: 0.8678\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 75s 4ms/step - loss: 0.3119 - acc: 0.8691 - val_loss: 0.2872 - val_acc: 0.8816\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.2679 - acc: 0.8909 - val_loss: 0.2808 - val_acc: 0.8876\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.2300 - acc: 0.9076 - val_loss: 0.2864 - val_acc: 0.8840\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 75s 4ms/step - loss: 0.1977 - acc: 0.9208 - val_loss: 0.2713 - val_acc: 0.8892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4947a20e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part F\n",
    "\n",
    "Model constructed : (5 points)\n",
    "\n",
    "Test Accuracy Over 87.5%: (5 Points)\n",
    "\n",
    "Bonus: Min(10, Square of (test_score - 88%))\n",
    "\n",
    "Create your best model, use Validation score to judge your best model and check accuracy on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 300)          37275900  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 248, 128)          115328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 248, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 246, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 246, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 244, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7808)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               1999104   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 39,421,662\n",
      "Trainable params: 2,145,762\n",
      "Non-trainable params: 37,275,900\n",
      "_________________________________________________________________\n",
      "Model Built\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "## implement model here\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=g_word_embedding_matrix.shape[0],\n",
    "          output_dim=300,\n",
    "          weights=[g_word_embedding_matrix],\n",
    "          input_length=250,\n",
    "          trainable=False))\n",
    "\n",
    "model.add(Conv1D(256, 3))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Activation('relu'))\n",
    "     \n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(64, 3))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"Model Built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep saving models with different names in model_name, \n",
    "\n",
    "so you can retrieve their weights again for testing, you dont have to retrain \n",
    "(You would have to initialize the model definition again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 77s 4ms/step - loss: 0.4654 - acc: 0.7577 - val_loss: 0.3358 - val_acc: 0.8678\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 75s 4ms/step - loss: 0.3119 - acc: 0.8691 - val_loss: 0.2872 - val_acc: 0.8816\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.2679 - acc: 0.8909 - val_loss: 0.2808 - val_acc: 0.8876\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.2300 - acc: 0.9076 - val_loss: 0.2864 - val_acc: 0.8840\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 75s 4ms/step - loss: 0.1977 - acc: 0.9208 - val_loss: 0.2713 - val_acc: 0.8892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4947a20e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wt_dir = \"./weights/\"\n",
    "# model_name = 'model1_best'\n",
    "# early_stopping = EarlyStopping(monitor='val_acc', patience=2)\n",
    "# bst_model_path = wt_dir + model_name + '.h5'\n",
    "# model_checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "print('Train...')\n",
    "model_1.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=6,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True,\n",
    "#          callbacks=[early_stopping, model_checkpoint])\n",
    "        callbacks=[early_stopping])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 1,\n",
    "         shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on using Ensemble averaging, feel free to edit the code below or add multiple models.\n",
    "\n",
    "Make sure they get saved and can be retrieved when executing serially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 34s 1ms/step\n",
      "Accuracy: 88.61%\n"
     ]
    }
   ],
   "source": [
    "#model.load_weights(bst_model_path)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part G\n",
    "\n",
    "Explain how Dense, LSTM and Convolution Layers work.\n",
    "\n",
    "Explain Relu, Dropout, and Softmax work.\n",
    "\n",
    "Analyze the architectures you constructed, with the accuracies you achieved and the training time it took. \n",
    "\n",
    "What are some insights you gained with these experiments? \n",
    "\n",
    "(5 Points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense, LSTM, CNN --\n",
    "    The Dense layer is a fully connected layer where every layer node is connected to each node in the following layer. LSTM is a form of recurrent nueral net (RNN) where the state space is halved with one being the memory and the other working memory. CNN or convolution nueral networks come from computer vision/image recognition field of DL and are great for learning complex features by taking local ordering patterns into account - great for ngrams.\n",
    "    \n",
    "Relu, Dropout, Softmax --\n",
    "    Relu is a not linear activation applied during feed-forward step that takes the max(1,0) of the input transformation creating a nonlinear output. Dropout can be applied before or after activation but essentially turns the node in the layer off such that the node is not used during the forward pass forcing the network to not rely on that nodes contribution. Softmax transforms the linear prediction into a probability [0,1] summing to 1 in the multi-score case. \n",
    "    \n",
    "Architectures Analysis --\n",
    "    In Part A and B we used sequential models and I noticed that adding another Dense layer had no impact on the validation accuracy. This is because the Dense layer was simply doing another linear transformation. Training time in A/B were four times faster than LSTM. Additionally accuracy was the best in LSTM(s) (~2% greater than sequential) particularly part D. Training epochs to reach 100% accuracy on train need to be increased in C/D which means training LSTMs takes longer which is inline with what we saw. CNN was fairly quick (2x sequential and 50% of lstm) and performed inline in terms of validation accuracy with LSTM albeit with more consistent generalization.\n",
    "    \n",
    "Insights --\n",
    "    Dropout had a modestly more noticeable effect on CNNs than sequential/LSTM consistent with literature that dropout in LSTMs has marginal effect. Furthermore, I found the best form of regularization to be decaying the learning rate and/or early stopping given the models tended to hit minimum loss around 4-5 epochs thus training for longer epochs required decaying learning rates or early stopping. I also found it interesting that wider (more activation in hidden layer) and deeper (more hd layers) did not necessarily lead to signification accuracy increases. In other sub-fields of deep learning like vision deeper and to some extent wider CNNs for instance have led to major accuracy increases such as in LeNet, Deep Dream from Google. Here the results did not immediately transcend. Pre-trained word embeddings lead to significant increases. I would be curious to experiment with different pre-trainings in future. \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
