{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/deep_learning/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import simplejson\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataImporter(object):\n",
    "    def __init__(self, path_to_data):\n",
    "        # get the raw data\n",
    "        raw_data = [json.loads(i) for i in gzip.open(path_to_data, \"rt\")]\n",
    "        self.data = pd.DataFrame(raw_data)\n",
    "        self.num_reviews = len(self.data)\n",
    "        \n",
    "        # get the counts of users and products\n",
    "        self.reviewers = self.data[\"reviewerID\"].unique()\n",
    "        self.num_reviewers = len(self.reviewers)\n",
    "        self.products = self.data[\"asin\"].unique()\n",
    "        self.num_products = len(self.products)\n",
    "        \n",
    "        # create int-to-name dictionaries for each\n",
    "        self.reviewer_to_num = {reviewer.lower(): idx for idx, reviewer in enumerate(self.reviewers)}\n",
    "        self.num_to_reviewer = {self.reviewer_to_num[reviewer]: reviewer for reviewer in self.reviewer_to_num}\n",
    "        self.product_to_num = {product.lower(): idx for idx, product in enumerate(self.products)}\n",
    "        self.num_to_product = {self.product_to_num[product]: product for product in self.product_to_num}\n",
    "        \n",
    "    def create_train_test_split(self, frac_test):\n",
    "        # get test data indices\n",
    "        test_indices = np.random.choice(self.num_reviews,\n",
    "                                         size=int(self.num_reviews * frac_test),\n",
    "                                         replace=False)\n",
    "        \n",
    "        # split raw data into train/test\n",
    "        raw_test = self.data.iloc[test_indices, :]\n",
    "        raw_train = self.data.drop(test_indices).dropna()\n",
    "        \n",
    "        # get dimensions of the matrices\n",
    "        dim = (self.num_reviewers, self.num_products)\n",
    "        self.test_matrix = self.populate_user_product_review_matrix(dim, raw_test)\n",
    "        self.train_matrix = self.populate_user_product_review_matrix(dim, raw_train)\n",
    "        \n",
    "        self.test = self.permute_matrix(self.test_matrix)\n",
    "        self.train = self.permute_matrix(self.train_matrix)\n",
    "        \n",
    "    def permute_matrix(self, mat):\n",
    "        perm = []\n",
    "        for u_idx, row in enumerate(mat):\n",
    "            for prod_idx, score in enumerate(row):\n",
    "                if score > 0.0:\n",
    "                    user_cpy = row.copy()\n",
    "                    score = user_cpy[prod_idx]\n",
    "                    user_cpy[prod_idx] = 0\n",
    "                    prod_cpy = mat[:, prod_idx].copy()\n",
    "                    prod_cpy[u_idx] = 0\n",
    "                    perm.append({\"user\": user_cpy,\n",
    "                                 \"reviewerID\": self.num_to_reviewer[u_idx],\n",
    "                                 \"product\": prod_cpy,\n",
    "                                 \"score\": score,\n",
    "                                 \"asin\": self.num_to_product[prod_idx]\n",
    "                                 })\n",
    "        return perm\n",
    "    \n",
    "    def populate_user_product_review_matrix(self, dimensions, dataset):\n",
    "        assert isinstance(dataset, pd.DataFrame)\n",
    "        ret_matrix = np.zeros(dimensions)\n",
    "        for idx, row in dataset.iterrows():\n",
    "            ret_matrix[self.reviewer_to_num[row[\"reviewerID\"].lower()], self.product_to_num[row[\"asin\"].lower()]] = row[\"overall\"]\n",
    "        return ret_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_importer = DataImporter(\"data/reviews_Amazon_Instant_Video_5.json.gz\")\n",
    "data_importer.create_train_test_split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wide_and_deep(object):\n",
    "    \n",
    "    def __init__(self, train, test, num_users, num_prods):\n",
    "        self.num_users = num_users\n",
    "        self.num_products = num_prods\n",
    "        self.train = np.copy(train)\n",
    "        self.test = np.copy(test)\n",
    "        \n",
    "        np.random.shuffle(self.train)\n",
    "        np.random.shuffle(self.test)\n",
    "        \n",
    "        self.num_examples = self.train.shape[0] + self.test.shape[0]\n",
    "        self.perc_test = self.test.shape[0] / self.train.shape[0]\n",
    "        \n",
    "        self.train = self.add_all_features_columns(self.train)\n",
    "        self.test = self.add_all_features_columns(self.test)\n",
    "        \n",
    "    def add_all_features_columns(self, dataset):\n",
    "        dataset = np.array(list(map(self.add_num_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_top_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_bottom_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_average_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_percent_rating, dataset)) )\n",
    "        \n",
    "        dataset = pd.DataFrame(list(dataset))\n",
    "        return dataset\n",
    "        \n",
    "    def add_num_ratings(self, data):\n",
    "        data['num_user_ratings'] = np.count_nonzero(data['user'])\n",
    "        data['num_movie_ratings'] = np.count_nonzero(data['product'])\n",
    "        return data\n",
    "        \n",
    "    def add_top_ratings(self, data):\n",
    "        data['top_user_rating'] = np.amax(data['user'])\n",
    "        data['top_movie_rating'] = np.amax(data['product'])\n",
    "        return data\n",
    "    \n",
    "    def add_bottom_ratings(self, data):\n",
    "        data['bottom_user_rating'] = np.amin(data['user'])\n",
    "        data['bottom_movie_rating'] = np.amin(data['product'])\n",
    "        return data\n",
    "    \n",
    "    def add_average_ratings(self, data):\n",
    "        data['average_user_rating'] = np.average(data['user'])\n",
    "        data['average_movie_rating'] = np.average(data['product'])\n",
    "        return data\n",
    "    \n",
    "    def add_percent_rating(self, data):\n",
    "        l = len(np.where(data['user'] > 0.0)[0])\n",
    "        if l > 0:\n",
    "            data['percent_one_star'] = len(np.where(data['user'] == 1.0)[0])/l\n",
    "            data['percent_two_star'] = len(np.where(data['user'] == 2.0)[0])/l\n",
    "            data['percent_three_star'] = len(np.where(data['user'] == 3.0)[0])/l\n",
    "            data['percent_four_star'] = len(np.where(data['user'] == 4.0)[0])/l\n",
    "            data['percent_five_star'] = len(np.where(data['user'] == 5.0)[0])/l\n",
    "        return data\n",
    "        \n",
    "    def total_ratings(self, data):\n",
    "        count = 0.0\n",
    "        for i in data:\n",
    "            if i['score'] != 0.0:\n",
    "                count += 1\n",
    "        print(count)\n",
    "        \n",
    "    def build_model(self, deep_layers, deep_layer_activation, wide_output_dim, wide_activation):\n",
    "        \n",
    "        D = self.train.drop([\"reviewerID\", \"asin\", \"score\", \"product\", \"user\"], axis=1).shape[1] +\\\n",
    "            self.num_users * self.num_products\n",
    "        \n",
    "        with tf.name_scope(\"input\"):\n",
    "            X = tf.placeholder(tf.float32, [None, D], name=\"x_input\")\n",
    "        \n",
    "        with tf.name_scope(\"input_normalization\"):\n",
    "            X = tf.layers.batch_normalization(X)\n",
    "        \n",
    "        T = lambda x: tf.matmul(tf.reshape(x, [x.shape[0], 1]), tf.reshape(x, [1, x.shape[0]]))\n",
    "        cross_prod_mat = tf.map_fn(T, X)\n",
    "        cross_prod = tf.map_fn(lambda x: tf.reshape(x, [-1]), cross_prod_mat)\n",
    "        \n",
    "        # wide part\n",
    "        wide = tf.layers.dense(cross_prod, wide_output_dim, activation=wide_activation)\n",
    "        \n",
    "        deep = X\n",
    "        \n",
    "        # deep part\n",
    "        for i in deep_layers:\n",
    "            deep = tf.layers.dense(deep, i, activation=deep_layer_activation)\n",
    "            \n",
    "        # combine the output of wide and deep\n",
    "        wide_and_deep = tf.concat([wide, deep], 1)\n",
    "        logits = tf.layers.dense(wide_and_deep, 5, name=\"logits\")\n",
    "        \n",
    "    def add_optimizer(self, learning_rate):\n",
    "        \n",
    "        default_graph = tf.get_default_graph()\n",
    "        logits = default_graph.get_tensor_by_name(\"logits/BiasAdd:0\")\n",
    "        \n",
    "        with tf.name_scope(\"optimizer_input\"):\n",
    "            global_step = tf.placeholder(tf.int64, [], name=\"global_step\")\n",
    "            y = tf.placeholder(dtype=tf.int64, shape=[None], name=\"y_input\")\n",
    "        \n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "                xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y, 4), logits=logits)\n",
    "                with tf.name_scope(\"loss\"):\n",
    "                    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"softmax\"):\n",
    "            # Softmax layer\n",
    "            softmax = tf.nn.softmax(logits)\n",
    "            # Predict train\n",
    "            y_hat = tf.argmax(softmax, axis=1, name=\"y_hat\")\n",
    "            \n",
    "        with tf.name_scope('train'):\n",
    "            learning_rate = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.96, staircase=True)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(xentropy, name=\"training_op\")\n",
    "            \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # Train Evaluation\n",
    "            train_correct = tf.equal(y, y_hat)\n",
    "            tf.reduce_mean(tf.cast(train_correct, tf.float64), name=\"acc\")\n",
    "            \n",
    "    def train_model(self, sess, n_epochs, batch_size):\n",
    "        \n",
    "        # New***\n",
    "        init = tf.global_variables_initializer() \n",
    "\n",
    "        # get default graph\n",
    "        default_graph = tf.get_default_graph()\n",
    "        \n",
    "        # Get individual graph tensors\n",
    "        X = default_graph.get_tensor_by_name(\"input/x_input:0\")\n",
    "        Y = default_graph.get_tensor_by_name(\"optimizer_input/y_input:0\")\n",
    "        global_step = default_graph.get_tensor_by_name(\"optimizer_input/global_step:0\")\n",
    "        \n",
    "        # Get specific operations in graph\n",
    "        training_op = default_graph.get_operation_by_name(\"train/training_op\")\n",
    "        accuracy = default_graph.get_tensor_by_name(\"accuracy/acc:0\")\n",
    "        loss = default_graph.get_tensor_by_name(\"cross_entropy/loss/loss:0\")\n",
    "        \n",
    "        # Create logdir\n",
    "        now = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"tf_logs\"\n",
    "        logdir = \"{root_logdir}/run\"\n",
    "        \n",
    "        # Create Tensorboard file_writers\n",
    "        summary_writer = tf.summary.FileWriter(logdir, default_graph)\n",
    "        \n",
    "        # Create Tensorboard Summaries \n",
    "        acc_train_summary = tf.summary.scalar(\"train_accuracy\", accuracy)\n",
    "        acc_test_summary = tf.summary.scalar(\"test_accuracy\", accuracy)\n",
    "        loss_summary = tf.summary.scalar('train_loss', loss)\n",
    "        \n",
    "        # Create Tensorboard histograms\n",
    "        conv_histograms = []\n",
    "        for i in tf.global_variables():\n",
    "            if i.name[:6] == \"conv2d\":\n",
    "                # need to switch colon with underscore because colons aren't allowed in summary names\n",
    "                conv_histograms.append(tf.summary.histogram(i.name.replace(\":\", \"_\"), i))\n",
    "        \n",
    "        # Wrapper tells Tensorboard to assume everything is within same session\n",
    "        sess.run(init)\n",
    "\n",
    "        step = 0\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            n_iters = int((self.num_examples * (1 - self.perc_test)) / batch_size)\n",
    "            print_interval = 100\n",
    "            best_test = 0.\n",
    "\n",
    "            print(f\"\\nStarting epoch {epoch}, running for {n_iters} iterations\")\n",
    "            print(f\"Printing evaluation metrics every {print_interval} iterations\\n\")\n",
    "\n",
    "            for iteration in range(1, n_iters):\n",
    "                step += 1\n",
    "                indices = np.random.choice(int(self.num_examples * (1 - self.perc_test)), batch_size)\n",
    "                X_batch = self.train.iloc[indices, :]\n",
    "                user_series = X_batch[\"user\"].apply(pd.Series)\n",
    "                prod_series = X_batch[\"product\"].apply(pd.Series)\n",
    "                x_numerical_cols = X_batch.drop([\"reviewerID\", \"asin\", \"score\", \"product\", \"user\"], axis=1).dropna()\n",
    "                batch = pd.concat([x_numerical_cols, prod_series, user_series]).dropna().as_matrix()\n",
    "                y_batch = self.train.iloc[indices, :][\"score\"].as_matrix()\n",
    "                sess.run(training_op, feed_dict={X: batch,\n",
    "                                                 Y: y_batch,\n",
    "                                                 global_step: step})\n",
    "\n",
    "                if iteration % print_interval == 0:\n",
    "\n",
    "                    acc_train, train_summary = sess.run([accuracy, tf.summary.merge([loss_summary, acc_train_summary])],\n",
    "                                                        {X: X_batch, Y: y_batch})\n",
    "\n",
    "                    acc_test, test_summary = sess.run([accuracy, acc_test_summary], {X: self.test_x, Y: self.test_y})\n",
    "\n",
    "                    # Add batch train loss & accuracy to Tensorboard\n",
    "                    merged_histograms = tf.summary.merge([i.eval(session=sess) for i in conv_histograms])\n",
    "                    summary_writer.add_summary(tf.summary.merge([train_summary, test_summary, merged_histograms]).eval(session=sess), step)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                    # Print out batch evaluation metrics\n",
    "                    print(\"Iteration: {} train acc: {:.4f} test acc: {:.4f}\".format(iteration, \n",
    "                                                                                    acc_train, \n",
    "                                                                                    acc_test))\n",
    "        # Ensure file_writers closed\n",
    "        summary_writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "wad = wide_and_deep(data_importer.train, data_importer.test, data_importer.num_reviewers, data_importer.num_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "wad.build_model([128, 128], tf.nn.relu, 256, tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "wad.add_optimizer(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[74719825147969,256]\n\t [[Node: dense/kernel/Initializer/random_uniform/RandomUniform = RandomUniform[T=DT_INT64, _class=[\"loc:@dense/kernel\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense/kernel/Initializer/random_uniform/shape)]]\n\nCaused by op 'dense/kernel/Initializer/random_uniform/RandomUniform', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-86-375385517e43>\", line 1, in <module>\n    wad.build_model([128, 128], tf.nn.relu, 256, tf.nn.relu)\n  File \"<ipython-input-84-5cdfed8bc843>\", line 80, in build_model\n    wide = tf.layers.dense(cross_prod, wide_output_dim, activation=wide_activation)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 250, in dense\n    return layer.apply(inputs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 559, in __call__\n    self.build(input_shapes[0])\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 137, in build\n    trainable=True)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 458, in add_variable\n    trainable=trainable and self.trainable)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1203, in get_variable\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1092, in get_variable\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 805, in _get_single_variable\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 303, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 779, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 445, in __call__\n    shape, -limit, limit, dtype, seed=self.seed)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 240, in random_uniform\n    shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 473, in _random_uniform\n    name=name)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[74719825147969,256]\n\t [[Node: dense/kernel/Initializer/random_uniform/RandomUniform = RandomUniform[T=DT_INT64, _class=[\"loc:@dense/kernel\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense/kernel/Initializer/random_uniform/shape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[74719825147969,256]\n\t [[Node: dense/kernel/Initializer/random_uniform/RandomUniform = RandomUniform[T=DT_INT64, _class=[\"loc:@dense/kernel\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense/kernel/Initializer/random_uniform/shape)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-b03242ab4d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-5cdfed8bc843>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, sess, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Wrapper tells Tensorboard to assume everything is within same session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[74719825147969,256]\n\t [[Node: dense/kernel/Initializer/random_uniform/RandomUniform = RandomUniform[T=DT_INT64, _class=[\"loc:@dense/kernel\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense/kernel/Initializer/random_uniform/shape)]]\n\nCaused by op 'dense/kernel/Initializer/random_uniform/RandomUniform', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-86-375385517e43>\", line 1, in <module>\n    wad.build_model([128, 128], tf.nn.relu, 256, tf.nn.relu)\n  File \"<ipython-input-84-5cdfed8bc843>\", line 80, in build_model\n    wide = tf.layers.dense(cross_prod, wide_output_dim, activation=wide_activation)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 250, in dense\n    return layer.apply(inputs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 671, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 559, in __call__\n    self.build(input_shapes[0])\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 137, in build\n    trainable=True)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 458, in add_variable\n    trainable=trainable and self.trainable)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1203, in get_variable\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1092, in get_variable\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 805, in _get_single_variable\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 303, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 779, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 445, in __call__\n    shape, -limit, limit, dtype, seed=self.seed)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 240, in random_uniform\n    shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 473, in _random_uniform\n    name=name)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/michael/Documents/deep_learning/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[74719825147969,256]\n\t [[Node: dense/kernel/Initializer/random_uniform/RandomUniform = RandomUniform[T=DT_INT64, _class=[\"loc:@dense/kernel\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dense/kernel/Initializer/random_uniform/shape)]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "wad.train_model(sess, 10, 100)\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
