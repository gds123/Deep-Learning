{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/Documents/deep_learning/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import simplejson\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataImporter(object):\n",
    "    def __init__(self, path_to_data):\n",
    "        # get the raw data\n",
    "        raw_data = [json.loads(i) for i in gzip.open(path_to_data, \"rt\")]\n",
    "        raw_data = raw_data[:20]\n",
    "        self.data = pd.DataFrame(raw_data)\n",
    "        self.num_reviews = len(self.data)\n",
    "        \n",
    "        # get the counts of users and products\n",
    "        self.reviewers = self.data[\"reviewerID\"].unique()\n",
    "        self.num_reviewers = len(self.reviewers)\n",
    "        self.products = self.data[\"asin\"].unique()\n",
    "        self.num_products = len(self.products)\n",
    "        \n",
    "        # create int-to-name dictionaries for each\n",
    "        self.reviewer_to_num = {reviewer.lower(): idx for idx, reviewer in enumerate(self.reviewers)}\n",
    "        self.num_to_reviewer = {self.reviewer_to_num[reviewer]: reviewer for reviewer in self.reviewer_to_num}\n",
    "        self.product_to_num = {product.lower(): idx for idx, product in enumerate(self.products)}\n",
    "        self.num_to_product = {self.product_to_num[product]: product for product in self.product_to_num}\n",
    "        \n",
    "    def create_train_test_split(self, frac_test):\n",
    "        # get test data indices\n",
    "        test_indices = np.random.choice(self.num_reviews,\n",
    "                                         size=int(self.num_reviews * frac_test),\n",
    "                                         replace=False)\n",
    "        \n",
    "        # split raw data into train/test\n",
    "        raw_test = self.data.iloc[test_indices, :]\n",
    "        raw_train = self.data.drop(test_indices).dropna()\n",
    "        \n",
    "        # get dimensions of the matrices\n",
    "        dim = (self.num_reviewers, self.num_products)\n",
    "        self.test_matrix = self.populate_user_product_review_matrix(dim, raw_test)\n",
    "        self.train_matrix = self.populate_user_product_review_matrix(dim, raw_train)\n",
    "        print(self.train_matrix)\n",
    "        \n",
    "        self.test = self.permute_matrix(self.test_matrix)\n",
    "        self.train = self.permute_matrix(self.train_matrix)\n",
    "        \n",
    "    def permute_matrix(self, mat):\n",
    "        perm = []\n",
    "        for u_idx, row in enumerate(mat):\n",
    "            for prod_idx, score in enumerate(row):\n",
    "                if score > 0.0:\n",
    "                    print(row)\n",
    "                    print(score)\n",
    "                    user_cpy = row.copy()\n",
    "                    score = user_cpy[prod_idx]\n",
    "                    user_cpy[prod_idx] = 0\n",
    "                    prod_cpy = mat[:, prod_idx]\n",
    "                    prod_cpy[u_idx] = 0\n",
    "                    perm.append({\"user\": user_cpy,\n",
    "                                 \"reviewerID\": self.num_to_reviewer[u_idx],\n",
    "                                 \"product\": prod_cpy,\n",
    "                                 \"score\": score,\n",
    "                                 \"asin\": self.num_to_product[prod_idx]\n",
    "                                 })\n",
    "        return perm\n",
    "    \n",
    "    def populate_user_product_review_matrix(self, dimensions, dataset):\n",
    "        assert isinstance(dataset, pd.DataFrame)\n",
    "        ret_matrix = np.zeros(dimensions)\n",
    "        for idx, row in dataset.iterrows():\n",
    "            ret_matrix[self.reviewer_to_num[row[\"reviewerID\"].lower()], self.product_to_num[row[\"asin\"].lower()]] = row[\"overall\"]\n",
    "        return ret_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 4.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.]\n",
      " [ 0.  3.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  5.  0.  5.]\n",
      " [ 0.  3.  0.  0.]\n",
      " [ 0.  4.  0.  0.]\n",
      " [ 0.  0.  4.  0.]\n",
      " [ 0.  0.  3.  0.]\n",
      " [ 0.  0.  3.  5.]\n",
      " [ 0.  0.  5.  0.]\n",
      " [ 0.  0.  2.  0.]\n",
      " [ 0.  0.  0.  3.]\n",
      " [ 0.  0.  0.  3.]]\n",
      "[ 5.  0.  0.  0.]\n",
      "5.0\n",
      "[ 0.  3.  0.  0.]\n",
      "3.0\n",
      "[ 2.  0.  0.  0.]\n",
      "2.0\n",
      "[ 1.  0.  0.  0.]\n",
      "1.0\n",
      "[ 4.  0.  0.  0.]\n",
      "4.0\n",
      "[ 5.  0.  0.  0.]\n",
      "5.0\n",
      "[ 5.  0.  0.  0.]\n",
      "5.0\n",
      "[ 0.  3.  0.  0.]\n",
      "3.0\n",
      "[ 0.  5.  0.  5.]\n",
      "5.0\n",
      "[ 0.  0.  0.  5.]\n",
      "5.0\n",
      "[ 0.  3.  0.  0.]\n",
      "3.0\n",
      "[ 0.  4.  0.  0.]\n",
      "4.0\n",
      "[ 0.  0.  4.  0.]\n",
      "4.0\n",
      "[ 0.  0.  3.  0.]\n",
      "3.0\n",
      "[ 0.  0.  3.  5.]\n",
      "3.0\n",
      "[ 0.  0.  0.  5.]\n",
      "5.0\n",
      "[ 0.  0.  5.  0.]\n",
      "5.0\n",
      "[ 0.  0.  2.  0.]\n",
      "2.0\n",
      "[ 0.  0.  0.  3.]\n",
      "3.0\n",
      "[ 0.  0.  0.  3.]\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "data_importer = DataImporter(\"data/reviews_Amazon_Instant_Video_5.json.gz\")\n",
    "data_importer.create_train_test_split(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wide_and_deep(object):\n",
    "    \n",
    "    def __init__(self, train, test):\n",
    "        \n",
    "        self.train = np.copy(train)\n",
    "        self.test = np.copy(test)\n",
    "        \n",
    "        np.random.shuffle(self.train)\n",
    "        np.random.shuffle(self.test)\n",
    "        \n",
    "        self.num_examples = self.train.shape[0] + self.test.shape[0]\n",
    "        self.perc_test = self.test.shape[0] / self.train.shape[0]\n",
    "        \n",
    "        self.train = self.add_all_features_columns(self.train)\n",
    "        self.test = self.add_all_features_columns(self.test)\n",
    "        \n",
    "    def add_all_features_columns(self, dataset):\n",
    "        dataset = np.array(list(map(self.add_num_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_top_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_bottom_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_average_ratings, dataset)) )\n",
    "        dataset = np.array(list(map(self.add_percent_rating, dataset)) )\n",
    "        \n",
    "        dataset = pd.DataFrame(list(dataset))\n",
    "        return dataset\n",
    "        \n",
    "    def add_num_ratings(self, data):\n",
    "        data['num_user_ratings'] = np.count_nonzero(data['user'])\n",
    "        data['num_movie_ratings'] = np.count_nonzero(data['product'])\n",
    "        return data\n",
    "        \n",
    "    def add_top_ratings(self, data):\n",
    "        data['top_user_rating'] = np.amax(data['user'])\n",
    "        data['top_movie_rating'] = np.amax(data['product'])\n",
    "        return data\n",
    "    \n",
    "    def add_bottom_ratings(self, data):\n",
    "        data['bottom_user_rating'] = np.amin(data['user'])\n",
    "        data['bottom_movie_rating'] = np.amin(data['product'])\n",
    "        return data\n",
    "    \n",
    "    def add_average_ratings(self, data):\n",
    "        data['average_user_rating'] = np.average(data['user'])\n",
    "        data['average_movie_rating'] = np.average(data['product'])\n",
    "        return data\n",
    "    \n",
    "    def add_percent_rating(self, data):\n",
    "        data['percent_one_star'] = len(np.where(data['user'] == 1.0)[0])/len(data['user'])\n",
    "        data['percent_two_star'] = len(np.where(data['user'] == 2.0)[0])/len(data['user'])\n",
    "        data['percent_three_star'] = len(np.where(data['user'] == 3.0)[0])/len(data['user'])\n",
    "        data['percent_four_star'] = len(np.where(data['user'] == 4.0)[0])/len(data['user'])\n",
    "        data['percent_five_star'] = len(np.where(data['user'] == 5.0)[0])/len(data['user'])\n",
    "        return data\n",
    "        \n",
    "    def total_ratings(self, data):\n",
    "        count = 0.0\n",
    "        for i in data:\n",
    "            if i['score'] != 0.0:\n",
    "                count += 1\n",
    "        print(count)\n",
    "        \n",
    "    def build_model(self, deep_layers, deep_layer_activation, wide_output_dim, wide_activation):\n",
    "        \n",
    "        N, D = self.train.shape\n",
    "        \n",
    "        with tf.name_scope(\"input\"):\n",
    "            X = tf.placeholder(tf.float32, [None, D], name=\"x_input\")\n",
    "        \n",
    "        with tf.name_scope(\"input_normalization\"):\n",
    "            X = tf.layers.batch_normalization(X)\n",
    "        \n",
    "        T = lambda x: tf.matmul(tf.reshape(x, [x.shape[0], 1]), tf.reshape(x, [1, x.shape[0]]))\n",
    "        cross_prod_mat = tf.map_fn(T, X)\n",
    "        cross_prod = tf.map_fn(lambda x: tf.reshape(x, [-1]), cross_prod_mat)\n",
    "        \n",
    "        # wide part\n",
    "        wide = tf.layers.dense(cross_prod, wide_output_dim, activation=wide_activation)\n",
    "        \n",
    "        deep = X\n",
    "        \n",
    "        # deep part\n",
    "        for i in deep_layers:\n",
    "            deep = tf.layers.dense(deep, i, activation=deep_layer_activation)\n",
    "            \n",
    "        # combine the output of wide and deep\n",
    "        wide_and_deep = tf.concat([wide, deep], 1)\n",
    "        logits = tf.layers.dense(wide_and_deep, 5, name=\"logits\")\n",
    "        \n",
    "    def add_optimizer(self, learning_rate):\n",
    "        \n",
    "        default_graph = tf.get_default_graph()\n",
    "        logits = default_graph.get_tensor_by_name(\"logits/BiasAdd:0\")\n",
    "        \n",
    "        with tf.name_scope(\"optimizer_input\"):\n",
    "            global_step = tf.placeholder(tf.int64, [], name=\"global_step\")\n",
    "            y = tf.placeholder(dtype=tf.int64, shape=[None], name=\"y_input\")\n",
    "        \n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "                xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(y, 4), logits=logits)\n",
    "                with tf.name_scope(\"loss\"):\n",
    "                    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"softmax\"):\n",
    "            # Softmax layer\n",
    "            softmax = tf.nn.softmax(logits)\n",
    "            # Predict train\n",
    "            y_hat = tf.argmax(softmax, axis=1, name=\"y_hat\")\n",
    "            \n",
    "        with tf.name_scope('train'):\n",
    "            learning_rate = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.96, staircase=True)\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(xentropy, name=\"training_op\")\n",
    "            \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # Train Evaluation\n",
    "            train_correct = tf.equal(y, y_hat)\n",
    "            tf.reduce_mean(tf.cast(train_correct, tf.float64), name=\"acc\")\n",
    "            \n",
    "    def train_model(self, sess, n_epochs, batch_size):\n",
    "        \n",
    "        # New***\n",
    "        init = tf.global_variables_initializer() \n",
    "\n",
    "        # get default graph\n",
    "        default_graph = tf.get_default_graph()\n",
    "        \n",
    "        # Get individual graph tensors\n",
    "        X = default_graph.get_tensor_by_name(\"input/x_input:0\")\n",
    "        Y = default_graph.get_tensor_by_name(\"optimizer_input/y_input:0\")\n",
    "        global_step = default_graph.get_tensor_by_name(\"optimizer_input/global_step:0\")\n",
    "        \n",
    "        # Get specific operations in graph\n",
    "        training_op = default_graph.get_operation_by_name(\"train/training_op\")\n",
    "        accuracy = default_graph.get_tensor_by_name(\"accuracy/acc:0\")\n",
    "        loss = default_graph.get_tensor_by_name(\"cross_entropy/loss/loss:0\")\n",
    "        \n",
    "        # Create logdir\n",
    "        now = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "        root_logdir = \"tf_logs\"\n",
    "        logdir = \"{root_logdir}/run\"\n",
    "        \n",
    "        # Create Tensorboard file_writers\n",
    "        summary_writer = tf.summary.FileWriter(logdir, default_graph)\n",
    "        \n",
    "        # Create Tensorboard Summaries \n",
    "        acc_train_summary = tf.summary.scalar(\"train_accuracy\", accuracy)\n",
    "        acc_test_summary = tf.summary.scalar(\"test_accuracy\", accuracy)\n",
    "        loss_summary = tf.summary.scalar('train_loss', loss)\n",
    "        \n",
    "        # Create Tensorboard histograms\n",
    "        conv_histograms = []\n",
    "        for i in tf.global_variables():\n",
    "            if i.name[:6] == \"conv2d\":\n",
    "                # need to switch colon with underscore because colons aren't allowed in summary names\n",
    "                conv_histograms.append(tf.summary.histogram(i.name.replace(\":\", \"_\"), i))\n",
    "        \n",
    "        # Wrapper tells Tensorboard to assume everything is within same session\n",
    "        sess.run(init)\n",
    "\n",
    "        step = 0\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            n_iters = int((self.num_examples * (1 - self.perc_test)) / batch_size)\n",
    "            print_interval = 100\n",
    "            best_test = 0.\n",
    "\n",
    "            print(f\"\\nStarting epoch {epoch}, running for {n_iters} iterations\")\n",
    "            print(f\"Printing evaluation metrics every {print_interval} iterations\\n\")\n",
    "\n",
    "            for iteration in range(1, n_iters):\n",
    "                step += 1\n",
    "                indices = np.random.choice(int(self.num_examples * (1 - self.perc_test)), batch_size)\n",
    "                X_batch = self.train[indices]\n",
    "                y_batch = self.train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch,\n",
    "                                                 Y: y_batch,\n",
    "                                                 global_step: step})\n",
    "\n",
    "                if iteration % print_interval == 0:\n",
    "\n",
    "                    acc_train, train_summary = sess.run([accuracy, tf.summary.merge([loss_summary, acc_train_summary])],\n",
    "                                                        {X: X_batch, Y: y_batch})\n",
    "\n",
    "                    acc_test, test_summary = sess.run([accuracy, acc_test_summary], {X: self.test_x, Y: self.test_y})\n",
    "\n",
    "                    # Add batch train loss & accuracy to Tensorboard\n",
    "                    merged_histograms = tf.summary.merge([i.eval(session=sess) for i in conv_histograms])\n",
    "                    summary_writer.add_summary(tf.summary.merge([train_summary, test_summary, merged_histograms]).eval(session=sess), step)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                    # Print out batch evaluation metrics\n",
    "                    print(\"Iteration: {} train acc: {:.4f} test acc: {:.4f}\".format(iteration, \n",
    "                                                                                    acc_train, \n",
    "                                                                                    acc_test))\n",
    "        # Ensure file_writers closed\n",
    "        summary_writer.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "wad = wide_and_deep(data_importer.train, data_importer.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wad.build_model([128, 128], tf.nn.relu, 256, tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wad.add_optimizer(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 1, running for 1777 iterations\n",
      "Printing evaluation metrics every 100 iterations\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[1697] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-921d5ce670db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-3002edefbbce>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, sess, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_examples\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 sess.run(training_op, feed_dict={X: X_batch,\n",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep_learning/venv/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1269\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[1697] not in index'"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "wad.train_model(sess, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>average_movie_rating</th>\n",
       "      <th>average_user_rating</th>\n",
       "      <th>bottom_movie_rating</th>\n",
       "      <th>bottom_user_rating</th>\n",
       "      <th>num_movie_ratings</th>\n",
       "      <th>num_user_ratings</th>\n",
       "      <th>percent_five_star</th>\n",
       "      <th>percent_four_star</th>\n",
       "      <th>percent_one_star</th>\n",
       "      <th>percent_three_star</th>\n",
       "      <th>percent_two_star</th>\n",
       "      <th>product</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>score</th>\n",
       "      <th>top_movie_rating</th>\n",
       "      <th>top_user_rating</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [asin, average_movie_rating, average_user_rating, bottom_movie_rating, bottom_user_rating, num_movie_ratings, num_user_ratings, percent_five_star, percent_four_star, percent_one_star, percent_three_star, percent_two_star, product, reviewerID, score, top_movie_rating, top_user_rating, user]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wad.train.where(wad.train[\"average_movie_rating\"] > 0.0).dropna()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
