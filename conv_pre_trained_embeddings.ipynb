{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/michael/Documents/deep_learning/venv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import time\n",
    "import scipy.spatial\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def initEmbeddingMap(fileName):\n",
    "    print(\"initializing embeddings\")\n",
    "    with open(join(\"data\", \"glove.6B\", fileName)) as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in [line.split() for line in glove]}\n",
    "\n",
    "def clean(text):\n",
    "    return text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                 lower=True, split=\" \")\n",
    "\n",
    "\n",
    "# returns dict with users and movies they rated as repeated rows\n",
    "# cleans review text and add to rawOutput\n",
    "def initRawData(input_file):\n",
    "    print(\"initializing raw data\")\n",
    "    rawInputData = []\n",
    "    rawOutputData = []\n",
    "    with open(input_file,\"r\") as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            if len(line) < 4:\n",
    "                break\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj[\"reviewerID\"]\n",
    "            item = lineObj[\"asin\"]\n",
    "            rawInputDataObj = {\"user\": user, \"asin\": item}\n",
    "            rawOutputDataObj = clean(lineObj[\"reviewText\"])\n",
    "            rawInputData.append(rawInputDataObj)\n",
    "            rawOutputData.append(rawOutputDataObj)\n",
    "    return rawInputData, rawOutputData\n",
    "\n",
    "# creates dict of usrs w/ all movies rated + movies w/ all user ratings *** \n",
    "def group_data(inputData):\n",
    "    users = {}\n",
    "    items = {}\n",
    "    for datum in inputData:\n",
    "        u = datum[\"user\"]\n",
    "        i = datum[\"asin\"]\n",
    "        users.setdefault(u, []).append(i)\n",
    "        items.setdefault(i, []).append(u)\n",
    "    return users, items\n",
    "\n",
    "def getSetFromData(key, data):\n",
    "    return set([datum.get(key) for datum in data])\n",
    "\n",
    "def seq_2_matrix(sequence, embedding_map):\n",
    "    return np.array([embedding_map.get(word) for word in sequence if word in embedding_map])\n",
    "\n",
    "def matrix_2_avg(emb_matrix):\n",
    "    return np.mean(emb_matrix, 0)\n",
    "\n",
    "\n",
    "# utils - one hot encodes all data \n",
    "def initVecData(rawInputData, rawOutputData, embedding_map):\n",
    "    print('initializing vectorized data')\n",
    "    dictVect = DictVectorizer()\n",
    "    vecInputData = dictVect.fit_transform(rawInputData).toarray()\n",
    "    vecOutputData = [matrix_2_avg(seq_2_matrix(review, embedding_map)) for review in rawOutputData]\n",
    "    return vecInputData, vecOutputData\n",
    "\n",
    "def initMatInputData(rawInputData, rawOutputData, embedding_map, save=False):\n",
    "    print('initializing matrix data')\n",
    "    if len(rawInputData) != len(rawOutputData):\n",
    "        raise ValueError(\"Need same size of input and output\")\n",
    "    users = {}\n",
    "    extra_info = {}\n",
    "    items = {}\n",
    "    dictVect = DictVectorizer()\n",
    "    for i in range(len(rawInputData)):\n",
    "        vecOutput = seq_2_matrix(rawOutputData[i], embedding_map)\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        users.setdefault(user, []).append(vecOutput)\n",
    "        items.setdefault(item, []).append(vecOutput)\n",
    "        \n",
    "    matUserInputData = []\n",
    "    matItemInputData = []\n",
    "    users = {k: np.vstack(v) for k, v in users.items()}\n",
    "    items = {k: np.vstack(v) for k, v in items.items()}\n",
    "    extra_info['user_seq_sizes'] = [m.shape[0] for m in users.values()]\n",
    "    extra_info['item_seq_sizes'] = [m.shape[0] for m in items.values()]\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        matUserInputData.append(users.get(user))\n",
    "        matItemInputData.append(items.get(item))\n",
    "    return matUserInputData, matItemInputData, extra_info\n",
    "\n",
    "def toKey(user, item):\n",
    "    return (user, item)\n",
    "\n",
    "def initRatingsOutputData(rawInputData, input_file, save=False):\n",
    "    ratingsData = []\n",
    "    userItemDict = {}\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        userItem = toKey(rawInput['user'], rawInput['asin'])\n",
    "        userItemDict[userItem] = i\n",
    "        ratingsData.append(None) # check later to make sure no Nones left\n",
    "        \n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rating = lineObj['overall']\n",
    "            i = userItemDict.get(toKey(user, item))\n",
    "            if i is not None:\n",
    "                ratingsData[i] = rating\n",
    "        failure = None in ratingsData\n",
    "        if failure:\n",
    "            raise ValueError(str(len([r for r in ratingsData if r is None])) + \" reviews did not have corresponding rating.\")\n",
    "    return ratingsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing raw data\n"
     ]
    }
   ],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "rawInputData, rawOutputData = initRawData(input_file=fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, movies = group_data(rawInputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idxs = np.random.permutation(len(rawOutputData))\n",
    "rawInputData = [rawInputData[i] for i in rand_idxs]\n",
    "rawOutputData = [rawOutputData[i] for i in rand_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing embeddings\n"
     ]
    }
   ],
   "source": [
    "embedding_map = initEmbeddingMap(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing vectorized data\n"
     ]
    }
   ],
   "source": [
    "all_users = getSetFromData('user', rawInputData)\n",
    "all_movies = getSetFromData('asin', rawInputData)\n",
    "vecInputData, vecOutputData = initVecData(rawInputData, rawOutputData, embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing matrix data\n"
     ]
    }
   ],
   "source": [
    "matUserInputData, matMovieInputData, extra_info = initMatInputData(rawInputData, rawOutputData, embedding_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "ratingsData = initRatingsOutputData(rawInputData, input_file=fileName,save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Dot\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.layers.merge import Add, Dot, Concatenate\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoNN():\n",
    "    def __init__(self, embedding_size, hidden_size, u_seq_len, m_seq_len, filters=2, kernel_size=8, strides=6):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.inputU, self.towerU = self.create_deepconn_tower(u_seq_len)\n",
    "        self.inputM, self.towerM = self.create_deepconn_tower(m_seq_len)\n",
    "        self.joined = Concatenate()([self.towerU, self.towerM])\n",
    "        self.outNeuron = Dense(1)(self.joined)\n",
    "\n",
    "    def create_deepconn_tower(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        tower = Conv1D(filters=self.filters, kernel_size=self.kernel_size, activation=\"relu\")(input_layer)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Flatten()(tower)\n",
    "        tower = Dense(self.hidden_size, activation=\"relu\")(tower)\n",
    "        return input_layer, tower\n",
    "\n",
    "    def create_deepconn_dp(self):\n",
    "        dotproduct = Dot(axes=1)([self.towerU, self.towerM])\n",
    "        output = Add()([self.outNeuron, dotproduct])\n",
    "        self.model = Model(inputs=[self.inputU, self.inputM], outputs=[output])\n",
    "        self.model.compile(optimizer='Adam', loss='mse')\n",
    "        \n",
    "    def train(self, matUserInputData, matItemInputData, ratingsData, u_seq_len=200, i_seq_len=200, epochs=3500, training=None):\n",
    "        \n",
    "        tensorboard = TensorBoard(log_dir=\"tf_logs/{}\".format(time()))\n",
    "        self.create_deepconn_dp()\n",
    "        \n",
    "        self.user_input = pad_sequences(np.asarray(matUserInputData), maxlen=u_seq_len)\n",
    "        self.item_input = pad_sequences(np.asarray(matItemInputData), maxlen=i_seq_len)\n",
    "\n",
    "        self.trainingN = int(len(user_input) * training) if type(training) is float else training\n",
    "\n",
    "        self.outputs = np.asarray(ratingsData)\n",
    "        print(self.model.summary())\n",
    "\n",
    "        self.train_inputs = [self.user_input[:self.trainingN], self.item_input[:self.trainingN]]\n",
    "        self.train_outputs = self.outputs[:self.trainingN]\n",
    "        self.test_inputs = [self.user_input[self.trainingN:], self.item_input[self.trainingN:]]\n",
    "        self.test_outputs = self.outputs[self.trainingN:]\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=4)\n",
    "        early_stopping_val = EarlyStopping(monitor='val_loss', patience=6)\n",
    "        batch_size = 32\n",
    "        \n",
    "        self.history = self.model.fit(self.train_inputs, self.train_outputs, callbacks=[early_stopping, early_stopping_val, tensorboard], validation_split=0.2, batch_size=batch_size, epochs=epochs)\n",
    "        self.predicts = self.model.predict(self.test_inputs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 141, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 474, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 134, 2)       802         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 467, 2)       802         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 67, 2)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 233, 2)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 134)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 466)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            540         flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            1868        flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8)            0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            9           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dense_6[0][0]                    \n",
      "                                                                 dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,021\n",
      "Trainable params: 4,021\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 14850 samples, validate on 3713 samples\n",
      "Epoch 1/20\n",
      "14850/14850 [==============================] - 29s 2ms/step - loss: 1.8817 - val_loss: 1.2937\n",
      "Epoch 2/20\n",
      "14850/14850 [==============================] - 24s 2ms/step - loss: 1.2338 - val_loss: 1.1453\n",
      "Epoch 3/20\n",
      "14850/14850 [==============================] - 21s 1ms/step - loss: 1.1694 - val_loss: 1.1030\n",
      "Epoch 4/20\n",
      "14850/14850 [==============================] - 20s 1ms/step - loss: 1.0759 - val_loss: 1.0926\n",
      "Epoch 5/20\n",
      "14850/14850 [==============================] - 22s 1ms/step - loss: 1.0394 - val_loss: 1.0855\n",
      "Epoch 6/20\n",
      "14850/14850 [==============================] - 20s 1ms/step - loss: 1.0302 - val_loss: 1.0705\n",
      "Epoch 7/20\n",
      "14850/14850 [==============================] - 19s 1ms/step - loss: 1.0048 - val_loss: 1.0882\n",
      "Epoch 8/20\n",
      "14850/14850 [==============================] - 20s 1ms/step - loss: 0.9898 - val_loss: 1.0810\n",
      "Epoch 9/20\n",
      "14850/14850 [==============================] - 21s 1ms/step - loss: 0.9796 - val_loss: 1.0835\n",
      "Epoch 10/20\n",
      "14850/14850 [==============================] - 21s 1ms/step - loss: 0.9576 - val_loss: 1.2052\n",
      "Epoch 11/20\n",
      "14850/14850 [==============================] - 24s 2ms/step - loss: 0.9591 - val_loss: 1.0908\n",
      "Epoch 12/20\n",
      "14850/14850 [==============================] - 21s 1ms/step - loss: 0.9363 - val_loss: 1.0954\n"
     ]
    }
   ],
   "source": [
    "# Calculates median user review length and item length. We then pad each review to these numbers\n",
    "ptile = 50\n",
    "u_seq_len = int(np.percentile(np.array(extra_info['user_seq_sizes']), ptile))\n",
    "i_seq_len = int(np.percentile(np.array(extra_info['item_seq_sizes']), ptile))\n",
    "embed_dims = matUserInputData[0].shape[1]\n",
    "hidden_size = 4\n",
    "deepconn = DeepCoNN(embed_dims, hidden_size, u_seq_len, i_seq_len)\n",
    "\n",
    "deepconn.train(matUserInputData, matMovieInputData, ratingsData, \n",
    "           u_seq_len=u_seq_len, i_seq_len=i_seq_len,\n",
    "           epochs=20, training=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/run_data/conv_50dEmb_50ptile.npy\", deepconn.history.history)\n",
    "# x = np.load(\"data/run_data/conv_50dEmb_50ptile.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def graph_loss(loss_arr, title):\n",
    "    plt.scatter(x=range(len(loss)), y=loss)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "history = histories[0]\n",
    "loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "graph_loss(loss, \"Training Loss\")\n",
    "graph_loss(validation_loss, \"Validation Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def graph_loss(loss_arr, title):\n",
    "    plt.scatter(x=range(len(loss)), y=loss)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "graph_loss(loss, \"Training Loss\")\n",
    "graph_loss(validation_loss, \"Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this just as good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
