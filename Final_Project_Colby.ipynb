{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import simplejson\n",
    "import json\n",
    "import time\n",
    "import scipy.spatial\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer as tokenizer\n",
    "\n",
    "# utils\n",
    "def initEmbeddingMap(fileName='glove.6B.50d.txt'):\n",
    "    embedding_map = {}\n",
    "    BASE_DIR = ''\n",
    "    GLOVE_DIR = os.path.join(BASE_DIR, 'data/glove.6B')\n",
    "    glove = open(os.path.join(GLOVE_DIR, fileName))\n",
    "    \n",
    "    print('constructing embedding dictionary')\n",
    "    embedding_map = {}\n",
    "    for line in glove:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        value = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_map[word] = value\n",
    "    glove.close()\n",
    "    return embedding_map\n",
    "\n",
    "def clean(text):\n",
    "    return text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                 lower=True, split=\" \")\n",
    "\n",
    "\n",
    "# returns dict with users and movies they rated as repeated rows\n",
    "# cleans review text and add to rawOutput\n",
    "def initRawData(input_file, save=False):\n",
    "    print('initializing raw data')\n",
    "    rawInputData = []\n",
    "    rawOutputData = []\n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            if len(line) < 4:\n",
    "                break\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rawInputDataObj = {'user':user, 'asin':item}\n",
    "            rawOutputDataObj = clean(lineObj['reviewText'])\n",
    "            rawInputData.append(rawInputDataObj)\n",
    "            rawOutputData.append(rawOutputDataObj)\n",
    "#     if save:\n",
    "#         pickle.dump((rawInputData, rawOutputData), open(fileName,'wb'))\n",
    "    return rawInputData, rawOutputData\n",
    "\n",
    "# creates dict of usrs w/ all movies rated + movies w/ all user ratings *** \n",
    "def group_data(inputData):\n",
    "    users = {}\n",
    "    items = {}\n",
    "    for datum in inputData:\n",
    "        u = datum['user']\n",
    "        i = datum['asin']\n",
    "        users.setdefault(u, []).append(i)\n",
    "        items.setdefault(i, []).append(u)\n",
    "    return (users, items)\n",
    "\n",
    "# utils - doesnt seem necessary\n",
    "def getSetFromData(key, data):\n",
    "    result = set()\n",
    "    for datum in data:\n",
    "        result.add(datum.get(key))\n",
    "    return result\n",
    "\n",
    "def seq_2_matrix(sequence, embedding_map):\n",
    "    m = []\n",
    "    for word in sequence:\n",
    "        emb = embedding_map.get(word)\n",
    "        if emb is not None:\n",
    "            m.append(emb)\n",
    "    return np.array(m)\n",
    "\n",
    "def matrix_2_avg(emb_matrix):\n",
    "    return np.mean(emb_matrix, 0)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# utils - one hot encodes all data \n",
    "def initVecData(rawInputData, rawOutputData, embedding_map):\n",
    "    print('initializing vectorized data')\n",
    "    dictVect = DictVectorizer()\n",
    "    vecInputData = dictVect.fit_transform(rawInputData).toarray()\n",
    "    vecOutputData = [matrix_2_avg(seq_2_matrix(review, embedding_map)) for review in rawOutputData]\n",
    "#     if save:\n",
    "#         pickle.dump((vecInputData, vecOutputData), open(fileName,'wb'))\n",
    "    return vecInputData, vecOutputData\n",
    "\n",
    "def initMatInputData(rawInputData, rawOutputData, embedding_map, save=False):\n",
    "    print('initializing matrix data')\n",
    "    if len(rawInputData) != len(rawOutputData):\n",
    "        raise ValueError(\"Need same size of input and output\")\n",
    "    users = {}\n",
    "    extra_info = {}\n",
    "    items = {}\n",
    "    dictVect = DictVectorizer()\n",
    "    for i in range(len(rawInputData)):\n",
    "        vecOutput = seq_2_matrix(rawOutputData[i], embedding_map)\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        users.setdefault(user, []).append(vecOutput)\n",
    "        items.setdefault(item, []).append(vecOutput)\n",
    "        \n",
    "    matUserInputData = []\n",
    "    matItemInputData = []\n",
    "    users = {k: np.vstack(v) for k, v in users.items()}\n",
    "    items = {k: np.vstack(v) for k, v in items.items()}\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        matUserInputData.append(users.get(user))\n",
    "        matItemInputData.append(items.get(item))\n",
    "#     if save:\n",
    "#         pickle.dump((matUserInputData, matItemInputData), open(fileName,'wb'))\n",
    "    return matUserInputData, matItemInputData\n",
    "\n",
    "def toKey(user, item):\n",
    "    return (user, item)\n",
    "\n",
    "def initRatingsOutputData(rawInputData, input_file, save=False):\n",
    "    ratingsData = []\n",
    "    userItemDict = {}\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        userItem = toKey(rawInput['user'], rawInput['asin'])\n",
    "        userItemDict[userItem] = i\n",
    "        ratingsData.append(None) # check later to make sure no Nones left\n",
    "        \n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rating = lineObj['overall']\n",
    "            #rating = float(terms[2]) / 2.5 - 1.0\n",
    "            i = userItemDict.get(toKey(user, item))\n",
    "            if i is not None:\n",
    "                ratingsData[i] = rating\n",
    "        failure = None in ratingsData\n",
    "        if failure:\n",
    "            raise ValueError(str(len([r for r in ratingsData if r is None])) + \" reviews did not have corresponding rating.\")\n",
    "#     if save:\n",
    "#         pickle.dump(ratingsData, open(fileName,'wb'))\n",
    "    return ratingsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_initRawData( )_** - returns rawInputData and rawOutputData.\n",
    "\n",
    "**_rawInputData:_** List of dicts where each dict is a userID and movieID grouping. Groupings are unqiue but a user \n",
    "or movie can/do repeat if multiple users have reviewed a particular movie\n",
    "    \n",
    "**_rawOutputData:_** List of lists where each sub list is text from a unique review. The reviews correspond to the\n",
    "user/movie key pairs in rawInputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing raw data\n",
      "Number of user/item data: 18563\n",
      "Number of reviews: 18563\n",
      "\n",
      "[{'user': 'A3BC8O2KCL29V2', 'asin': 'B000H00VBQ'}, {'user': 'A1RJPIGRSNX4PW', 'asin': 'B000H00VBQ'}, {'user': 'A1POFVVXUZR3IQ', 'asin': 'B000H00VBQ'}, {'user': 'ATASGS8HZHGIB', 'asin': 'B000H0X79O'}, {'user': 'AUX8EUBNTHIIU', 'asin': 'B000H0X79O'}]\n",
      "\n",
      "['i', 'highly', 'recommend', 'this', 'series', 'it', 'is', 'a', 'must', 'for', 'anyone', 'who', 'is', 'yearning', 'to', 'watch', 'grown', 'up', 'television', 'complex', 'characters', 'and', 'plots', 'to', 'keep', 'one', 'totally', 'involved', 'thank', 'you', 'amazin', 'prime']\n",
      "\n",
      "['mysteries', 'are', 'interesting', 'the', 'tension', 'between', 'robson', 'and', 'the', 'tall', 'blond', 'is', 'good', 'but', 'not', 'always', 'believable', 'she', 'often', 'seemed', 'uncomfortable']\n"
     ]
    }
   ],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "rawInputData, rawOutputData = initRawData(input_file=fileName, save=False)\n",
    "print(\"Number of user/item data:\", len(rawInputData))\n",
    "print(\"Number of reviews:\", len(rawOutputData))\n",
    "print()\n",
    "print(rawInputData[0:5])\n",
    "print()\n",
    "print(rawOutputData[0])\n",
    "print()\n",
    "print(rawOutputData[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_group-data( )_** - returns dicts of userID/movieID and movieID/userID groups\n",
    "\n",
    "**_users:_** Dict where each userID key has a corresponding list of movieID they've' reviewed\n",
    "    \n",
    "**_movies:_** Dict where each movieID key has all the userIDs of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All movies userID 'A3BC8O2KCL29V2' has seen:\n",
      " ['B000H00VBQ', 'B00F0CLHQO', 'B00I3MMN4I']\n",
      "\n",
      "All userIDs of watches of movie 'B00F0CLHQO':\n",
      " ['A3BC8O2KCL29V2', 'A821GRKOLGILD', 'AGTOYPUYZCCWN', 'AXO4PQU0XG3TG', 'A2LCOSYZL96HHA', 'A328S9RN3U5M68', 'AGUN5F2W85777', 'A1WXOTZHDNAV0Q', 'A3QLAOOTFEHCJI', 'A1TPW86OHXTXFC', 'AFVHO52P7IMPG', 'A18758S1PUYIDT']\n",
      "\n",
      "Number of movies: 1685\n",
      "Number of users: 5033\n"
     ]
    }
   ],
   "source": [
    "users, movies = group_data(rawInputData)\n",
    "print(\"All movies userID 'A3BC8O2KCL29V2' has seen:\\n\", users['A3BC8O2KCL29V2'])\n",
    "print()\n",
    "print(\"All userIDs of watches of movie 'B00F0CLHQO':\\n\", items['B00F0CLHQO'])\n",
    "print()\n",
    "print(\"Number of movies:\", len(movies))\n",
    "print(\"Number of users:\", len(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized index list: [ 9053 12191  8189 ..., 13575  4828 13342]\n",
      "Total number of rand_indxs: 18563\n"
     ]
    }
   ],
   "source": [
    "rand_idxs = np.random.permutation(len(rawOutputData))\n",
    "print(\"Randomized index list:\", rand_idxs)\n",
    "print(\"Total number of rand_indxs:\", len(rand_idxs))\n",
    "rawInputData = [rawInputData[i] for i in rand_idxs]\n",
    "rawOutputData = [rawOutputData[i] for i in rand_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Embedding Matrix **\n",
    "\n",
    "Uses GloVe 50d (100d, 300d) pre-trained embedding to create a giant word dict of possible words.\n",
    "We then use this matrix to create encodings of movie reviews \n",
    "\n",
    "__requires__: GloVe Embeddings to be downloaded - http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing embedding dictionary\n"
     ]
    }
   ],
   "source": [
    "embedding_map = initEmbeddingMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_getSetFromData( )_**: Gets list of unique userID or unique movieID from data\n",
    "\n",
    "**_initVecData( )_**: Returns vectorized versions _InputData_ and dense encodings of review text using embedding matrix\n",
    "\n",
    "**_vecInputData_**: \n",
    "Is rawInputData dict of 18563 samples of userID to movieID, converted to one-hot encoding of where first 5033 col represent user encodings and remaining represent 1685 movieIDs\n",
    "\n",
    "**_vecOutputData_**: \n",
    "Is rawOutputData list of reviews seperated into words where each \n",
    "word in the review is swapped for it's embedding representation i.e.\n",
    "1x50 row per word. These rows are turn into a matrix of words x 50 dims\n",
    "then you take mean of each col to get a 1x50 array (why?). These arrays\n",
    "are returned for each  18563 sample yielding a 18563x50 list matrix\n",
    "\n",
    "**_vecUsers_**:\n",
    "Why like this? Unclear... dimesions are strange but seems to be required in factorization machine\n",
    "\n",
    "**_vecMovies_**:\n",
    "Why like this? Unclear... dimesions are strange but seems to be required in factorization machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing vectorized data\n",
      "Shape of matrix vecInputData: (18563, 6718)\n",
      "Shape of list vecOutputData: (18563, 50)\n",
      "(18563, 1685)\n",
      "(18563, 1685)\n"
     ]
    }
   ],
   "source": [
    "all_users = getSetFromData('user', rawInputData)\n",
    "all_movies = getSetFromData('asin', rawInputData)\n",
    "vecInputData, vecOutputData = initVecData(rawInputData, rawOutputData, embedding_map)\n",
    "vecUsers = vecInputData[:,len(all_users):] #why like this...still unclear\n",
    "vecMovies = vecInputData[:,:len(all_movies)]\n",
    "\n",
    "print(\"Shape of matrix vecInputData: {}\".format(vecInputData.shape))\n",
    "print(\"Shape of list vecOutputData: ({}, {})\".format(len(vecOutputData), len(vecOutputData[0])))\n",
    "\n",
    "print(vecUsers.shape)\n",
    "print(vecMovies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_initMatInputData_**: Creates dense encodings UserID/MovieID data and text review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing matrix data\n",
      "[array([[ 0.11891   ,  0.15255   , -0.082073  , ..., -0.57511997,\n",
      "        -0.26671001,  0.92120999],\n",
      "       [-0.31384   , -0.23372   ,  0.73334002, ..., -0.38913   ,\n",
      "        -0.42574   , -0.20341   ],\n",
      "       [ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
      "        -0.11514   , -0.78580999],\n",
      "       ..., \n",
      "       [-0.35585999,  0.52130002, -0.61070001, ...,  0.27206999,\n",
      "         0.31305   ,  0.92771   ],\n",
      "       [ 0.68046999, -0.039263  ,  0.30186   , ..., -0.073297  ,\n",
      "        -0.064699  , -0.26043999],\n",
      "       [ 0.062712  , -0.0039197 ,  0.23992001, ..., -0.58393002,\n",
      "        -0.32018   ,  0.39699   ]], dtype=float32)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matUserInputData, matMovieInputData = initMatInputData(rawInputData, rawOutputData, embedding_map)\n",
    "#print(len(matUserInputData))\n",
    "#print(len(matMovieInputData))\n",
    "print(matUserInputData[0:1])\n",
    "print()\n",
    "#print(matMovieInputData[0:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_initRatingsOutputData( )_**: \n",
    "Index each unique key pair (usr/movie) in rawInputData then assigns the correct\n",
    "rating to the ratings list based on that index of the key pair in dict userItemDict.\n",
    "Returns list of scores ordered correctly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few ratings...\n",
      " [4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 4.0, 5.0, 4.0, 4.0, 5.0, 3.0, 5.0, 5.0, 5.0, 1.0, 4.0, 2.0, 5.0, 3.0]\n",
      "\n",
      "Total ratings: 18563\n"
     ]
    }
   ],
   "source": [
    "fileName = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "ratingsData = initRatingsOutputData(rawInputData, input_file=fileName,save=False)\n",
    "\n",
    "print(\"Printing a few ratings...\\n\", ratingsData[0:20])\n",
    "print()\n",
    "print(\"Total ratings:\", len(ratingsData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Dot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_create-factorization-model_**\n",
    "Builds and compiles the factorization model which is essentially the row by row dot product of\n",
    "user data with movie data. Returns Keras model\n",
    "\n",
    "_Optimization_: Adam \n",
    "\n",
    "_Loss function_: Mean Squared Error\n",
    "\n",
    "**_matrix-factorization_**:\n",
    "Splits _vecUsers_ and _vecMovies_ into train/test splits and evaluates factorization-model given vecUser/vecMovie input data\n",
    "\n",
    "_batch-size_: preset 32\n",
    "\n",
    "_training-percent_: 90%\n",
    "\n",
    "_epochs_: default 100\n",
    "\n",
    "_earlystopping_: Set implementation\n",
    "\n",
    "**_Evaluate_**:\n",
    "Evaluates factorization model using MSE. Prints predictions vs true labels and shows plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_factorization_model(user_size, item_size, hidden_size):\n",
    "    inputU = Input(shape=(user_size,))\n",
    "    inputI = Input(shape=(item_size,))\n",
    "    hiddenU = Dense(hidden_size)(inputU)\n",
    "    hiddenI = Dense(hidden_size)(inputI)\n",
    "    dotproduct = Dot(axes=1)([hiddenU, hiddenI])\n",
    "    model = Model(inputs=[inputU, inputI], outputs=[dotproduct])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def matrix_factorization(vecUsers, vecMovies, ratingsData, hidden_size=4, epochs=100, training=0.9):\n",
    "    model = create_factorization_model(vecUsers.shape[1], vecMovies.shape[1], hidden_size)\n",
    "\n",
    "    trainingN = int(len(ratingsData) * training) if type(training) is float else training\n",
    "\n",
    "    train_inputs = [vecUsers[:trainingN], vecMovies[:trainingN]]\n",
    "    train_outputs = ratingsData[:trainingN]\n",
    "    test_inputs = [vecUsers[trainingN:], vecMovies[trainingN:]]\n",
    "    test_outputs = ratingsData[trainingN:]\n",
    "    print(model.summary())\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=8)\n",
    "    early_stopping_val = EarlyStopping(monitor='val_loss', patience=12)\n",
    "    batch_size = 32\n",
    "    model.fit(train_inputs, train_outputs, validation_split=0.2, callbacks=[early_stopping, early_stopping_val], batch_size=batch_size, epochs=epochs)\n",
    "    evaluate(model, train_inputs, train_outputs, test_inputs, test_outputs)\n",
    "\n",
    "def evaluate(model, train_inputs, train_outputs, test_inputs, test_outputs):\n",
    "    model_train_mse = model.evaluate(train_inputs, train_outputs, verbose=0)\n",
    "    print(\"Printing a few test outputs...\\n\", test_outputs[0:20])\n",
    "    print()\n",
    "    baseline = np.mean(train_outputs)\n",
    "    baseline_train_mse = np.mean((baseline - train_outputs) ** 2)\n",
    "    print(\"baseline train mse: \" + str(baseline_train_mse))\n",
    "    print(\"model train mse: \" + str(model_train_mse))\n",
    "    print()\n",
    "\n",
    "    model_test_mse = model.evaluate(test_inputs, test_outputs, verbose=1, batch_size=len(test_outputs))\n",
    "    baseline_test_mse = np.mean((baseline - test_outputs) ** 2)\n",
    "    print(\"baseline test mse: \" + str(baseline_test_mse))\n",
    "    print(\"model test mse: \" + str(model_test_mse))\n",
    "\n",
    "\n",
    "    predictions = model.predict(test_inputs, verbose=0)\n",
    "    print(\"Printing a few predictions...\\n\", predictions[0:20])\n",
    "    tf_session = K.get_session()\n",
    "    print(\"MSE Test:\", np.mean(metrics.mean_squared_error(predictions, test_outputs).eval(session=tf_session)))\n",
    "    #print(np.mean((predictions - test_outputs) ** 2))\n",
    "    jitter = lambda x : np.random.normal(x, np.std(x)/10)\n",
    "    plt.scatter(jitter(predictions), jitter(test_outputs))\n",
    "    #plt.scatter(predictions, test_outputs)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Factorization Model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 1685)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1685)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            6744        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4)            6744        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1)            0           dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,488\n",
      "Trainable params: 13,488\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 13364 samples, validate on 3342 samples\n",
      "Epoch 1/10\n",
      "13364/13364 [==============================] - 2s 130us/step - loss: 14.4730 - val_loss: 6.8875\n",
      "Epoch 2/10\n",
      "13364/13364 [==============================] - 2s 124us/step - loss: 2.8001 - val_loss: 1.3346\n",
      "Epoch 3/10\n",
      "13364/13364 [==============================] - 2s 112us/step - loss: 1.1839 - val_loss: 1.1784\n",
      "Epoch 4/10\n",
      "13364/13364 [==============================] - 1s 111us/step - loss: 1.0517 - val_loss: 1.1285\n",
      "Epoch 5/10\n",
      "13364/13364 [==============================] - 1s 111us/step - loss: 0.9733 - val_loss: 1.1031\n",
      "Epoch 6/10\n",
      "13364/13364 [==============================] - 1s 111us/step - loss: 0.9210 - val_loss: 1.0948\n",
      "Epoch 7/10\n",
      " 3552/13364 [======>.......................] - ETA: 1s - loss: 0.9239"
     ]
    }
   ],
   "source": [
    "matrix_factorization(vecUsers, vecItems, ratingsData, epochs=10, training=.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCoNN Recommendation Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_tf",
   "language": "python",
   "name": "dl_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
