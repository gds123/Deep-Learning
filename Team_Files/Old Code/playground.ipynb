{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import simplejson\n",
    "import json\n",
    "import time\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer as tokenizer\n",
    "\n",
    "def get_embedding_weights(tokenizer, embedding_dim=100, fileName='data/glove.6B/glove.6B.50d.txt'):\n",
    "# utils\n",
    "    embedding_map = {}\n",
    "    BASE_DIR = ''\n",
    "    GLOVE_DIR = os.path.join(BASE_DIR, 'data/glove.6B')\n",
    "    glove = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "    \n",
    "    print('constructing embedding dictionary...pls wait ~2min')\n",
    "    #dimension of Glove 300 Embeddings\n",
    "    EMBEDDING_DIM = embedding_dim\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "    #load glove embeddings\n",
    "    embedding_map = {}\n",
    "    for line in glove:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_map[word] = embedding\n",
    "    glove.close()\n",
    "    print('GloVe Word embeddings:', len(embedding_map))\n",
    "\n",
    "    # nb_words contains the total length of vocab\n",
    "    nb_words = len(word_index) + 1\n",
    "\n",
    "    #get glove embeddings for each word in tokenizer.\n",
    "    #word_embedding_matrix holds the embeddings dictionary\n",
    "    word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            word_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    #total words in the tokenizer not in Embedding matrix\n",
    "    print('Null words in GloVe embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
    "    return word_embedding_matrix\n",
    "\n",
    "# utils\n",
    "def initEmbeddingMap(fileName='glove.6B.50d.txt'):\n",
    "    embedding_map = {}\n",
    "    BASE_DIR = ''\n",
    "    GLOVE_DIR = os.path.join(BASE_DIR, 'data/glove.6B')\n",
    "    glove = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "    \n",
    "    print('constructing embedding dictionary')\n",
    "    embedding_map = {}\n",
    "    for line in glove:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        value = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_map[word] = value\n",
    "    glove.close()\n",
    "\n",
    "    #pickle.dump(embedding_map, open(fileName,'wb'))\n",
    "    return embedding_map\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "def clean(text):\n",
    "    return text_to_word_sequence(text,\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "        lower=True,\n",
    "        split=\" \")\n",
    "\n",
    "\n",
    "import pickle\n",
    "# utils - returns dict with users and movies they rated as repeated rows\n",
    "# cleans review text and add to rawOutput\n",
    "def initRawData(input_file, save=False):\n",
    "    print('initializing raw data')\n",
    "    rawInputData = []\n",
    "    rawOutputData = []\n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            if len(line) < 4:\n",
    "                break\n",
    "            lineObj = json.loads(line)\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rawInputDataObj = {'user':user, 'asin':item}\n",
    "            rawOutputDataObj = clean(lineObj['reviewText'])\n",
    "            rawInputData.append(rawInputDataObj)\n",
    "            rawOutputData.append(rawOutputDataObj)\n",
    "    if save:\n",
    "        pickle.dump((rawInputData, rawOutputData), open(fileName,'wb'))\n",
    "    return rawInputData, rawOutputData\n",
    "\n",
    "# utils - creates dict of usrs w/ all movies rated + movies w/ all user ratings *** \n",
    "def getSparcityInfo(inputData):\n",
    "\tusers = {}\n",
    "\titems = {}\n",
    "\tfor datum in inputData:\n",
    "\t\tu = datum['user']\n",
    "\t\ti = datum['asin']\n",
    "\t\tusers.setdefault(u, []).append(i)\n",
    "\t\titems.setdefault(i, []).append(u)\n",
    "\treturn (users, items)\n",
    "\n",
    "# utils - doesnt seem necessary\n",
    "def getSetFromData(key, data):\n",
    "\tresult = set()\n",
    "\tfor datum in data:\n",
    "\t\tresult.add(datum.get(key))\n",
    "\treturn result\n",
    "\n",
    "def seq_2_matrix(sequence, embedding_map):\n",
    "\tm = []\n",
    "\tfor word in sequence:\n",
    "\t\temb = embedding_map.get(word)\n",
    "\t\tif emb is not None:\n",
    "\t\t\tm.append(emb)\n",
    "\treturn np.array(m)\n",
    "\n",
    "def matrix_2_avg(emb_matrix):\n",
    "\treturn np.mean(emb_matrix, 0)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# utils - one hot encodes all data \n",
    "def initVecData(rawInputData, rawOutputData, embedding_map):\n",
    "    print('initializing vectorized data')\n",
    "    dictVect = DictVectorizer()\n",
    "    vecInputData = dictVect.fit_transform(rawInputData).toarray()\n",
    "    vecOutputData = [matrix_2_avg(seq_2_matrix(review, embedding_map)) for review in rawOutputData]\n",
    "    if save:\n",
    "        pickle.dump((vecInputData, vecOutputData), open(fileName,'wb'))\n",
    "    return vecInputData, vecOutputData\n",
    "\n",
    "def initMatInputData(rawInputData, rawOutputData, embedding_map, save=False):\n",
    "    print('initializing matrix data')\n",
    "    if len(rawInputData) != len(rawOutputData):\n",
    "        raise ValueError(\"Need same size of input and output\")\n",
    "    users = {}\n",
    "    extra_info = {}\n",
    "    items = {}\n",
    "    dictVect = DictVectorizer()\n",
    "    for i in range(len(rawInputData)):\n",
    "        vecOutput = seq_2_matrix(rawOutputData[i], embedding_map)\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        users.setdefault(user, []).append(vecOutput)\n",
    "        items.setdefault(item, []).append(vecOutput)\n",
    "    matUserInputData = []\n",
    "    matItemInputData = []\n",
    "    users = {k: np.vstack(v) for k, v in users.items()}\n",
    "    items = {k: np.vstack(v) for k, v in items.items()}\n",
    "    extra_info['user_seq_sizes'] = [m.shape[0] for m in users.values()]\n",
    "    extra_info['item_seq_sizes'] = [m.shape[0] for m in items.values()]\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        user = rawInput['user']\n",
    "        item = rawInput['asin']\n",
    "        matUserInputData.append(users.get(user))\n",
    "        matItemInputData.append(items.get(item))\n",
    "    if save:\n",
    "        pickle.dump((matUserInputData, matItemInputData), open(fileName,'wb'))\n",
    "    return matUserInputData, matItemInputData\n",
    "\n",
    "def toKey(user, item):\n",
    "    return (user, item)\n",
    "\n",
    "def initRatingsOutputData(rawInputData, input_file, maxlines = 1000, fileName='ratingsData.p', save=True):\n",
    "    try:\n",
    "        ratingsData = pickle.load(open(fileName,'rb'))\n",
    "        print('loaded saved ratings data')\n",
    "    except (OSError, IOError) as e:\n",
    "        ratingsData = []\n",
    "        userItemDict = {}\n",
    "        for i in range(len(rawInputData)):\n",
    "            rawInput = rawInputData[i]\n",
    "            userItem = toKey(rawInput['user'], rawInput['asin'])\n",
    "            userItemDict[userItem] = i\n",
    "            ratingsData.append(None) # check later to make sure no Nones left\n",
    "        with open(input_file,'r') as f:\n",
    "            for i in range(maxlines):\n",
    "                line = f.readline()\n",
    "                #print(\"line:\\n\",line)\n",
    "                if len(line) < 4:\n",
    "                    break\n",
    "                #terms = line.split(',')\n",
    "                lineObj = json.loads(line)\n",
    "#                 user = terms[0]\n",
    "#                 item = terms[1]\n",
    "                user = lineObj['reviewerID']\n",
    "                item = lineObj['asin']\n",
    "                rating = lineObj['overall']\n",
    "                #print(\"user:\", user)\n",
    "                #print(\"item:\", item)\n",
    "                #print(\"rating:\",rating)\n",
    "                #rating = float(terms[2]) / 2.5 - 1.0\n",
    "                i = userItemDict.get(toKey(user, item))\n",
    "                if i is not None:\n",
    "                    ratingsData[i] = rating\n",
    "            failure = None in ratingsData\n",
    "            if failure:\n",
    "                raise ValueError(str(len([r for r in ratingsData if r is None])) + \" reviews did not have corresponding rating.\")\n",
    "        if save:\n",
    "            pickle.dump(ratingsData, open(fileName,'wb'))\n",
    "    return ratingsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = \"data/reviews_Amazon_Instant_Video_5.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = initEmbeddingMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawInputData, rawOutputData = initRawData(input_file=path_to_data, save=False)\n",
    "print(\"Number of user/item data:\", len(rawInputData))\n",
    "print(\"Number of reviews:\", len(rawOutputData))\n",
    "print()\n",
    "print(rawInputData[0:5])\n",
    "print()\n",
    "print(rawOutputData[0])\n",
    "print()\n",
    "print(rawOutputData[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, items = getSparcityInfo(rawInputData)\n",
    "print(users['A3BC8O2KCL29V2'])\n",
    "print()\n",
    "print(items['B00F0CLHQO'])\n",
    "print(\"Num reviews per distinct user:\")\n",
    "print(\"lenth items:\", len(items))\n",
    "print()\n",
    "print(\"lenth users:\", len(users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idxs = np.random.permutation(len(rawOutputData))\n",
    "print(\"Random index list:\", rand_idxs)\n",
    "print(\"Number of rand. indexes == len(data):\", len(rand_idxs))\n",
    "rawInputData = [rawInputData[i] for i in rinds]\n",
    "rawOutputData = [rawOutputData[i] for i in rinds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecInputData, vecOutputData = initVecData(rawInputData, rawOutputData, embedding_map, save=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = getSetFromData('user', rawInputData)\n",
    "all_items = getSetFromData('asin', rawInputData)\n",
    "vecInputData, vecOutputData = initVecData(rawInputData, rawOutputData, embedding_map, save=False)\n",
    "vecUsers = vecInputData[:,len(all_users):]\n",
    "vecItems = vecInputData[:,:len(all_items)]\n",
    "\n",
    "\"\"\"\n",
    "vecInputData:\n",
    "    Is rawInputData dict of 18563 samples of userID to movieID,\n",
    "    converted to one-hot encoding of\n",
    "    where first 5033 col represent user encodings and remaining\n",
    "    represent 1685 movieIDs\n",
    "\"\"\"\n",
    "print(\"Shape of matrix vecInputData: {}\".format(vecInputData.shape))\n",
    "\n",
    "\"\"\"\n",
    "vecOutputData:\n",
    "    Is rawOutputData list of reviews seperated into words where each\n",
    "    word in the review is swapped for it's embedding representation i.e.\n",
    "    1x50 row per word. These rows are turn into a matrix of words x 50 dims\n",
    "    then you take mean of each col to get a 1x50 array (why?). These arrays\n",
    "    are returned for each  18563 sample yielding a 18563x50 list matrix\n",
    "\"\"\"\n",
    "print(\"Shape of list vecOutputData: ({}, {})\".format(len(vecOutputData), len(vecOutputData[0])))\n",
    "\n",
    "\"\"\"\n",
    "vecUsers | vecItems:\n",
    "    Seperates the user one-hot encodings from the item encodings for each row/group\n",
    "\"\"\"\n",
    "vecUsers = vecInputData[:,len(all_users):]\n",
    "vecItems = vecInputData[:,:len(all_items)]\n",
    "\n",
    "print(vecUsers.shape)\n",
    "print(vecItems.shape)\n",
    "\n",
    "#print(\"Length of vecUsers == Num Users:\", len(vecUsers[0]))\n",
    "#print(\"Length of vecItems == Num Items:\", len(vecItems[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_file = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "\n",
    "matUserInputData, matItemInputData = initMatInputData(rawInputData, rawOutputData, embedding_map)\n",
    "print(len(matUserInputData))\n",
    "print(len(matUserInputData[2]))\n",
    "\n",
    "print(len(matItemInputData))\n",
    "print(len(matItemInputData[2]))\n",
    "\n",
    "#matItemInputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18563"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initRatingsOutputData(rawInputData, input_file, save=True):\n",
    "    ratingsData = []\n",
    "    userItemDict = {}\n",
    "    for i in range(len(rawInputData)):\n",
    "        rawInput = rawInputData[i]\n",
    "        userItem = toKey(rawInput['user'], rawInput['asin'])\n",
    "        userItemDict[userItem] = i\n",
    "        ratingsData.append(None) # check later to make sure no Nones left\n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            #print(\"line:\\n\",line)\n",
    "            if len(line) < 4:\n",
    "                break\n",
    "            #terms = line.split(',')\n",
    "            lineObj = json.loads(line)\n",
    "#                 user = terms[0]\n",
    "#                 item = terms[1]\n",
    "            user = lineObj['reviewerID']\n",
    "            item = lineObj['asin']\n",
    "            rating = lineObj['overall']\n",
    "            #rating = float(terms[2]) / 2.5 - 1.0\n",
    "            i = userItemDict.get(toKey(user, item))\n",
    "            if i is not None:\n",
    "                ratingsData[i] = rating\n",
    "        failure = None in ratingsData\n",
    "        if failure:\n",
    "            raise ValueError(str(len([r for r in ratingsData if r is None])) + \" reviews did not have corresponding rating.\")\n",
    "    if save:\n",
    "        pickle.dump(ratingsData, open(fileName,'wb'))\n",
    "    return ratingsData\n",
    "\n",
    "\"\"\"\n",
    "rData:\n",
    "    Index each unique key pair (usr/movie) in rawInputData then assigns the correct\n",
    "    rating to the ratings list based on that index of the key pair in dict userItemDict.\n",
    "    Returns list of scores ordered correctly\n",
    "\"\"\"\n",
    "rData = initRatingsOutputData(rawInputData, input_file=ratings_file,save=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Dot\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_factorization_model(user_size, item_size, hidden_size):\n",
    "    inputU = Input(shape=(user_size,))\n",
    "    inputI = Input(shape=(item_size,))\n",
    "    hiddenU = Dense(hidden_size)(inputU)\n",
    "    hiddenI = Dense(hidden_size)(inputI)\n",
    "    dotproduct = Dot(axes=1)([hiddenU, hiddenI])\n",
    "    model = Model(inputs=[inputU, inputI], outputs=[dotproduct])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def matrix_factorization(vecUsers, vecItems, ratingsData, hidden_size=4, epochs=100, training=0.9):\n",
    "    model = create_factorization_model(vecUsers.shape[1], vecItems.shape[1], hidden_size)\n",
    "\n",
    "    trainingN = int(len(ratingsData) * training) if type(training) is float else training\n",
    "\n",
    "    train_inputs = [vecUsers[:trainingN], vecItems[:trainingN]]\n",
    "    train_outputs = ratingsData[:trainingN]\n",
    "    test_inputs = [vecUsers[trainingN:], vecItems[trainingN:]]\n",
    "    test_outputs = ratingsData[trainingN:]\n",
    "    print(model.summary())\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=8)\n",
    "    early_stopping_val = EarlyStopping(monitor='val_loss', patience=12)\n",
    "    batch_size = 32\n",
    "    model.fit(train_inputs, train_outputs, validation_split=0.2, callbacks=[early_stopping, early_stopping_val], batch_size=batch_size, epochs=epochs)\n",
    "    evaluate(model, train_inputs, train_outputs, test_inputs, test_outputs)\n",
    "\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "\n",
    "def evaluate(model, train_inputs, train_outputs, test_inputs, test_outputs):\n",
    "    model_train_mse = model.evaluate(train_inputs, train_outputs, verbose=0)\n",
    "    print(\"Printing a few test outputs...\\n\", test_outputs[0:20])\n",
    "    print()\n",
    "    baseline = np.mean(train_outputs)\n",
    "    baseline_train_mse = np.mean((baseline - train_outputs) ** 2)\n",
    "    print(\"baseline train mse: \" + str(baseline_train_mse))\n",
    "    print(\"model train mse: \" + str(model_train_mse))\n",
    "    print()\n",
    "\n",
    "    model_test_mse = model.evaluate(test_inputs, test_outputs, verbose=1, batch_size=len(test_outputs))\n",
    "    baseline_test_mse = np.mean((baseline - test_outputs) ** 2)\n",
    "    print(\"baseline test mse: \" + str(baseline_test_mse))\n",
    "    print(\"model test mse: \" + str(model_test_mse))\n",
    "\n",
    "    jitter = lambda x : np.random.normal(x, np.std(x)/10)\n",
    "\n",
    "    predictions = model.predict(test_inputs, verbose=0)\n",
    "    print(\"Printing a few predictions...\\n\", predictions[0:20])\n",
    "    tf_session = K.get_session()\n",
    "    print(np.mean(metrics.mean_squared_error(predictions, test_outputs).eval(session=tf_session)))\n",
    "    print(np.mean((predictions - test_outputs) ** 2))\n",
    "    plt.scatter(jitter(predictions), jitter(test_outputs))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18563, 1685)\n",
      "(18563, 1685)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 1685)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 1685)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 4)            6744        input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 4)            6744        input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_8 (Dot)                     (None, 1)            0           dense_15[0][0]                   \n",
      "                                                                 dense_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 13,488\n",
      "Trainable params: 13,488\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 13364 samples, validate on 3342 samples\n",
      "Epoch 1/150\n",
      "13364/13364 [==============================] - 2s 135us/step - loss: 14.4494 - val_loss: 6.9131\n",
      "Epoch 2/150\n",
      "13364/13364 [==============================] - 2s 118us/step - loss: 2.7910 - val_loss: 1.3975\n",
      "Epoch 3/150\n",
      "13364/13364 [==============================] - 2s 115us/step - loss: 1.1776 - val_loss: 1.2265\n",
      "Epoch 4/150\n",
      "13364/13364 [==============================] - 1s 103us/step - loss: 1.0464 - val_loss: 1.1654\n",
      "Epoch 5/150\n",
      "13364/13364 [==============================] - 1s 112us/step - loss: 0.9701 - val_loss: 1.1360\n",
      "Epoch 6/150\n",
      "13364/13364 [==============================] - 2s 131us/step - loss: 0.9196 - val_loss: 1.1186\n",
      "Epoch 7/150\n",
      "13364/13364 [==============================] - 2s 141us/step - loss: 0.8831 - val_loss: 1.1113\n",
      "Epoch 8/150\n",
      "13364/13364 [==============================] - 1s 110us/step - loss: 0.8564 - val_loss: 1.1105\n",
      "Epoch 9/150\n",
      "13364/13364 [==============================] - 1s 94us/step - loss: 0.8353 - val_loss: 1.1109\n",
      "Epoch 10/150\n",
      "13364/13364 [==============================] - 1s 103us/step - loss: 0.8184 - val_loss: 1.1156\n",
      "Epoch 11/150\n",
      "13364/13364 [==============================] - 2s 128us/step - loss: 0.8056 - val_loss: 1.1222\n",
      "Epoch 12/150\n",
      "13364/13364 [==============================] - 2s 116us/step - loss: 0.7956 - val_loss: 1.1235\n",
      "Epoch 13/150\n",
      "13364/13364 [==============================] - 1s 96us/step - loss: 0.7868 - val_loss: 1.1322\n",
      "Epoch 14/150\n",
      "13364/13364 [==============================] - 1s 106us/step - loss: 0.7794 - val_loss: 1.1354\n",
      "Epoch 15/150\n",
      "13364/13364 [==============================] - 2s 152us/step - loss: 0.7740 - val_loss: 1.1448\n",
      "Epoch 16/150\n",
      "13364/13364 [==============================] - 2s 138us/step - loss: 0.7687 - val_loss: 1.1479\n",
      "Epoch 17/150\n",
      "13364/13364 [==============================] - 2s 139us/step - loss: 0.7645 - val_loss: 1.1567\n",
      "Epoch 18/150\n",
      "13364/13364 [==============================] - 2s 157us/step - loss: 0.7611 - val_loss: 1.1617\n",
      "Epoch 19/150\n",
      "13364/13364 [==============================] - 2s 149us/step - loss: 0.7581 - val_loss: 1.1691\n",
      "Epoch 20/150\n",
      "13364/13364 [==============================] - 2s 143us/step - loss: 0.7553 - val_loss: 1.1761\n",
      "Printing a few test outputs...\n",
      " [5.0, 5.0, 5.0, 5.0, 4.0, 3.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 2.0, 3.0, 5.0, 4.0, 4.0, 3.0, 4.0, 1.0]\n",
      "\n",
      "baseline train mse: 1.2668639781\n",
      "model train mse: 0.820695000738\n",
      "\n",
      "1857/1857 [==============================] - 0s 63us/step\n",
      "baseline test mse: 1.24368066804\n",
      "model test mse: 1.20948505402\n",
      "Printing a few predictions...\n",
      " [[ 4.12295628]\n",
      " [ 3.10907698]\n",
      " [ 4.80224752]\n",
      " [ 4.3836422 ]\n",
      " [ 3.50092173]\n",
      " [ 3.6700058 ]\n",
      " [ 4.32393932]\n",
      " [ 4.70022297]\n",
      " [ 3.83021736]\n",
      " [ 5.27058649]\n",
      " [ 4.02635288]\n",
      " [ 3.33182764]\n",
      " [ 3.78323841]\n",
      " [ 3.30540133]\n",
      " [ 4.1636405 ]\n",
      " [ 1.88485646]\n",
      " [ 4.19530201]\n",
      " [ 2.70941997]\n",
      " [ 4.48214722]\n",
      " [ 4.59820461]]\n",
      "1.75701155246\n",
      "1.75701155246\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14b209320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(vecUsers.shape)\n",
    "print(vecItems.shape)\n",
    "\n",
    "matrix_factorization(vecUsers, vecItems, rData, epochs=150, training=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"data/reviews_Amazon_Instant_Video_5.json.gz\"\n",
    "raw_data = [json.loads(i) for i in gzip.open(path_to_data, \"rt\")]\n",
    "raw_data = raw_data[:100]\n",
    "data = pd.DataFrame(raw_data)\n",
    "\n",
    "print(\"\\ncolumn names:\\n\", data.columns)\n",
    "\n",
    "reviewers = data[\"reviewerID\"].unique()\n",
    "num_reviewers = len(reviewers)\n",
    "movies = data[\"asin\"].unique()\n",
    "num_movies = len(movies)\n",
    "\n",
    "print(\"number unique reviewers: \", num_reviewers)\n",
    "print(\"number unique movies: \", num_movies)\n",
    "print(\"\\nExample review:\\n\")\n",
    "print(data[['reviewerID','reviewText']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create int-to-name dictionaries for each - Q: Juice why do we need?\n",
    "reviewer_to_num = {reviewer: idx for idx, reviewer in enumerate(reviewers)}\n",
    "num_to_reviewer = {reviewer_to_num[reviewer]: reviewer for reviewer in reviewer_to_num}\n",
    "product_to_num = {product: idx for idx, product in enumerate(products)}\n",
    "num_to_product = {product_to_num[product]: product for product in product_to_num}\n",
    "\n",
    "num_to_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_user_product_review_matrix(dimensions, dataset):\n",
    "    assert isinstance(dataset, pd.DataFrame)\n",
    "    ret_matrix = np.zeros(dimensions)\n",
    "    for idx, row in dataset.iterrows():\n",
    "        ret_matrix[reviewer_to_num[row[\"reviewerID\"]], product_to_num[row[\"asin\"]]] = row[\"overall\"]\n",
    "    return ret_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test data indices\n",
    "frac_test = 0.1\n",
    "test_indices = np.random.choice(num_reviews,\n",
    "                                size=int(num_reviews * frac_test),\n",
    "                                replace=False)\n",
    "\n",
    "# split raw data into train/test\n",
    "raw_test = data.iloc[test_indices, :]\n",
    "raw_train = data.drop(test_indices).dropna()\n",
    "\n",
    "# get dimensions of the matrices\n",
    "dim = (num_reviewers, num_products)\n",
    "test_matrix = populate_user_product_review_matrix(dim, raw_test)\n",
    "train_matrix = populate_user_product_review_matrix(dim, raw_train)\n",
    "\n",
    "# self.cos_sim_mat = np.zeros((self.num_products, self.num_products))\n",
    "# for root_prod_idx, prod_ratings in enumerate(self.train_matrix.T):\n",
    "#     for comp_prod_idx, comp_ratings in enumerate(self.train_matrix.T):\n",
    "#         if comp_prod_idx >= root_prod_idx:\n",
    "#             self.cos_sim_mat[root_prod_idx, comp_prod_idx] = 1 - scipy.spatial.distance.cosine(prod_ratings,\n",
    "#                                                                                                comp_ratings)\n",
    "\n",
    "# self.test = self.permute_matrix(self.test_matrix)\n",
    "# self.train = self.permute_matrix(self.train_matrix)\n",
    "\n",
    "print(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text, token):\n",
    "    return text_to_word_sequence(text,\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "        lower=True,\n",
    "        split=\" \")\n",
    "def agg_reviews(input_file):\n",
    "    reviews = []\n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            review = lineObj['reviewText']\n",
    "            reviews.append(review) \n",
    "    return reviews\n",
    "\n",
    "def process(input_file, token):\n",
    "    reviews = []\n",
    "    with open(input_file,'r') as f:\n",
    "        for i in f:\n",
    "            line = f.readline()\n",
    "            lineObj = json.loads(line)\n",
    "            review = lineObj['reviewText']\n",
    "            clean_review = clean(review, token)\n",
    "            reviews.append(clean_review)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "path_to_data = \"data/reviews_Amazon_Instant_Video_5.json\"\n",
    "raw_reviews = agg_reviews(path_to_data)\n",
    "print(len(raw_reviews))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(raw_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = process(path_to_data, tokenizer)\n",
    "x = reviews[100]\n",
    "print(len(reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(raw_reviews)\n",
    "x1 = np.hstack(train_seq)\n",
    "print(x1.shape)\n",
    "train_pad = sequence.pad_sequences(train_seq, maxlen = 250)\n",
    "train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_tf",
   "language": "python",
   "name": "dl_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
